{
    "sourceFile": "layers/Embed.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 4,
            "patches": [
                {
                    "date": 1730555066486,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1730637067031,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -3,9 +3,9 @@\n import math\n \n \n class PositionalEmbedding(nn.Module):\n-    def __init__(self, d_model, max_len=5000):\n+    def __init__(self, d_model, max_len=6500):\n         super(PositionalEmbedding, self).__init__()\n         # Compute the positional encodings once in log space.\n         pe = torch.zeros(max_len, d_model).float()\n         pe.require_grad = False\n"
                },
                {
                    "date": 1730637083862,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -3,9 +3,9 @@\n import math\n \n \n class PositionalEmbedding(nn.Module):\n-    def __init__(self, d_model, max_len=6500):\n+    def __init__(self, d_model, max_len=5000):\n         super(PositionalEmbedding, self).__init__()\n         # Compute the positional encodings once in log space.\n         pe = torch.zeros(max_len, d_model).float()\n         pe.require_grad = False\n"
                },
                {
                    "date": 1730708676674,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -3,9 +3,9 @@\n import math\n \n \n class PositionalEmbedding(nn.Module):\n-    def __init__(self, d_model, max_len=5000):\n+    def __init__(self, d_model, max_len=6500):\n         super(PositionalEmbedding, self).__init__()\n         # Compute the positional encodings once in log space.\n         pe = torch.zeros(max_len, d_model).float()\n         pe.require_grad = False\n"
                },
                {
                    "date": 1730721638927,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -3,9 +3,9 @@\n import math\n \n \n class PositionalEmbedding(nn.Module):\n-    def __init__(self, d_model, max_len=6500):\n+    def __init__(self, d_model, max_len=5000):\n         super(PositionalEmbedding, self).__init__()\n         # Compute the positional encodings once in log space.\n         pe = torch.zeros(max_len, d_model).float()\n         pe.require_grad = False\n"
                }
            ],
            "date": 1730555066486,
            "name": "Commit-0",
            "content": "import torch\nimport torch.nn as nn\nimport math\n\n\nclass PositionalEmbedding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super(PositionalEmbedding, self).__init__()\n        # Compute the positional encodings once in log space.\n        pe = torch.zeros(max_len, d_model).float()\n        pe.require_grad = False\n\n        position = torch.arange(0, max_len).float().unsqueeze(1)\n        div_term = (torch.arange(0, d_model, 2).float()\n                    * -(math.log(10000.0) / d_model)).exp()\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        return self.pe[:, :x.size(1)]\n\n\nclass TokenEmbedding(nn.Module):\n    def __init__(self, c_in, d_model):\n        super(TokenEmbedding, self).__init__()\n        padding = 1 if torch.__version__ >= '1.5.0' else 2\n        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,\n                                   kernel_size=3, padding=padding, padding_mode='circular', bias=False)\n        for m in self.modules():\n            if isinstance(m, nn.Conv1d):\n                nn.init.kaiming_normal_(\n                    m.weight, mode='fan_in', nonlinearity='leaky_relu')\n\n    def forward(self, x):\n        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)\n        return x\n\n\nclass FixedEmbedding(nn.Module):\n    def __init__(self, c_in, d_model):\n        super(FixedEmbedding, self).__init__()\n\n        w = torch.zeros(c_in, d_model).float()\n        w.require_grad = False\n\n        position = torch.arange(0, c_in).float().unsqueeze(1)\n        div_term = (torch.arange(0, d_model, 2).float()\n                    * -(math.log(10000.0) / d_model)).exp()\n\n        w[:, 0::2] = torch.sin(position * div_term)\n        w[:, 1::2] = torch.cos(position * div_term)\n\n        self.emb = nn.Embedding(c_in, d_model)\n        self.emb.weight = nn.Parameter(w, requires_grad=False)\n\n    def forward(self, x):\n        return self.emb(x).detach()\n\n\nclass TemporalEmbedding(nn.Module):\n    def __init__(self, d_model, embed_type='fixed', freq='h'):\n        super(TemporalEmbedding, self).__init__()\n\n        minute_size = 4\n        hour_size = 24\n        weekday_size = 7\n        day_size = 32\n        month_size = 13\n\n        Embed = FixedEmbedding if embed_type == 'fixed' else nn.Embedding\n        if freq == 't':\n            self.minute_embed = Embed(minute_size, d_model)\n        self.hour_embed = Embed(hour_size, d_model)\n        self.weekday_embed = Embed(weekday_size, d_model)\n        self.day_embed = Embed(day_size, d_model)\n        self.month_embed = Embed(month_size, d_model)\n\n    def forward(self, x):\n        x = x.long()\n        minute_x = self.minute_embed(x[:, :, 4]) if hasattr(\n            self, 'minute_embed') else 0.\n        hour_x = self.hour_embed(x[:, :, 3])\n        weekday_x = self.weekday_embed(x[:, :, 2])\n        day_x = self.day_embed(x[:, :, 1])\n        month_x = self.month_embed(x[:, :, 0])\n\n        return hour_x + weekday_x + day_x + month_x + minute_x\n\n\nclass TimeFeatureEmbedding(nn.Module):\n    def __init__(self, d_model, embed_type='timeF', freq='h'):\n        super(TimeFeatureEmbedding, self).__init__()\n\n        freq_map = {'h': 4, 't': 5, 's': 6,\n                    'm': 1, 'a': 1, 'w': 2, 'd': 3, 'b': 3}\n        d_inp = freq_map[freq]\n        self.embed = nn.Linear(d_inp, d_model, bias=False)\n\n    def forward(self, x):\n        return self.embed(x)\n\n\nclass DataEmbedding(nn.Module):\n    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n        super(DataEmbedding, self).__init__()\n\n        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n        self.position_embedding = PositionalEmbedding(d_model=d_model)\n        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type,\n                                                    freq=freq) if embed_type != 'timeF' else TimeFeatureEmbedding(\n            d_model=d_model, embed_type=embed_type, freq=freq)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, x, x_mark):\n        if x_mark is None:\n            x = self.value_embedding(x) + self.position_embedding(x)\n        else:\n            x = self.value_embedding(\n                x) + self.temporal_embedding(x_mark) + self.position_embedding(x)\n        return self.dropout(x)\n\n\nclass DataEmbedding_inverted(nn.Module):\n    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n        super(DataEmbedding_inverted, self).__init__()\n        self.value_embedding = nn.Linear(c_in, d_model)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, x, x_mark):\n        x = x.permute(0, 2, 1)\n        # x: [Batch Variate Time]\n        if x_mark is None:\n            x = self.value_embedding(x)\n        else:\n            x = self.value_embedding(\n                torch.cat([x, x_mark.permute(0, 2, 1)], 1))\n        # x: [Batch Variate d_model]\n        return self.dropout(x)\n\n\nclass DataEmbedding_wo_pos(nn.Module):\n    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n        super(DataEmbedding_wo_pos, self).__init__()\n\n        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n        self.position_embedding = PositionalEmbedding(d_model=d_model)\n        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type,\n                                                    freq=freq) if embed_type != 'timeF' else TimeFeatureEmbedding(\n            d_model=d_model, embed_type=embed_type, freq=freq)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, x, x_mark):\n        if x_mark is None:\n            x = self.value_embedding(x)\n        else:\n            x = self.value_embedding(x) + self.temporal_embedding(x_mark)\n        return self.dropout(x)\n\n\nclass PatchEmbedding(nn.Module):\n    def __init__(self, d_model, patch_len, stride, padding, dropout):\n        super(PatchEmbedding, self).__init__()\n        # Patching\n        self.patch_len = patch_len\n        self.stride = stride\n        self.padding_patch_layer = nn.ReplicationPad1d((0, padding))\n\n        # Backbone, Input encoding: projection of feature vectors onto a d-dim vector space\n        self.value_embedding = nn.Linear(patch_len, d_model, bias=False)\n\n        # Positional embedding\n        self.position_embedding = PositionalEmbedding(d_model)\n\n        # Residual dropout\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # do patching\n        n_vars = x.shape[1]\n        x = self.padding_patch_layer(x)\n        x = x.unfold(dimension=-1, size=self.patch_len, step=self.stride)\n        x = torch.reshape(x, (x.shape[0] * x.shape[1], x.shape[2], x.shape[3]))\n        # Input encoding\n        x = self.value_embedding(x) + self.position_embedding(x)\n        return self.dropout(x), n_vars\n"
        }
    ]
}