{
    "sourceFile": "layers/SelfAttention_Family.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 119,
            "patches": [
                {
                    "date": 1730855017210,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1730855509090,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -110,10 +110,10 @@\n         self.output_attention = output_attention\n         self.dropout = nn.Dropout(attention_dropout)\n         self.covariate = covariate\n         self.flash_attention = flash_attention\n-        self.qk_proj = QueryKeyProjection(dim=d_model, num_heads=num_heads, proj_layer=RotaryProjection, kwargs=dict(max_len=max_len),\n-                                          partial_factor=(0.0, 0.5),)\n+        # self.qk_proj = QueryKeyProjection(dim=d_model, num_heads=num_heads, proj_layer=RotaryProjection, kwargs=dict(max_len=max_len),\n+        #                                   partial_factor=(0.0, 0.5),)\n         self.attn_bias = BinaryAttentionBias(dim=d_model, num_heads=num_heads)\n \n     def forward(self, queries, keys, values, attn_mask, n_vars, n_tokens, tau=None, delta=None):\n         B, L, H, E = queries.shape\n@@ -127,10 +127,10 @@\n \n         seq_id = torch.arange(n_tokens * n_vars)\n         seq_id = repeat(seq_id, 'n -> b h n', b=B, h=H)\n \n-        queries, keys = self.qk_proj(\n-            queries, keys, query_id=seq_id, kv_id=seq_id)\n+        # queries, keys = self.qk_proj(\n+        #     queries, keys, query_id=seq_id, kv_id=seq_id)\n \n         scale = self.scale or 1. / sqrt(E)\n \n         var_id = repeat(torch.arange(n_vars),\n"
                },
                {
                    "date": 1730855660921,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -124,10 +124,10 @@\n         keys = keys.permute(0, 2, 1, 3)\n         if self.flash_attention:\n             values = values.permute(0, 2, 1, 3)\n \n-        seq_id = torch.arange(n_tokens * n_vars)\n-        seq_id = repeat(seq_id, 'n -> b h n', b=B, h=H)\n+        # seq_id = torch.arange(n_tokens * n_vars)\n+        # seq_id = repeat(seq_id, 'n -> b h n', b=B, h=H)\n \n         # queries, keys = self.qk_proj(\n         #     queries, keys, query_id=seq_id, kv_id=seq_id)\n \n"
                },
                {
                    "date": 1730856874885,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -101,11 +101,11 @@\n             return V.contiguous(), None\n         else:\n             return V.contiguous(), None\n \n-class HybridAttention(nn.Module):\n+class TimeAttention(nn.Module):\n     def __init__(self, mask_flag=True, scale=None, attention_dropout=0.1, output_attention=False, d_model=512, num_heads=8, max_len=100, covariate=False, flash_attention=False):\n-        super(HybridAttention, self).__init__()\n+        super(TimeAttention, self).__init__()\n         self.scale = scale\n         self.mask_flag = mask_flag\n         self.output_attention = output_attention\n         self.dropout = nn.Dropout(attention_dropout)\n"
                },
                {
                    "date": 1730856887587,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -37,11 +37,11 @@\n         else:\n             return V.contiguous(), None\n \n \n-class TimeAttention(nn.Module):\n+class HybridAttention(nn.Module):\n     def __init__(self, mask_flag=True, scale=None, attention_dropout=0.1, output_attention=False, d_model=512, num_heads=8, max_len=100, covariate=False, flash_attention=False):\n-        super(TimeAttention, self).__init__()\n+        super(HybridAttention, self).__init__()\n         self.scale = scale\n         self.mask_flag = mask_flag\n         self.output_attention = output_attention\n         self.dropout = nn.Dropout(attention_dropout)\n"
                },
                {
                    "date": 1730860285172,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -110,10 +110,10 @@\n         self.output_attention = output_attention\n         self.dropout = nn.Dropout(attention_dropout)\n         self.covariate = covariate\n         self.flash_attention = flash_attention\n-        # self.qk_proj = QueryKeyProjection(dim=d_model, num_heads=num_heads, proj_layer=RotaryProjection, kwargs=dict(max_len=max_len),\n-        #                                   partial_factor=(0.0, 0.5),)\n+        self.qk_proj = QueryKeyProjection(dim=d_model, num_heads=num_heads, proj_layer=RotaryProjection, kwargs=dict(max_len=max_len),\n+                                          partial_factor=(0.0, 0.5),)\n         self.attn_bias = BinaryAttentionBias(dim=d_model, num_heads=num_heads)\n \n     def forward(self, queries, keys, values, attn_mask, n_vars, n_tokens, tau=None, delta=None):\n         B, L, H, E = queries.shape\n@@ -125,12 +125,12 @@\n         if self.flash_attention:\n             values = values.permute(0, 2, 1, 3)\n \n         # seq_id = torch.arange(n_tokens * n_vars)\n-        # seq_id = repeat(seq_id, 'n -> b h n', b=B, h=H)\n+        seq_id = repeat(seq_id, 'n -> b h n', b=B, h=H)\n \n-        # queries, keys = self.qk_proj(\n-        #     queries, keys, query_id=seq_id, kv_id=seq_id)\n+        queries, keys = self.qk_proj(\n+            queries, keys, query_id=seq_id, kv_id=seq_id)\n \n         scale = self.scale or 1. / sqrt(E)\n \n         var_id = repeat(torch.arange(n_vars),\n"
                },
                {
                    "date": 1730860355334,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -125,8 +125,9 @@\n         if self.flash_attention:\n             values = values.permute(0, 2, 1, 3)\n \n         # seq_id = torch.arange(n_tokens * n_vars)\n+        seq_id = repeat(torch.arange(n_tokens), 'n -> c n', c=n_vars)\n         seq_id = repeat(seq_id, 'n -> b h n', b=B, h=H)\n \n         queries, keys = self.qk_proj(\n             queries, keys, query_id=seq_id, kv_id=seq_id)\n"
                },
                {
                    "date": 1730860361825,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -125,9 +125,9 @@\n         if self.flash_attention:\n             values = values.permute(0, 2, 1, 3)\n \n         # seq_id = torch.arange(n_tokens * n_vars)\n-        seq_id = repeat(torch.arange(n_tokens), 'n -> c n', c=n_vars)\n+        seq_id = repeat(torch.arange(n_tokens), 'n -> c n', c=n_vars).reshape(-1)\n         seq_id = repeat(seq_id, 'n -> b h n', b=B, h=H)\n \n         queries, keys = self.qk_proj(\n             queries, keys, query_id=seq_id, kv_id=seq_id)\n"
                },
                {
                    "date": 1730877071209,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -125,13 +125,13 @@\n         if self.flash_attention:\n             values = values.permute(0, 2, 1, 3)\n \n         # seq_id = torch.arange(n_tokens * n_vars)\n-        seq_id = repeat(torch.arange(n_tokens), 'n -> c n', c=n_vars).reshape(-1)\n-        seq_id = repeat(seq_id, 'n -> b h n', b=B, h=H)\n+        # seq_id = repeat(torch.arange(n_tokens), 'n -> c n', c=n_vars).reshape(-1)\n+        # seq_id = repeat(seq_id, 'n -> b h n', b=B, h=H)\n \n-        queries, keys = self.qk_proj(\n-            queries, keys, query_id=seq_id, kv_id=seq_id)\n+        # queries, keys = self.qk_proj(\n+        #     queries, keys, query_id=seq_id, kv_id=seq_id)\n \n         scale = self.scale or 1. / sqrt(E)\n \n         var_id = repeat(torch.arange(n_vars),\n"
                },
                {
                    "date": 1730881379673,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -125,13 +125,13 @@\n         if self.flash_attention:\n             values = values.permute(0, 2, 1, 3)\n \n         # seq_id = torch.arange(n_tokens * n_vars)\n-        # seq_id = repeat(torch.arange(n_tokens), 'n -> c n', c=n_vars).reshape(-1)\n-        # seq_id = repeat(seq_id, 'n -> b h n', b=B, h=H)\n+        seq_id = repeat(torch.arange(n_tokens), 'n -> c n', c=n_vars).reshape(-1)\n+        seq_id = repeat(seq_id, 'n -> b h n', b=B, h=H)\n \n-        # queries, keys = self.qk_proj(\n-        #     queries, keys, query_id=seq_id, kv_id=seq_id)\n+        queries, keys = self.qk_proj(\n+            queries, keys, query_id=seq_id, kv_id=seq_id)\n \n         scale = self.scale or 1. / sqrt(E)\n \n         var_id = repeat(torch.arange(n_vars),\n"
                },
                {
                    "date": 1730882760474,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -19,9 +19,15 @@\n     def forward(self, queries, keys, values, attn_mask, n_vars=None, n_tokens=None, tau=None, delta=None):\n         B, L, H, E = queries.shape\n         _, S, _, D = values.shape\n         scale = self.scale or 1. / sqrt(E)\n+        \n+        seq_id = repeat(torch.arange(n_tokens), 'n -> c n', c=n_vars).reshape(-1)\n+        seq_id = repeat(seq_id, 'n -> b h n', b=B, h=H)\n \n+        queries, keys = self.qk_proj(\n+            queries, keys, query_id=seq_id, kv_id=seq_id)\n+\n         scores = torch.einsum(\"blhe,bshe->bhls\", queries, keys)\n \n         if self.mask_flag:\n             if attn_mask is None:\n"
                },
                {
                    "date": 1730882809477,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -13,8 +13,10 @@\n         super(FullAttention, self).__init__()\n         self.scale = scale\n         self.mask_flag = mask_flag\n         self.output_attention = output_attention\n+        self.qk_proj = QueryKeyProjection(dim=d_model, num_heads=num_heads, proj_layer=RotaryProjection, kwargs=dict(max_len=max_len),\n+                                          partial_factor=(0.0, 0.5),)\n         self.dropout = nn.Dropout(attention_dropout)\n \n     def forward(self, queries, keys, values, attn_mask, n_vars=None, n_tokens=None, tau=None, delta=None):\n         B, L, H, E = queries.shape\n"
                },
                {
                    "date": 1730882841046,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -8,9 +8,9 @@\n from layers.Attn_Bias import BinaryAttentionBias\n \n \n class FullAttention(nn.Module):\n-    def __init__(self, mask_flag=True, scale=None, attention_dropout=0.1, output_attention=False):\n+    def __init__(self, mask_flag=True, scale=None, attention_dropout=0.1, output_attention=False, d_model=512, num_heads=8, max_len=100,):\n         super(FullAttention, self).__init__()\n         self.scale = scale\n         self.mask_flag = mask_flag\n         self.output_attention = output_attention\n"
                },
                {
                    "date": 1730882879068,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -22,13 +22,14 @@\n         B, L, H, E = queries.shape\n         _, S, _, D = values.shape\n         scale = self.scale or 1. / sqrt(E)\n         \n-        seq_id = repeat(torch.arange(n_tokens), 'n -> c n', c=n_vars).reshape(-1)\n-        seq_id = repeat(seq_id, 'n -> b h n', b=B, h=H)\n+        if n_vars is not None and n_tokens is not None:\n+            seq_id = repeat(torch.arange(n_tokens), 'n -> c n', c=n_vars).reshape(-1)\n+            seq_id = repeat(seq_id, 'n -> b h n', b=B, h=H)\n \n-        queries, keys = self.qk_proj(\n-            queries, keys, query_id=seq_id, kv_id=seq_id)\n+            queries, keys = self.qk_proj(\n+                queries, keys, query_id=seq_id, kv_id=seq_id)\n \n         scores = torch.einsum(\"blhe,bshe->bhls\", queries, keys)\n \n         if self.mask_flag:\n"
                },
                {
                    "date": 1730883389420,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -25,9 +25,9 @@\n         \n         if n_vars is not None and n_tokens is not None:\n             seq_id = repeat(torch.arange(n_tokens), 'n -> c n', c=n_vars).reshape(-1)\n             seq_id = repeat(seq_id, 'n -> b h n', b=B, h=H)\n-\n+            print(seq_id.shape, queries.shape, keys.shape)\n             queries, keys = self.qk_proj(\n                 queries, keys, query_id=seq_id, kv_id=seq_id)\n \n         scores = torch.einsum(\"blhe,bshe->bhls\", queries, keys)\n"
                },
                {
                    "date": 1730883964507,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -22,8 +22,11 @@\n         B, L, H, E = queries.shape\n         _, S, _, D = values.shape\n         scale = self.scale or 1. / sqrt(E)\n         \n+        queries = queries.permute(0, 2, 1, 3)\n+        keys = keys.permute(0, 2, 1, 3)\n+        \n         if n_vars is not None and n_tokens is not None:\n             seq_id = repeat(torch.arange(n_tokens), 'n -> c n', c=n_vars).reshape(-1)\n             seq_id = repeat(seq_id, 'n -> b h n', b=B, h=H)\n             print(seq_id.shape, queries.shape, keys.shape)\n"
                },
                {
                    "date": 1730883969968,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -32,9 +32,9 @@\n             print(seq_id.shape, queries.shape, keys.shape)\n             queries, keys = self.qk_proj(\n                 queries, keys, query_id=seq_id, kv_id=seq_id)\n \n-        scores = torch.einsum(\"blhe,bshe->bhls\", queries, keys)\n+        scores = torch.einsum(\"bhle,bhse->bhls\", queries, keys)\n \n         if self.mask_flag:\n             if attn_mask is None:\n                 attn_mask = TriangularCausalMask(B, L, device=queries.device)\n"
                },
                {
                    "date": 1730883978331,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -28,9 +28,8 @@\n         \n         if n_vars is not None and n_tokens is not None:\n             seq_id = repeat(torch.arange(n_tokens), 'n -> c n', c=n_vars).reshape(-1)\n             seq_id = repeat(seq_id, 'n -> b h n', b=B, h=H)\n-            print(seq_id.shape, queries.shape, keys.shape)\n             queries, keys = self.qk_proj(\n                 queries, keys, query_id=seq_id, kv_id=seq_id)\n \n         scores = torch.einsum(\"bhle,bhse->bhls\", queries, keys)\n"
                },
                {
                    "date": 1731048303020,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -5,8 +5,9 @@\n from utils.masking import TriangularCausalMask, TimerMultivariateMask, TimerCovariateMask\n from einops import repeat\n from layers.Attn_Projection import QueryKeyProjection, RotaryProjection\n from layers.Attn_Bias import BinaryAttentionBias\n+from transformers.models.llama.modeling_llama import apply_rotary_pos_emb, precompute_freqs_cis\n \n \n class FullAttention(nn.Module):\n     def __init__(self, mask_flag=True, scale=None, attention_dropout=0.1, output_attention=False, d_model=512, num_heads=8, max_len=100,):\n@@ -16,9 +17,30 @@\n         self.output_attention = output_attention\n         self.qk_proj = QueryKeyProjection(dim=d_model, num_heads=num_heads, proj_layer=RotaryProjection, kwargs=dict(max_len=max_len),\n                                           partial_factor=(0.0, 0.5),)\n         self.dropout = nn.Dropout(attention_dropout)\n+        \n+    def apply_rope(self, q, k, position_ids):\n+        \"\"\"\n+        Apply rotary position embeddings to query and key tensors.\n+        \n+        Args:\n+            q: Query tensor of shape (batch_size, seq_len, num_heads, head_dim)\n+            k: Key tensor of shape (batch_size, seq_len, num_heads, head_dim)\n+            position_ids: Position IDs tensor of shape (batch_size, seq_len)\n+            \n+        Returns:\n+            q_rot: Rotated query tensor\n+            k_rot: Rotated key tensor\n+        \"\"\"\n+        # Get the frequencies for the specified positions\n+        freqs = self.freqs_cis[position_ids]\n \n+        # Apply rotary embeddings\n+        q_rot, k_rot = apply_rotary_pos_emb(q, k, freqs)\n+        \n+        return q_rot, k_rot\n+\n     def forward(self, queries, keys, values, attn_mask, n_vars=None, n_tokens=None, tau=None, delta=None):\n         B, L, H, E = queries.shape\n         _, S, _, D = values.shape\n         scale = self.scale or 1. / sqrt(E)\n"
                },
                {
                    "date": 1731048321522,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -50,10 +50,11 @@\n         \n         if n_vars is not None and n_tokens is not None:\n             seq_id = repeat(torch.arange(n_tokens), 'n -> c n', c=n_vars).reshape(-1)\n             seq_id = repeat(seq_id, 'n -> b h n', b=B, h=H)\n-            queries, keys = self.qk_proj(\n-                queries, keys, query_id=seq_id, kv_id=seq_id)\n+            self.apply_rope(queries, keys, seq_id)\n+            # queries, keys = self.qk_proj(\n+            #     queries, keys, query_id=seq_id, kv_id=seq_id)\n \n         scores = torch.einsum(\"bhle,bhse->bhls\", queries, keys)\n \n         if self.mask_flag:\n"
                },
                {
                    "date": 1731048394020,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -5,9 +5,9 @@\n from utils.masking import TriangularCausalMask, TimerMultivariateMask, TimerCovariateMask\n from einops import repeat\n from layers.Attn_Projection import QueryKeyProjection, RotaryProjection\n from layers.Attn_Bias import BinaryAttentionBias\n-from transformers.models.llama.modeling_llama import apply_rotary_pos_emb, precompute_freqs_cis\n+from transformers.models.llama.modeling_llama import apply_rotary_pos_emb\n \n \n class FullAttention(nn.Module):\n     def __init__(self, mask_flag=True, scale=None, attention_dropout=0.1, output_attention=False, d_model=512, num_heads=8, max_len=100,):\n"
                },
                {
                    "date": 1731048437745,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -16,8 +16,14 @@\n         self.mask_flag = mask_flag\n         self.output_attention = output_attention\n         self.qk_proj = QueryKeyProjection(dim=d_model, num_heads=num_heads, proj_layer=RotaryProjection, kwargs=dict(max_len=max_len),\n                                           partial_factor=(0.0, 0.5),)\n+        self.freqs_cis = precompute_freqs_cis(\n+            self.head_dim,\n+            self.max_seq_len,\n+            theta=10000.0,\n+            device=self.model.device,\n+        )\n         self.dropout = nn.Dropout(attention_dropout)\n         \n     def apply_rope(self, q, k, position_ids):\n         \"\"\"\n"
                },
                {
                    "date": 1731048444777,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -5,9 +5,9 @@\n from utils.masking import TriangularCausalMask, TimerMultivariateMask, TimerCovariateMask\n from einops import repeat\n from layers.Attn_Projection import QueryKeyProjection, RotaryProjection\n from layers.Attn_Bias import BinaryAttentionBias\n-from transformers.models.llama.modeling_llama import apply_rotary_pos_emb\n+from transformers.models.llama.modeling_llama import apply_rotary_pos_emb,precompute_freqs_cis\n \n \n class FullAttention(nn.Module):\n     def __init__(self, mask_flag=True, scale=None, attention_dropout=0.1, output_attention=False, d_model=512, num_heads=8, max_len=100,):\n"
                },
                {
                    "date": 1731048590240,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -14,17 +14,24 @@\n         super(FullAttention, self).__init__()\n         self.scale = scale\n         self.mask_flag = mask_flag\n         self.output_attention = output_attention\n-        self.qk_proj = QueryKeyProjection(dim=d_model, num_heads=num_heads, proj_layer=RotaryProjection, kwargs=dict(max_len=max_len),\n-                                          partial_factor=(0.0, 0.5),)\n-        self.freqs_cis = precompute_freqs_cis(\n-            self.head_dim,\n-            self.max_seq_len,\n-            theta=10000.0,\n-            device=self.model.device,\n+        # self.qk_proj = QueryKeyProjection(dim=d_model, num_heads=num_heads, proj_layer=RotaryProjection, kwargs=dict(max_len=max_len),\n+        #                                   partial_factor=(0.0, 0.5),)\n+        self._set_cos_sin_cache(\n+            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n         )\n         self.dropout = nn.Dropout(attention_dropout)\n+    \n+    def _set_cos_sin_cache(self, seq_len, device, dtype):\n+        self.max_seq_len_cached = seq_len\n+        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n+\n+        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n+        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n+        emb = torch.cat((freqs, freqs), dim=-1)\n+        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :].to(dtype), persistent=False)\n+        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :].to(dtype), persistent=False)\n         \n     def apply_rope(self, q, k, position_ids):\n         \"\"\"\n         Apply rotary position embeddings to query and key tensors.\n"
                },
                {
                    "date": 1731048600881,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -17,9 +17,9 @@\n         self.output_attention = output_attention\n         # self.qk_proj = QueryKeyProjection(dim=d_model, num_heads=num_heads, proj_layer=RotaryProjection, kwargs=dict(max_len=max_len),\n         #                                   partial_factor=(0.0, 0.5),)\n         self._set_cos_sin_cache(\n-            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n+            seq_len=max_len, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n         )\n         self.dropout = nn.Dropout(attention_dropout)\n     \n     def _set_cos_sin_cache(self, seq_len, device, dtype):\n"
                },
                {
                    "date": 1731048652967,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -16,8 +16,9 @@\n         self.mask_flag = mask_flag\n         self.output_attention = output_attention\n         # self.qk_proj = QueryKeyProjection(dim=d_model, num_heads=num_heads, proj_layer=RotaryProjection, kwargs=dict(max_len=max_len),\n         #                                   partial_factor=(0.0, 0.5),)\n+        inv_freq = 1.0 / (10000 ** (torch.arange(0, self.dim, 2).float().to(self.modeldevice) / self.dim))\n         self._set_cos_sin_cache(\n             seq_len=max_len, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n         )\n         self.dropout = nn.Dropout(attention_dropout)\n"
                },
                {
                    "date": 1731048735773,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -16,11 +16,12 @@\n         self.mask_flag = mask_flag\n         self.output_attention = output_attention\n         # self.qk_proj = QueryKeyProjection(dim=d_model, num_heads=num_heads, proj_layer=RotaryProjection, kwargs=dict(max_len=max_len),\n         #                                   partial_factor=(0.0, 0.5),)\n-        inv_freq = 1.0 / (10000 ** (torch.arange(0, self.dim, 2).float().to(self.modeldevice) / self.dim))\n+        self.dim = d_model // num_heads\n+        inv_freq = 1.0 / (10000 ** (torch.arange(0, d_model, 2).float().to(self.model.device) / d_model))\n         self._set_cos_sin_cache(\n-            seq_len=max_len, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n+            seq_len=max_len, device=inv_freq.device, dtype=torch.get_default_dtype()\n         )\n         self.dropout = nn.Dropout(attention_dropout)\n     \n     def _set_cos_sin_cache(self, seq_len, device, dtype):\n"
                },
                {
                    "date": 1731048742609,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -17,9 +17,9 @@\n         self.output_attention = output_attention\n         # self.qk_proj = QueryKeyProjection(dim=d_model, num_heads=num_heads, proj_layer=RotaryProjection, kwargs=dict(max_len=max_len),\n         #                                   partial_factor=(0.0, 0.5),)\n         self.dim = d_model // num_heads\n-        inv_freq = 1.0 / (10000 ** (torch.arange(0, d_model, 2).float().to(self.model.device) / d_model))\n+        inv_freq = 1.0 / (10000 ** (torch.arange(0, self.dim, 2).float().to(self.model.device) / self.dim)\n         self._set_cos_sin_cache(\n             seq_len=max_len, device=inv_freq.device, dtype=torch.get_default_dtype()\n         )\n         self.dropout = nn.Dropout(attention_dropout)\n"
                },
                {
                    "date": 1731048749604,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -17,9 +17,9 @@\n         self.output_attention = output_attention\n         # self.qk_proj = QueryKeyProjection(dim=d_model, num_heads=num_heads, proj_layer=RotaryProjection, kwargs=dict(max_len=max_len),\n         #                                   partial_factor=(0.0, 0.5),)\n         self.dim = d_model // num_heads\n-        inv_freq = 1.0 / (10000 ** (torch.arange(0, self.dim, 2).float().to(self.model.device) / self.dim)\n+        inv_freq = 1.0 / (10000 ** (torch.arange(0, self.dim, 2).float().to(self.model.device) / self.dim))\n         self._set_cos_sin_cache(\n             seq_len=max_len, device=inv_freq.device, dtype=torch.get_default_dtype()\n         )\n         self.dropout = nn.Dropout(attention_dropout)\n"
                },
                {
                    "date": 1731048759994,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -5,9 +5,9 @@\n from utils.masking import TriangularCausalMask, TimerMultivariateMask, TimerCovariateMask\n from einops import repeat\n from layers.Attn_Projection import QueryKeyProjection, RotaryProjection\n from layers.Attn_Bias import BinaryAttentionBias\n-from transformers.models.llama.modeling_llama import apply_rotary_pos_emb,precompute_freqs_cis\n+from transformers.models.llama.modeling_llama import apply_rotary_pos_emb\n \n \n class FullAttention(nn.Module):\n     def __init__(self, mask_flag=True, scale=None, attention_dropout=0.1, output_attention=False, d_model=512, num_heads=8, max_len=100,):\n"
                },
                {
                    "date": 1731048774920,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -17,9 +17,9 @@\n         self.output_attention = output_attention\n         # self.qk_proj = QueryKeyProjection(dim=d_model, num_heads=num_heads, proj_layer=RotaryProjection, kwargs=dict(max_len=max_len),\n         #                                   partial_factor=(0.0, 0.5),)\n         self.dim = d_model // num_heads\n-        inv_freq = 1.0 / (10000 ** (torch.arange(0, self.dim, 2).float().to(self.model.device) / self.dim))\n+        inv_freq = 1.0 / (10000 ** (torch.arange(0, self.dim, 2).float().to(self..device) / self.dim))\n         self._set_cos_sin_cache(\n             seq_len=max_len, device=inv_freq.device, dtype=torch.get_default_dtype()\n         )\n         self.dropout = nn.Dropout(attention_dropout)\n"
                },
                {
                    "date": 1731048851334,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -17,11 +17,11 @@\n         self.output_attention = output_attention\n         # self.qk_proj = QueryKeyProjection(dim=d_model, num_heads=num_heads, proj_layer=RotaryProjection, kwargs=dict(max_len=max_len),\n         #                                   partial_factor=(0.0, 0.5),)\n         self.dim = d_model // num_heads\n-        inv_freq = 1.0 / (10000 ** (torch.arange(0, self.dim, 2).float().to(self..device) / self.dim))\n+        inv_freq = 1.0 / (10000 ** (torch.arange(0, self.dim, 2).float() / self.dim))\n         self._set_cos_sin_cache(\n-            seq_len=max_len, device=inv_freq.device, dtype=torch.get_default_dtype()\n+            seq_len=max_len, dtype=torch.get_default_dtype()\n         )\n         self.dropout = nn.Dropout(attention_dropout)\n     \n     def _set_cos_sin_cache(self, seq_len, device, dtype):\n"
                },
                {
                    "date": 1731048865866,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -17,9 +17,9 @@\n         self.output_attention = output_attention\n         # self.qk_proj = QueryKeyProjection(dim=d_model, num_heads=num_heads, proj_layer=RotaryProjection, kwargs=dict(max_len=max_len),\n         #                                   partial_factor=(0.0, 0.5),)\n         self.dim = d_model // num_heads\n-        inv_freq = 1.0 / (10000 ** (torch.arange(0, self.dim, 2).float() / self.dim))\n+        self.inv_freq = 1.0 / (10000 ** (torch.arange(0, self.dim, 2).float() / self.dim))\n         self._set_cos_sin_cache(\n             seq_len=max_len, dtype=torch.get_default_dtype()\n         )\n         self.dropout = nn.Dropout(attention_dropout)\n"
                },
                {
                    "date": 1731048892767,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -17,11 +17,11 @@\n         self.output_attention = output_attention\n         # self.qk_proj = QueryKeyProjection(dim=d_model, num_heads=num_heads, proj_layer=RotaryProjection, kwargs=dict(max_len=max_len),\n         #                                   partial_factor=(0.0, 0.5),)\n         self.dim = d_model // num_heads\n-        self.inv_freq = 1.0 / (10000 ** (torch.arange(0, self.dim, 2).float() / self.dim))\n+        self.inv_freq = 1.0 / (10000 ** (torch.arange(0, self.dim, 2).float().to(self.device) / self.dim))\n         self._set_cos_sin_cache(\n-            seq_len=max_len, dtype=torch.get_default_dtype()\n+            seq_len=max_len, device=inv_freq.device, dtype=torch.get_default_dtype()\n         )\n         self.dropout = nn.Dropout(attention_dropout)\n     \n     def _set_cos_sin_cache(self, seq_len, device, dtype):\n"
                },
                {
                    "date": 1731048906676,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -9,9 +9,9 @@\n from transformers.models.llama.modeling_llama import apply_rotary_pos_emb\n \n \n class FullAttention(nn.Module):\n-    def __init__(self, mask_flag=True, scale=None, attention_dropout=0.1, output_attention=False, d_model=512, num_heads=8, max_len=100,):\n+    def __init__(self, mask_flag=True, scale=None, attention_dropout=0.1, output_attention=False, d_model=512, num_heads=8, max_len=100, device=\"cuda\"):\n         super(FullAttention, self).__init__()\n         self.scale = scale\n         self.mask_flag = mask_flag\n         self.output_attention = output_attention\n"
                },
                {
                    "date": 1731048916325,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -17,9 +17,9 @@\n         self.output_attention = output_attention\n         # self.qk_proj = QueryKeyProjection(dim=d_model, num_heads=num_heads, proj_layer=RotaryProjection, kwargs=dict(max_len=max_len),\n         #                                   partial_factor=(0.0, 0.5),)\n         self.dim = d_model // num_heads\n-        self.inv_freq = 1.0 / (10000 ** (torch.arange(0, self.dim, 2).float().to(self.device) / self.dim))\n+        self.inv_freq = 1.0 / (10000 ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n         self._set_cos_sin_cache(\n             seq_len=max_len, device=inv_freq.device, dtype=torch.get_default_dtype()\n         )\n         self.dropout = nn.Dropout(attention_dropout)\n"
                },
                {
                    "date": 1731049092693,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -17,24 +17,35 @@\n         self.output_attention = output_attention\n         # self.qk_proj = QueryKeyProjection(dim=d_model, num_heads=num_heads, proj_layer=RotaryProjection, kwargs=dict(max_len=max_len),\n         #                                   partial_factor=(0.0, 0.5),)\n         self.dim = d_model // num_heads\n-        self.inv_freq = 1.0 / (10000 ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n-        self._set_cos_sin_cache(\n-            seq_len=max_len, device=inv_freq.device, dtype=torch.get_default_dtype()\n-        )\n+        self.freqs_cis = precompute_freqs_cis(self.dim, max_len)\n         self.dropout = nn.Dropout(attention_dropout)\n     \n-    def _set_cos_sin_cache(self, seq_len, device, dtype):\n-        self.max_seq_len_cached = seq_len\n-        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n-\n-        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n-        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n-        emb = torch.cat((freqs, freqs), dim=-1)\n-        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :].to(dtype), persistent=False)\n-        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :].to(dtype), persistent=False)\n+    def precompute_freqs_cis(self, dim: int, max_seq_len: int, theta: float = 10000.0):\n+        \"\"\"\n+        Precompute the frequency tensor for complex exponentials (cos/sin) for RoPE.\n         \n+        Args:\n+            dim: Dimension of the embedding vector (typically head_dim)\n+            max_seq_len: Maximum sequence length\n+            theta: Base value for frequencies (default: 10000.0)\n+        \"\"\"\n+        # Generate frequency bases\n+        freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n+        \n+        # Create position tensor\n+        t = torch.arange(max_seq_len, dtype=torch.float)\n+        \n+        # Compute frequencies\n+        freqs = torch.outer(t, freqs)\n+        \n+        # Convert to complex exponentials (cos + i*sin)\n+        freqs_cos = torch.cos(freqs)  # Real part\n+        freqs_sin = torch.sin(freqs)  # Imaginary part\n+        \n+        return torch.stack([freqs_cos, freqs_sin], dim=-1)\n+        \n     def apply_rope(self, q, k, position_ids):\n         \"\"\"\n         Apply rotary position embeddings to query and key tensors.\n         \n"
                },
                {
                    "date": 1731049101866,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -17,9 +17,9 @@\n         self.output_attention = output_attention\n         # self.qk_proj = QueryKeyProjection(dim=d_model, num_heads=num_heads, proj_layer=RotaryProjection, kwargs=dict(max_len=max_len),\n         #                                   partial_factor=(0.0, 0.5),)\n         self.dim = d_model // num_heads\n-        self.freqs_cis = precompute_freqs_cis(self.dim, max_len)\n+        self.freqs_cis = self.precompute_freqs_cis(self.dim, max_len).to(device)\n         self.dropout = nn.Dropout(attention_dropout)\n     \n     def precompute_freqs_cis(self, dim: int, max_seq_len: int, theta: float = 10000.0):\n         \"\"\"\n"
                },
                {
                    "date": 1731049170064,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -17,35 +17,24 @@\n         self.output_attention = output_attention\n         # self.qk_proj = QueryKeyProjection(dim=d_model, num_heads=num_heads, proj_layer=RotaryProjection, kwargs=dict(max_len=max_len),\n         #                                   partial_factor=(0.0, 0.5),)\n         self.dim = d_model // num_heads\n-        self.freqs_cis = self.precompute_freqs_cis(self.dim, max_len).to(device)\n+        self.inv_freq = 1.0 / (10000 ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n+        self._set_cos_sin_cache(\n+            seq_len=max_len, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n+        )\n         self.dropout = nn.Dropout(attention_dropout)\n     \n-    def precompute_freqs_cis(self, dim: int, max_seq_len: int, theta: float = 10000.0):\n-        \"\"\"\n-        Precompute the frequency tensor for complex exponentials (cos/sin) for RoPE.\n+    def _set_cos_sin_cache(self, seq_len, device, dtype):\n+        self.max_seq_len_cached = seq_len\n+        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n+\n+        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n+        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n+        emb = torch.cat((freqs, freqs), dim=-1)\n+        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :].to(dtype), persistent=False)\n+        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :].to(dtype), persistent=False)\n         \n-        Args:\n-            dim: Dimension of the embedding vector (typically head_dim)\n-            max_seq_len: Maximum sequence length\n-            theta: Base value for frequencies (default: 10000.0)\n-        \"\"\"\n-        # Generate frequency bases\n-        freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n-        \n-        # Create position tensor\n-        t = torch.arange(max_seq_len, dtype=torch.float)\n-        \n-        # Compute frequencies\n-        freqs = torch.outer(t, freqs)\n-        \n-        # Convert to complex exponentials (cos + i*sin)\n-        freqs_cos = torch.cos(freqs)  # Real part\n-        freqs_sin = torch.sin(freqs)  # Imaginary part\n-        \n-        return torch.stack([freqs_cos, freqs_sin], dim=-1)\n-        \n     def apply_rope(self, q, k, position_ids):\n         \"\"\"\n         Apply rotary position embeddings to query and key tensors.\n         \n"
                },
                {
                    "date": 1731049193907,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -47,12 +47,12 @@\n             q_rot: Rotated query tensor\n             k_rot: Rotated key tensor\n         \"\"\"\n         # Get the frequencies for the specified positions\n-        freqs = self.freqs_cis[position_ids]\n+        # freqs = self.freqs_cis[position_ids]\n \n         # Apply rotary embeddings\n-        q_rot, k_rot = apply_rotary_pos_emb(q, k, freqs)\n+        q_rot, k_rot = apply_rotary_pos_emb(q, k, self.cos_cahed, self.sin_cached, position_ids)\n         \n         return q_rot, k_rot\n \n     def forward(self, queries, keys, values, attn_mask, n_vars=None, n_tokens=None, tau=None, delta=None):\n"
                },
                {
                    "date": 1731049239950,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -50,9 +50,10 @@\n         # Get the frequencies for the specified positions\n         # freqs = self.freqs_cis[position_ids]\n \n         # Apply rotary embeddings\n-        q_rot, k_rot = apply_rotary_pos_emb(q, k, self.cos_cahed, self.sin_cached, position_ids)\n+        q_rot, k_rot = apply_rotary_pos_emb(q, k, self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n+            self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),, position_ids)\n         \n         return q_rot, k_rot\n \n     def forward(self, queries, keys, values, attn_mask, n_vars=None, n_tokens=None, tau=None, delta=None):\n"
                },
                {
                    "date": 1731049250834,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -50,10 +50,10 @@\n         # Get the frequencies for the specified positions\n         # freqs = self.freqs_cis[position_ids]\n \n         # Apply rotary embeddings\n-        q_rot, k_rot = apply_rotary_pos_emb(q, k, self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n-            self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),, position_ids)\n+        q_rot, k_rot = apply_rotary_pos_emb(q, k, self.cos_cached[:, :, :seq_len, ...].to(dtype=q.dtype),\n+            self.sin_cached[:, :, :seq_len, ...].to(dtype=q.dtype), position_ids)\n         \n         return q_rot, k_rot\n \n     def forward(self, queries, keys, values, attn_mask, n_vars=None, n_tokens=None, tau=None, delta=None):\n"
                },
                {
                    "date": 1731049269719,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -50,8 +50,9 @@\n         # Get the frequencies for the specified positions\n         # freqs = self.freqs_cis[position_ids]\n \n         # Apply rotary embeddings\n+        seq_len = q.shape[1]\n         q_rot, k_rot = apply_rotary_pos_emb(q, k, self.cos_cached[:, :, :seq_len, ...].to(dtype=q.dtype),\n             self.sin_cached[:, :, :seq_len, ...].to(dtype=q.dtype), position_ids)\n         \n         return q_rot, k_rot\n"
                },
                {
                    "date": 1731049301673,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -61,19 +61,19 @@\n         B, L, H, E = queries.shape\n         _, S, _, D = values.shape\n         scale = self.scale or 1. / sqrt(E)\n         \n-        queries = queries.permute(0, 2, 1, 3)\n-        keys = keys.permute(0, 2, 1, 3)\n+        # queries = queries.permute(0, 2, 1, 3)\n+        # keys = keys.permute(0, 2, 1, 3)\n         \n         if n_vars is not None and n_tokens is not None:\n             seq_id = repeat(torch.arange(n_tokens), 'n -> c n', c=n_vars).reshape(-1)\n             seq_id = repeat(seq_id, 'n -> b h n', b=B, h=H)\n             self.apply_rope(queries, keys, seq_id)\n             # queries, keys = self.qk_proj(\n             #     queries, keys, query_id=seq_id, kv_id=seq_id)\n \n-        scores = torch.einsum(\"bhle,bhse->bhls\", queries, keys)\n+        scores = torch.einsum(\"blhe,bshe->bhls\", queries, keys)\n \n         if self.mask_flag:\n             if attn_mask is None:\n                 attn_mask = TriangularCausalMask(B, L, device=queries.device)\n"
                },
                {
                    "date": 1731049349707,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -66,9 +66,9 @@\n         # keys = keys.permute(0, 2, 1, 3)\n         \n         if n_vars is not None and n_tokens is not None:\n             seq_id = repeat(torch.arange(n_tokens), 'n -> c n', c=n_vars).reshape(-1)\n-            seq_id = repeat(seq_id, 'n -> b h n', b=B, h=H)\n+            seq_id = repeat(seq_id, 'n -> b n', b=B)\n             self.apply_rope(queries, keys, seq_id)\n             # queries, keys = self.qk_proj(\n             #     queries, keys, query_id=seq_id, kv_id=seq_id)\n \n"
                },
                {
                    "date": 1731050320747,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -50,8 +50,9 @@\n         # Get the frequencies for the specified positions\n         # freqs = self.freqs_cis[position_ids]\n \n         # Apply rotary embeddings\n+        print(q.shape, k.shape, position_ids.shape)\n         seq_len = q.shape[1]\n         q_rot, k_rot = apply_rotary_pos_emb(q, k, self.cos_cached[:, :, :seq_len, ...].to(dtype=q.dtype),\n             self.sin_cached[:, :, :seq_len, ...].to(dtype=q.dtype), position_ids)\n         \n@@ -67,8 +68,9 @@\n         \n         if n_vars is not None and n_tokens is not None:\n             seq_id = repeat(torch.arange(n_tokens), 'n -> c n', c=n_vars).reshape(-1)\n             seq_id = repeat(seq_id, 'n -> b n', b=B)\n+            \n             self.apply_rope(queries, keys, seq_id)\n             # queries, keys = self.qk_proj(\n             #     queries, keys, query_id=seq_id, kv_id=seq_id)\n \n"
                },
                {
                    "date": 1731050349458,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -50,9 +50,9 @@\n         # Get the frequencies for the specified positions\n         # freqs = self.freqs_cis[position_ids]\n \n         # Apply rotary embeddings\n-        print(q.shape, k.shape, position_ids.shape)\n+        print(q.shape, k.shape, position_ids.shape, self.cos_cached.shape)\n         seq_len = q.shape[1]\n         q_rot, k_rot = apply_rotary_pos_emb(q, k, self.cos_cached[:, :, :seq_len, ...].to(dtype=q.dtype),\n             self.sin_cached[:, :, :seq_len, ...].to(dtype=q.dtype), position_ids)\n         \n"
                },
                {
                    "date": 1731050431957,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -30,10 +30,10 @@\n \n         freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n         # Different from paper, but it uses a different permutation in order to obtain the same calculation\n         emb = torch.cat((freqs, freqs), dim=-1)\n-        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :].to(dtype), persistent=False)\n-        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :].to(dtype), persistent=False)\n+        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n+        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n         \n     def apply_rope(self, q, k, position_ids):\n         \"\"\"\n         Apply rotary position embeddings to query and key tensors.\n"
                },
                {
                    "date": 1731050446711,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -52,10 +52,10 @@\n \n         # Apply rotary embeddings\n         print(q.shape, k.shape, position_ids.shape, self.cos_cached.shape)\n         seq_len = q.shape[1]\n-        q_rot, k_rot = apply_rotary_pos_emb(q, k, self.cos_cached[:, :, :seq_len, ...].to(dtype=q.dtype),\n-            self.sin_cached[:, :, :seq_len, ...].to(dtype=q.dtype), position_ids)\n+        q_rot, k_rot = apply_rotary_pos_emb(q, k, self.cos_cached[:seq_len].to(dtype=q.dtype),\n+            self.sin_cached[:seq_len].to(dtype=q.dtype), position_ids)\n         \n         return q_rot, k_rot\n \n     def forward(self, queries, keys, values, attn_mask, n_vars=None, n_tokens=None, tau=None, delta=None):\n"
                },
                {
                    "date": 1731050518017,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -50,9 +50,8 @@\n         # Get the frequencies for the specified positions\n         # freqs = self.freqs_cis[position_ids]\n \n         # Apply rotary embeddings\n-        print(q.shape, k.shape, position_ids.shape, self.cos_cached.shape)\n         seq_len = q.shape[1]\n         q_rot, k_rot = apply_rotary_pos_emb(q, k, self.cos_cached[:seq_len].to(dtype=q.dtype),\n             self.sin_cached[:seq_len].to(dtype=q.dtype), position_ids)\n         \n"
                },
                {
                    "date": 1731050761342,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -89,72 +89,8 @@\n         else:\n             return V.contiguous(), None\n \n \n-class HybridAttention(nn.Module):\n-    def __init__(self, mask_flag=True, scale=None, attention_dropout=0.1, output_attention=False, d_model=512, num_heads=8, max_len=100, covariate=False, flash_attention=False):\n-        super(HybridAttention, self).__init__()\n-        self.scale = scale\n-        self.mask_flag = mask_flag\n-        self.output_attention = output_attention\n-        self.dropout = nn.Dropout(attention_dropout)\n-        self.covariate = covariate\n-        self.flash_attention = flash_attention\n-        self.qk_proj = QueryKeyProjection(dim=d_model, num_heads=num_heads, proj_layer=RotaryProjection, kwargs=dict(max_len=max_len),\n-                                          partial_factor=(0.0, 0.5),)\n-        self.attn_bias = BinaryAttentionBias(dim=d_model, num_heads=num_heads)\n-\n-    def forward(self, queries, keys, values, attn_mask, n_vars, n_tokens, tau=None, delta=None):\n-        B, L, H, E = queries.shape\n-        _, S, _, D = values.shape\n-\n-        # [B, H, L, E]\n-        queries = queries.permute(0, 2, 1, 3)\n-        keys = keys.permute(0, 2, 1, 3)\n-        if self.flash_attention:\n-            values = values.permute(0, 2, 1, 3)\n-\n-        seq_id = torch.arange(n_tokens * n_vars)\n-        seq_id = repeat(seq_id, 'n -> b h n', b=B, h=H)\n-\n-        queries, keys = self.qk_proj(\n-            queries, keys, query_id=seq_id, kv_id=seq_id)\n-\n-        scale = self.scale or 1. / sqrt(E)\n-\n-        var_id = repeat(torch.arange(n_vars),\n-                        'C -> (C n_tokens)', n_tokens=n_tokens)\n-        var_id = repeat(var_id, 'L -> b h L', b=B, h=1).to(queries.device)\n-\n-        attn_bias = self.attn_bias(var_id, var_id)\n-\n-        if self.mask_flag:\n-            if attn_mask is None:\n-                if self.covariate:\n-                    attn_mask = TimerCovariateMask(\n-                        B, n_vars, n_tokens, device=queries.device)\n-                else:\n-                    attn_mask = TimerMultivariateMask(\n-                        B, n_vars, n_tokens, device=queries.device)\n-            attn_mask = attn_bias.masked_fill(attn_mask.mask, float(\"-inf\"))\n-        else:\n-            attn_mask = attn_bias\n-\n-        if self.flash_attention:\n-            V = torch.nn.functional.scaled_dot_product_attention(\n-                queries, keys, values, attn_mask)\n-        else:\n-            scores = torch.einsum(\"bhle,bhse->bhls\", queries, keys)\n-            scores += attn_mask\n-            \n-            A = self.dropout(torch.softmax(scale * scores, dim=-1))\n-            V = torch.einsum(\"bhls,bshd->blhd\", A, values)\n-\n-        if self.output_attention:\n-            return V.contiguous(), None\n-        else:\n-            return V.contiguous(), None\n-\n class TimeAttention(nn.Module):\n     def __init__(self, mask_flag=True, scale=None, attention_dropout=0.1, output_attention=False, d_model=512, num_heads=8, max_len=100, covariate=False, flash_attention=False):\n         super(TimeAttention, self).__init__()\n         self.scale = scale\n"
                },
                {
                    "date": 1731073057280,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -14,15 +14,15 @@\n         super(FullAttention, self).__init__()\n         self.scale = scale\n         self.mask_flag = mask_flag\n         self.output_attention = output_attention\n-        # self.qk_proj = QueryKeyProjection(dim=d_model, num_heads=num_heads, proj_layer=RotaryProjection, kwargs=dict(max_len=max_len),\n-        #                                   partial_factor=(0.0, 0.5),)\n-        self.dim = d_model // num_heads\n-        self.inv_freq = 1.0 / (10000 ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n-        self._set_cos_sin_cache(\n-            seq_len=max_len, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n-        )\n+        self.qk_proj = QueryKeyProjection(dim=d_model, num_heads=num_heads, proj_layer=RotaryProjection, kwargs=dict(max_len=max_len),\n+                                          partial_factor=(0.0, 0.5),)\n+        # self.dim = d_model // num_heads\n+        # self.inv_freq = 1.0 / (10000 ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n+        # self._set_cos_sin_cache(\n+        #     seq_len=max_len, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n+        # )\n         self.dropout = nn.Dropout(attention_dropout)\n     \n     def _set_cos_sin_cache(self, seq_len, device, dtype):\n         self.max_seq_len_cached = seq_len\n@@ -61,18 +61,18 @@\n         B, L, H, E = queries.shape\n         _, S, _, D = values.shape\n         scale = self.scale or 1. / sqrt(E)\n         \n-        # queries = queries.permute(0, 2, 1, 3)\n-        # keys = keys.permute(0, 2, 1, 3)\n+        queries = queries.permute(0, 2, 1, 3)\n+        keys = keys.permute(0, 2, 1, 3)\n         \n         if n_vars is not None and n_tokens is not None:\n             seq_id = repeat(torch.arange(n_tokens), 'n -> c n', c=n_vars).reshape(-1)\n             seq_id = repeat(seq_id, 'n -> b n', b=B)\n             \n-            self.apply_rope(queries, keys, seq_id)\n-            # queries, keys = self.qk_proj(\n-            #     queries, keys, query_id=seq_id, kv_id=seq_id)\n+            # self.apply_rope(queries, keys, seq_id)\n+            queries, keys = self.qk_proj(\n+                queries, keys, query_id=seq_id, kv_id=seq_id)\n \n         scores = torch.einsum(\"blhe,bshe->bhls\", queries, keys)\n \n         if self.mask_flag:\n"
                },
                {
                    "date": 1731073064350,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -72,9 +72,9 @@\n             # self.apply_rope(queries, keys, seq_id)\n             queries, keys = self.qk_proj(\n                 queries, keys, query_id=seq_id, kv_id=seq_id)\n \n-        scores = torch.einsum(\"blhe,bshe->bhls\", queries, keys)\n+        scores = torch.einsum(\"bhle,bhse->bhls\", queries, keys)\n \n         if self.mask_flag:\n             if attn_mask is None:\n                 attn_mask = TriangularCausalMask(B, L, device=queries.device)\n"
                },
                {
                    "date": 1731075518419,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -66,9 +66,9 @@\n         keys = keys.permute(0, 2, 1, 3)\n         \n         if n_vars is not None and n_tokens is not None:\n             seq_id = repeat(torch.arange(n_tokens), 'n -> c n', c=n_vars).reshape(-1)\n-            seq_id = repeat(seq_id, 'n -> b n', b=B)\n+            seq_id = repeat(seq_id, 'n -> b h n', b=B, h=H)\n             \n             # self.apply_rope(queries, keys, seq_id)\n             queries, keys = self.qk_proj(\n                 queries, keys, query_id=seq_id, kv_id=seq_id)\n"
                },
                {
                    "date": 1731149398829,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -64,15 +64,15 @@\n         \n         queries = queries.permute(0, 2, 1, 3)\n         keys = keys.permute(0, 2, 1, 3)\n         \n-        if n_vars is not None and n_tokens is not None:\n-            seq_id = repeat(torch.arange(n_tokens), 'n -> c n', c=n_vars).reshape(-1)\n-            seq_id = repeat(seq_id, 'n -> b h n', b=B, h=H)\n+        # if n_vars is not None and n_tokens is not None:\n+        #     seq_id = repeat(torch.arange(n_tokens), 'n -> c n', c=n_vars).reshape(-1)\n+        #     seq_id = repeat(seq_id, 'n -> b h n', b=B, h=H)\n             \n             # self.apply_rope(queries, keys, seq_id)\n-            queries, keys = self.qk_proj(\n-                queries, keys, query_id=seq_id, kv_id=seq_id)\n+            # queries, keys = self.qk_proj(\n+            #     queries, keys, query_id=seq_id, kv_id=seq_id)\n \n         scores = torch.einsum(\"bhle,bhse->bhls\", queries, keys)\n \n         if self.mask_flag:\n"
                },
                {
                    "date": 1731149410515,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -14,10 +14,10 @@\n         super(FullAttention, self).__init__()\n         self.scale = scale\n         self.mask_flag = mask_flag\n         self.output_attention = output_attention\n-        self.qk_proj = QueryKeyProjection(dim=d_model, num_heads=num_heads, proj_layer=RotaryProjection, kwargs=dict(max_len=max_len),\n-                                          partial_factor=(0.0, 0.5),)\n+        # self.qk_proj = QueryKeyProjection(dim=d_model, num_heads=num_heads, proj_layer=RotaryProjection, kwargs=dict(max_len=max_len),\n+        #                                   partial_factor=(0.0, 0.5),)\n         # self.dim = d_model // num_heads\n         # self.inv_freq = 1.0 / (10000 ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n         # self._set_cos_sin_cache(\n         #     seq_len=max_len, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n"
                },
                {
                    "date": 1731159612837,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -64,15 +64,15 @@\n         \n         queries = queries.permute(0, 2, 1, 3)\n         keys = keys.permute(0, 2, 1, 3)\n         \n-        # if n_vars is not None and n_tokens is not None:\n-        #     seq_id = repeat(torch.arange(n_tokens), 'n -> c n', c=n_vars).reshape(-1)\n-        #     seq_id = repeat(seq_id, 'n -> b h n', b=B, h=H)\n+        if n_vars is not None and n_tokens is not None:\n+            seq_id = repeat(torch.arange(n_tokens), 'n -> c n', c=n_vars).reshape(-1)\n+            seq_id = repeat(seq_id, 'n -> b h n', b=B, h=H)\n             \n-            # self.apply_rope(queries, keys, seq_id)\n-            # queries, keys = self.qk_proj(\n-            #     queries, keys, query_id=seq_id, kv_id=seq_id)\n+            self.apply_rope(queries, keys, seq_id)\n+            queries, keys = self.qk_proj(\n+                queries, keys, query_id=seq_id, kv_id=seq_id)\n \n         scores = torch.einsum(\"bhle,bhse->bhls\", queries, keys)\n \n         if self.mask_flag:\n"
                },
                {
                    "date": 1731159620245,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -14,10 +14,10 @@\n         super(FullAttention, self).__init__()\n         self.scale = scale\n         self.mask_flag = mask_flag\n         self.output_attention = output_attention\n-        # self.qk_proj = QueryKeyProjection(dim=d_model, num_heads=num_heads, proj_layer=RotaryProjection, kwargs=dict(max_len=max_len),\n-        #                                   partial_factor=(0.0, 0.5),)\n+        self.qk_proj = QueryKeyProjection(dim=d_model, num_heads=num_heads, proj_layer=RotaryProjection, kwargs=dict(max_len=max_len),\n+                                          partial_factor=(0.0, 0.5),)\n         # self.dim = d_model // num_heads\n         # self.inv_freq = 1.0 / (10000 ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n         # self._set_cos_sin_cache(\n         #     seq_len=max_len, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n"
                },
                {
                    "date": 1731160669769,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -68,9 +68,9 @@\n         if n_vars is not None and n_tokens is not None:\n             seq_id = repeat(torch.arange(n_tokens), 'n -> c n', c=n_vars).reshape(-1)\n             seq_id = repeat(seq_id, 'n -> b h n', b=B, h=H)\n             \n-            self.apply_rope(queries, keys, seq_id)\n+            # self.apply_rope(queries, keys, seq_id)\n             queries, keys = self.qk_proj(\n                 queries, keys, query_id=seq_id, kv_id=seq_id)\n \n         scores = torch.einsum(\"bhle,bhse->bhls\", queries, keys)\n"
                },
                {
                    "date": 1732714154258,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -112,10 +112,10 @@\n         keys = keys.permute(0, 2, 1, 3)\n         if self.flash_attention:\n             values = values.permute(0, 2, 1, 3)\n \n-        # seq_id = torch.arange(n_tokens * n_vars)\n-        seq_id = repeat(torch.arange(n_tokens), 'n -> c n', c=n_vars).reshape(-1)\n+        seq_id = torch.arange(n_tokens * n_vars)\n+        # seq_id = repeat(torch.arange(n_tokens), 'n -> c n', c=n_vars).reshape(-1)\n         seq_id = repeat(seq_id, 'n -> b h n', b=B, h=H)\n \n         queries, keys = self.qk_proj(\n             queries, keys, query_id=seq_id, kv_id=seq_id)\n"
                },
                {
                    "date": 1733051928468,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -55,9 +55,25 @@\n         q_rot, k_rot = apply_rotary_pos_emb(q, k, self.cos_cached[:seq_len].to(dtype=q.dtype),\n             self.sin_cached[:seq_len].to(dtype=q.dtype), position_ids)\n         \n         return q_rot, k_rot\n+    \n+    def get_slopes(n):\n+        def get_slopes_power_of_2(n):\n+            start = (2**(-2**-(math.log2(n)-3)))\n+            ratio = start\n+            return [start*ratio**i for i in range(n)]\n \n+        if math.log2(n).is_integer():\n+            return get_slopes_power_of_2(n)                   #In the paper, we only train models that have 2^a heads for some a. This function has\n+        else:                                                 #some good properties that only occur when the input is a power of 2. To maintain that even\n+            closest_power_of_2 = 2**math.floor(math.log2(n))  #when the number of heads is not a power of 2, we use this workaround. \n+            return get_slopes_power_of_2(closest_power_of_2) + get_slopes(2*closest_power_of_2)[0::2][:n-closest_power_of_2]\n+\n+    \n+    def alibi_2d(self, ):\n+        \n+\n     def forward(self, queries, keys, values, attn_mask, n_vars=None, n_tokens=None, tau=None, delta=None):\n         B, L, H, E = queries.shape\n         _, S, _, D = values.shape\n         scale = self.scale or 1. / sqrt(E)\n"
                },
                {
                    "date": 1733051985308,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,10 +6,23 @@\n from einops import repeat\n from layers.Attn_Projection import QueryKeyProjection, RotaryProjection\n from layers.Attn_Bias import BinaryAttentionBias\n from transformers.models.llama.modeling_llama import apply_rotary_pos_emb\n+import math\n \n+def get_slopes(n):\n+    def get_slopes_power_of_2(n):\n+        start = (2**(-2**-(math.log2(n)-3)))\n+        ratio = start\n+        return [start*ratio**i for i in range(n)]\n \n+    if math.log2(n).is_integer():\n+        return get_slopes_power_of_2(n)                   #In the paper, we only train models that have 2^a heads for some a. This function has\n+    else:                                                 #some good properties that only occur when the input is a power of 2. To maintain that even\n+        closest_power_of_2 = 2**math.floor(math.log2(n))  #when the number of heads is not a power of 2, we use this workaround. \n+        return get_slopes_power_of_2(closest_power_of_2) + get_slopes(2*closest_power_of_2)[0::2][:n-closest_power_of_2]\n+\n+\n class FullAttention(nn.Module):\n     def __init__(self, mask_flag=True, scale=None, attention_dropout=0.1, output_attention=False, d_model=512, num_heads=8, max_len=100, device=\"cuda\"):\n         super(FullAttention, self).__init__()\n         self.scale = scale\n@@ -56,20 +69,9 @@\n             self.sin_cached[:seq_len].to(dtype=q.dtype), position_ids)\n         \n         return q_rot, k_rot\n     \n-    def get_slopes(n):\n-        def get_slopes_power_of_2(n):\n-            start = (2**(-2**-(math.log2(n)-3)))\n-            ratio = start\n-            return [start*ratio**i for i in range(n)]\n \n-        if math.log2(n).is_integer():\n-            return get_slopes_power_of_2(n)                   #In the paper, we only train models that have 2^a heads for some a. This function has\n-        else:                                                 #some good properties that only occur when the input is a power of 2. To maintain that even\n-            closest_power_of_2 = 2**math.floor(math.log2(n))  #when the number of heads is not a power of 2, we use this workaround. \n-            return get_slopes_power_of_2(closest_power_of_2) + get_slopes(2*closest_power_of_2)[0::2][:n-closest_power_of_2]\n-\n     \n     def alibi_2d(self, ):\n         \n \n"
                },
                {
                    "date": 1733052013412,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -35,8 +35,9 @@\n         # self._set_cos_sin_cache(\n         #     seq_len=max_len, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n         # )\n         self.dropout = nn.Dropout(attention_dropout)\n+        self.alibi = get_slopes(num_heads)\n     \n     def _set_cos_sin_cache(self, seq_len, device, dtype):\n         self.max_seq_len_cached = seq_len\n         t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n"
                },
                {
                    "date": 1733052062107,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -8,21 +8,10 @@\n from layers.Attn_Bias import BinaryAttentionBias\n from transformers.models.llama.modeling_llama import apply_rotary_pos_emb\n import math\n \n-def get_slopes(n):\n-    def get_slopes_power_of_2(n):\n-        start = (2**(-2**-(math.log2(n)-3)))\n-        ratio = start\n-        return [start*ratio**i for i in range(n)]\n \n-    if math.log2(n).is_integer():\n-        return get_slopes_power_of_2(n)                   #In the paper, we only train models that have 2^a heads for some a. This function has\n-    else:                                                 #some good properties that only occur when the input is a power of 2. To maintain that even\n-        closest_power_of_2 = 2**math.floor(math.log2(n))  #when the number of heads is not a power of 2, we use this workaround. \n-        return get_slopes_power_of_2(closest_power_of_2) + get_slopes(2*closest_power_of_2)[0::2][:n-closest_power_of_2]\n \n-\n class FullAttention(nn.Module):\n     def __init__(self, mask_flag=True, scale=None, attention_dropout=0.1, output_attention=False, d_model=512, num_heads=8, max_len=100, device=\"cuda\"):\n         super(FullAttention, self).__init__()\n         self.scale = scale\n@@ -70,11 +59,8 @@\n             self.sin_cached[:seq_len].to(dtype=q.dtype), position_ids)\n         \n         return q_rot, k_rot\n     \n-\n-    \n-    def alibi_2d(self, ):\n         \n \n     def forward(self, queries, keys, values, attn_mask, n_vars=None, n_tokens=None, tau=None, delta=None):\n         B, L, H, E = queries.shape\n"
                },
                {
                    "date": 1733052157859,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,9 +6,8 @@\n from einops import repeat\n from layers.Attn_Projection import QueryKeyProjection, RotaryProjection\n from layers.Attn_Bias import BinaryAttentionBias\n from transformers.models.llama.modeling_llama import apply_rotary_pos_emb\n-import math\n \n \n \n class FullAttention(nn.Module):\n@@ -24,9 +23,8 @@\n         # self._set_cos_sin_cache(\n         #     seq_len=max_len, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n         # )\n         self.dropout = nn.Dropout(attention_dropout)\n-        self.alibi = get_slopes(num_heads)\n     \n     def _set_cos_sin_cache(self, seq_len, device, dtype):\n         self.max_seq_len_cached = seq_len\n         t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n"
                },
                {
                    "date": 1733058728159,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -57,10 +57,39 @@\n             self.sin_cached[:seq_len].to(dtype=q.dtype), position_ids)\n         \n         return q_rot, k_rot\n     \n-        \n+    def get_slopes(self,n):\n+        def get_slopes_power_of_2(n):\n+            start = (2**(-2**-(math.log2(n)-3)))\n+            ratio = start\n+            return [start*ratio**i for i in range(n)]\n \n+        if math.log2(n).is_integer():\n+            return get_slopes_power_of_2(n)                   #In the paper, we only train models that have 2^a heads for some a. This function has\n+        else:                                                 #some good properties that only occur when the input is a power of 2. To maintain that even\n+            closest_power_of_2 = 2**math.floor(math.log2(n))  #when the number of heads is not a power of 2, we use this workaround. \n+            return get_slopes_power_of_2(closest_power_of_2) + self.get_slopes(2*closest_power_of_2)[0::2][:n-closest_power_of_2]\n+\n+\n+    def get_relative_positions(self,seq_len: int) -> torch.tensor:\n+        x = torch.arange(seq_len)[None, :]\n+        y = torch.arange(seq_len)[:, None]\n+        return x - y\n+    \n+    def get_alibi_1d(self, seq_len: int):\n+        mask = self.slopes[:,None,None] * self.get_relative_positions(seq_len).unsqueeze(0)\n+        return torch.triu(mask).unsqueeze(0)\n+    \n+    def get_alibi_2d(self, n_vars: int, n_tokens: int):\n+        alibi_1d_intra = self.get_alibi_1d(n_tokens)\n+        alibi_1d_inter = torch.sqrt(1+alibi_1d_intra**2)\n+        intra_mask = torch.diag(torch.ones(n_tokens))\n+        inter_mask = torch.triu(torch.ones((n_tokens,n_tokens)))\n+        mask1 = torch.einsum(\"mui,vj->muvij\", alibi_1d_intra, intra_mask).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n+        mask2 = torch.einsum(\"mui,vj->muvij\", alibi_1d_inter, inter_mask).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n+        return (mask1+mask2).unsqueeze(0)\n+\n     def forward(self, queries, keys, values, attn_mask, n_vars=None, n_tokens=None, tau=None, delta=None):\n         B, L, H, E = queries.shape\n         _, S, _, D = values.shape\n         scale = self.scale or 1. / sqrt(E)\n"
                },
                {
                    "date": 1733058734515,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,11 +6,11 @@\n from einops import repeat\n from layers.Attn_Projection import QueryKeyProjection, RotaryProjection\n from layers.Attn_Bias import BinaryAttentionBias\n from transformers.models.llama.modeling_llama import apply_rotary_pos_emb\n+import math\n \n \n-\n class FullAttention(nn.Module):\n     def __init__(self, mask_flag=True, scale=None, attention_dropout=0.1, output_attention=False, d_model=512, num_heads=8, max_len=100, device=\"cuda\"):\n         super(FullAttention, self).__init__()\n         self.scale = scale\n"
                },
                {
                    "date": 1733058882125,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -101,10 +101,13 @@\n             seq_id = repeat(torch.arange(n_tokens), 'n -> c n', c=n_vars).reshape(-1)\n             seq_id = repeat(seq_id, 'n -> b h n', b=B, h=H)\n             \n             # self.apply_rope(queries, keys, seq_id)\n-            queries, keys = self.qk_proj(\n-                queries, keys, query_id=seq_id, kv_id=seq_id)\n+            # queries, keys = self.qk_proj(\n+            #     queries, keys, query_id=seq_id, kv_id=seq_id)\n+            alibi = self.get_alibi_2d(n_vars, n_tokens)\n+        else:\n+            alibi = self.get_alibi_1d(n_tokens)\n \n         scores = torch.einsum(\"bhle,bhse->bhls\", queries, keys)\n \n         if self.mask_flag:\n"
                },
                {
                    "date": 1733058892046,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -107,9 +107,9 @@\n             alibi = self.get_alibi_2d(n_vars, n_tokens)\n         else:\n             alibi = self.get_alibi_1d(n_tokens)\n \n-        scores = torch.einsum(\"bhle,bhse->bhls\", queries, keys)\n+        scores = torch.einsum(\"bhle,bhse->bhls\", queries, keys) + alibi\n \n         if self.mask_flag:\n             if attn_mask is None:\n                 attn_mask = TriangularCausalMask(B, L, device=queries.device)\n"
                },
                {
                    "date": 1733058940168,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -103,11 +103,11 @@\n             \n             # self.apply_rope(queries, keys, seq_id)\n             # queries, keys = self.qk_proj(\n             #     queries, keys, query_id=seq_id, kv_id=seq_id)\n-            alibi = self.get_alibi_2d(n_vars, n_tokens)\n+            alibi = self.get_alibi_2d(n_vars, n_tokens, queries.devices)\n         else:\n-            alibi = self.get_alibi_1d(n_tokens)\n+            alibi = self.get_alibi_1d(n_tokens, queries.devices)\n \n         scores = torch.einsum(\"bhle,bhse->bhls\", queries, keys) + alibi\n \n         if self.mask_flag:\n"
                },
                {
                    "date": 1733058949056,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -75,13 +75,13 @@\n         x = torch.arange(seq_len)[None, :]\n         y = torch.arange(seq_len)[:, None]\n         return x - y\n     \n-    def get_alibi_1d(self, seq_len: int):\n+    def get_alibi_1d(self, seq_len: int, device):\n         mask = self.slopes[:,None,None] * self.get_relative_positions(seq_len).unsqueeze(0)\n         return torch.triu(mask).unsqueeze(0)\n     \n-    def get_alibi_2d(self, n_vars: int, n_tokens: int):\n+    def get_alibi_2d(self, n_vars: int, n_tokens: int, device):\n         alibi_1d_intra = self.get_alibi_1d(n_tokens)\n         alibi_1d_inter = torch.sqrt(1+alibi_1d_intra**2)\n         intra_mask = torch.diag(torch.ones(n_tokens))\n         inter_mask = torch.triu(torch.ones((n_tokens,n_tokens)))\n"
                },
                {
                    "date": 1733058977951,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -23,8 +23,9 @@\n         # self._set_cos_sin_cache(\n         #     seq_len=max_len, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n         # )\n         self.dropout = nn.Dropout(attention_dropout)\n+        self.slopes = self.get_slopes(num_heads)\n     \n     def _set_cos_sin_cache(self, seq_len, device, dtype):\n         self.max_seq_len_cached = seq_len\n         t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n"
                },
                {
                    "date": 1733058990338,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -77,9 +77,9 @@\n         y = torch.arange(seq_len)[:, None]\n         return x - y\n     \n     def get_alibi_1d(self, seq_len: int, device):\n-        mask = self.slopes[:,None,None] * self.get_relative_positions(seq_len).unsqueeze(0)\n+        mask = self.slopes[:,None,None].to(device) * self.get_relative_positions(seq_len).unsqueeze(0)\n         return torch.triu(mask).unsqueeze(0)\n     \n     def get_alibi_2d(self, n_vars: int, n_tokens: int, device):\n         alibi_1d_intra = self.get_alibi_1d(n_tokens)\n"
                },
                {
                    "date": 1733059006035,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -71,15 +71,15 @@\n             closest_power_of_2 = 2**math.floor(math.log2(n))  #when the number of heads is not a power of 2, we use this workaround. \n             return get_slopes_power_of_2(closest_power_of_2) + self.get_slopes(2*closest_power_of_2)[0::2][:n-closest_power_of_2]\n \n \n-    def get_relative_positions(self,seq_len: int) -> torch.tensor:\n-        x = torch.arange(seq_len)[None, :]\n-        y = torch.arange(seq_len)[:, None]\n+    def get_relative_positions(self,seq_len: int,device) -> torch.tensor:\n+        x = torch.arange(seq_len)[None, :].to(device)\n+        y = torch.arange(seq_len)[:, None].to(device)\n         return x - y\n     \n     def get_alibi_1d(self, seq_len: int, device):\n-        mask = self.slopes[:,None,None].to(device) * self.get_relative_positions(seq_len).unsqueeze(0)\n+        mask = self.slopes[:,None,None].to(device) * self.get_relative_positions(seq_len,device).unsqueeze(0)\n         return torch.triu(mask).unsqueeze(0)\n     \n     def get_alibi_2d(self, n_vars: int, n_tokens: int, device):\n         alibi_1d_intra = self.get_alibi_1d(n_tokens)\n"
                },
                {
                    "date": 1733059018297,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -83,10 +83,10 @@\n     \n     def get_alibi_2d(self, n_vars: int, n_tokens: int, device):\n         alibi_1d_intra = self.get_alibi_1d(n_tokens)\n         alibi_1d_inter = torch.sqrt(1+alibi_1d_intra**2)\n-        intra_mask = torch.diag(torch.ones(n_tokens))\n-        inter_mask = torch.triu(torch.ones((n_tokens,n_tokens)))\n+        intra_mask = torch.diag(torch.ones(n_tokens)).to(device)\n+        inter_mask = torch.triu(torch.ones((n_tokens,n_tokens))).to(device)\n         mask1 = torch.einsum(\"mui,vj->muvij\", alibi_1d_intra, intra_mask).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n         mask2 = torch.einsum(\"mui,vj->muvij\", alibi_1d_inter, inter_mask).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n         return (mask1+mask2).unsqueeze(0)\n \n"
                },
                {
                    "date": 1733059052321,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -104,11 +104,11 @@\n             \n             # self.apply_rope(queries, keys, seq_id)\n             # queries, keys = self.qk_proj(\n             #     queries, keys, query_id=seq_id, kv_id=seq_id)\n-            alibi = self.get_alibi_2d(n_vars, n_tokens, queries.devices)\n+            alibi = self.get_alibi_2d(n_vars, n_tokens, queries.device)\n         else:\n-            alibi = self.get_alibi_1d(n_tokens, queries.devices)\n+            alibi = self.get_alibi_1d(n_tokens, queries.device)\n \n         scores = torch.einsum(\"bhle,bhse->bhls\", queries, keys) + alibi\n \n         if self.mask_flag:\n"
                },
                {
                    "date": 1733059189727,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -23,9 +23,9 @@\n         # self._set_cos_sin_cache(\n         #     seq_len=max_len, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n         # )\n         self.dropout = nn.Dropout(attention_dropout)\n-        self.slopes = self.get_slopes(num_heads)\n+        self.slopes = torch.Tensor(self.get_slopes(num_heads))\n     \n     def _set_cos_sin_cache(self, seq_len, device, dtype):\n         self.max_seq_len_cached = seq_len\n         t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n"
                },
                {
                    "date": 1733059458702,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -72,10 +72,10 @@\n             return get_slopes_power_of_2(closest_power_of_2) + self.get_slopes(2*closest_power_of_2)[0::2][:n-closest_power_of_2]\n \n \n     def get_relative_positions(self,seq_len: int,device) -> torch.tensor:\n-        x = torch.arange(seq_len)[None, :].to(device)\n-        y = torch.arange(seq_len)[:, None].to(device)\n+        x = torch.arange(0,seq_len)[None, :].to(device)\n+        y = torch.arange(0,seq_len)[:, None].to(device)\n         return x - y\n     \n     def get_alibi_1d(self, seq_len: int, device):\n         mask = self.slopes[:,None,None].to(device) * self.get_relative_positions(seq_len,device).unsqueeze(0)\n"
                },
                {
                    "date": 1733059529240,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -72,10 +72,10 @@\n             return get_slopes_power_of_2(closest_power_of_2) + self.get_slopes(2*closest_power_of_2)[0::2][:n-closest_power_of_2]\n \n \n     def get_relative_positions(self,seq_len: int,device) -> torch.tensor:\n-        x = torch.arange(0,seq_len)[None, :].to(device)\n-        y = torch.arange(0,seq_len)[:, None].to(device)\n+        x = torch.arange(seq_len)[None, :].to(device)\n+        y = torch.arange(seq_len)[:, None].to(device)\n         return x - y\n     \n     def get_alibi_1d(self, seq_len: int, device):\n         mask = self.slopes[:,None,None].to(device) * self.get_relative_positions(seq_len,device).unsqueeze(0)\n"
                },
                {
                    "date": 1733059560378,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -81,9 +81,9 @@\n         mask = self.slopes[:,None,None].to(device) * self.get_relative_positions(seq_len,device).unsqueeze(0)\n         return torch.triu(mask).unsqueeze(0)\n     \n     def get_alibi_2d(self, n_vars: int, n_tokens: int, device):\n-        alibi_1d_intra = self.get_alibi_1d(n_tokens)\n+        alibi_1d_intra = self.get_alibi_1d(n_tokens,device)\n         alibi_1d_inter = torch.sqrt(1+alibi_1d_intra**2)\n         intra_mask = torch.diag(torch.ones(n_tokens)).to(device)\n         inter_mask = torch.triu(torch.ones((n_tokens,n_tokens))).to(device)\n         mask1 = torch.einsum(\"mui,vj->muvij\", alibi_1d_intra, intra_mask).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n"
                },
                {
                    "date": 1733059698785,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -97,15 +97,16 @@\n         \n         queries = queries.permute(0, 2, 1, 3)\n         keys = keys.permute(0, 2, 1, 3)\n         \n-        if n_vars is not None and n_tokens is not None:\n-            seq_id = repeat(torch.arange(n_tokens), 'n -> c n', c=n_vars).reshape(-1)\n-            seq_id = repeat(seq_id, 'n -> b h n', b=B, h=H)\n+        # if n_vars is not None and n_tokens is not None:\n+            # seq_id = repeat(torch.arange(n_tokens), 'n -> c n', c=n_vars).reshape(-1)\n+            # seq_id = repeat(seq_id, 'n -> b h n', b=B, h=H)\n             \n             # self.apply_rope(queries, keys, seq_id)\n             # queries, keys = self.qk_proj(\n             #     queries, keys, query_id=seq_id, kv_id=seq_id)\n+        if n_vars>1:\n             alibi = self.get_alibi_2d(n_vars, n_tokens, queries.device)\n         else:\n             alibi = self.get_alibi_1d(n_tokens, queries.device)\n \n"
                },
                {
                    "date": 1733059820990,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -81,9 +81,9 @@\n         mask = self.slopes[:,None,None].to(device) * self.get_relative_positions(seq_len,device).unsqueeze(0)\n         return torch.triu(mask).unsqueeze(0)\n     \n     def get_alibi_2d(self, n_vars: int, n_tokens: int, device):\n-        alibi_1d_intra = self.get_alibi_1d(n_tokens,device)\n+        alibi_1d_intra = self.get_alibi_1d(n_tokens,device).squeeze(0)\n         alibi_1d_inter = torch.sqrt(1+alibi_1d_intra**2)\n         intra_mask = torch.diag(torch.ones(n_tokens)).to(device)\n         inter_mask = torch.triu(torch.ones((n_tokens,n_tokens))).to(device)\n         mask1 = torch.einsum(\"mui,vj->muvij\", alibi_1d_intra, intra_mask).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n"
                },
                {
                    "date": 1733059855281,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -23,8 +23,9 @@\n         # self._set_cos_sin_cache(\n         #     seq_len=max_len, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n         # )\n         self.dropout = nn.Dropout(attention_dropout)\n+        self.num_heads = num_heads\n         self.slopes = torch.Tensor(self.get_slopes(num_heads))\n     \n     def _set_cos_sin_cache(self, seq_len, device, dtype):\n         self.max_seq_len_cached = seq_len\n"
                },
                {
                    "date": 1733060033541,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -84,10 +84,10 @@\n     \n     def get_alibi_2d(self, n_vars: int, n_tokens: int, device):\n         alibi_1d_intra = self.get_alibi_1d(n_tokens,device).squeeze(0)\n         alibi_1d_inter = torch.sqrt(1+alibi_1d_intra**2)\n-        intra_mask = torch.diag(torch.ones(n_tokens)).to(device)\n-        inter_mask = torch.triu(torch.ones((n_tokens,n_tokens))).to(device)\n+        intra_mask = torch.diag(torch.ones(n_vars)).to(device)\n+        inter_mask = torch.triu(torch.ones((n_vars,n_vars))).to(device)\n         mask1 = torch.einsum(\"mui,vj->muvij\", alibi_1d_intra, intra_mask).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n         mask2 = torch.einsum(\"mui,vj->muvij\", alibi_1d_inter, inter_mask).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n         return (mask1+mask2).unsqueeze(0)\n \n"
                },
                {
                    "date": 1733060064218,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -86,10 +86,10 @@\n         alibi_1d_intra = self.get_alibi_1d(n_tokens,device).squeeze(0)\n         alibi_1d_inter = torch.sqrt(1+alibi_1d_intra**2)\n         intra_mask = torch.diag(torch.ones(n_vars)).to(device)\n         inter_mask = torch.triu(torch.ones((n_vars,n_vars))).to(device)\n-        mask1 = torch.einsum(\"mui,vj->muvij\", alibi_1d_intra, intra_mask).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n-        mask2 = torch.einsum(\"mui,vj->muvij\", alibi_1d_inter, inter_mask).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n+        mask1 = torch.einsum(\"mui,vj->muvij\", intra_mask, alibi_1d_intra).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n+        mask2 = torch.einsum(\"mui,vj->muvij\", inter_mask, alibi_1d_inter).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n         return (mask1+mask2).unsqueeze(0)\n \n     def forward(self, queries, keys, values, attn_mask, n_vars=None, n_tokens=None, tau=None, delta=None):\n         B, L, H, E = queries.shape\n"
                },
                {
                    "date": 1733060111012,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -86,10 +86,10 @@\n         alibi_1d_intra = self.get_alibi_1d(n_tokens,device).squeeze(0)\n         alibi_1d_inter = torch.sqrt(1+alibi_1d_intra**2)\n         intra_mask = torch.diag(torch.ones(n_vars)).to(device)\n         inter_mask = torch.triu(torch.ones((n_vars,n_vars))).to(device)\n-        mask1 = torch.einsum(\"mui,vj->muvij\", intra_mask, alibi_1d_intra).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n-        mask2 = torch.einsum(\"mui,vj->muvij\", inter_mask, alibi_1d_inter).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n+        mask1 = torch.einsum(\"ui,mvj->muvij\", intra_mask, alibi_1d_intra).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n+        mask2 = torch.einsum(\"ui,mvj->muvij\", inter_mask, alibi_1d_inter).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n         return (mask1+mask2).unsqueeze(0)\n \n     def forward(self, queries, keys, values, attn_mask, n_vars=None, n_tokens=None, tau=None, delta=None):\n         B, L, H, E = queries.shape\n"
                },
                {
                    "date": 1733060173252,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -84,10 +84,10 @@\n     \n     def get_alibi_2d(self, n_vars: int, n_tokens: int, device):\n         alibi_1d_intra = self.get_alibi_1d(n_tokens,device).squeeze(0)\n         alibi_1d_inter = torch.sqrt(1+alibi_1d_intra**2)\n-        intra_mask = torch.diag(torch.ones(n_vars)).to(device)\n-        inter_mask = torch.triu(torch.ones((n_vars,n_vars))).to(device)\n+        intra_mask = torch.diag(torch.ones(n_vars)).to(device).unsqueeze(0).repeat(self.num_heads,1,1)\n+        inter_mask = torch.triu(torch.ones((n_vars,n_vars))).to(device).unsqueeze(0).repeat(self.num_heads,1,1)\n         mask1 = torch.einsum(\"ui,mvj->muvij\", intra_mask, alibi_1d_intra).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n         mask2 = torch.einsum(\"ui,mvj->muvij\", inter_mask, alibi_1d_inter).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n         return (mask1+mask2).unsqueeze(0)\n \n"
                },
                {
                    "date": 1733060954674,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -79,17 +79,17 @@\n         return x - y\n     \n     def get_alibi_1d(self, seq_len: int, device):\n         mask = self.slopes[:,None,None].to(device) * self.get_relative_positions(seq_len,device).unsqueeze(0)\n-        return torch.triu(mask).unsqueeze(0)\n+        return torch.tril(mask).unsqueeze(0)\n     \n     def get_alibi_2d(self, n_vars: int, n_tokens: int, device):\n         alibi_1d_intra = self.get_alibi_1d(n_tokens,device).squeeze(0)\n         alibi_1d_inter = torch.sqrt(1+alibi_1d_intra**2)\n         intra_mask = torch.diag(torch.ones(n_vars)).to(device).unsqueeze(0).repeat(self.num_heads,1,1)\n-        inter_mask = torch.triu(torch.ones((n_vars,n_vars))).to(device).unsqueeze(0).repeat(self.num_heads,1,1)\n-        mask1 = torch.einsum(\"ui,mvj->muvij\", intra_mask, alibi_1d_intra).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n-        mask2 = torch.einsum(\"ui,mvj->muvij\", inter_mask, alibi_1d_inter).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n+        inter_mask = torch.tril(torch.ones((n_vars,n_vars)),diagnal=-1).to(device).unsqueeze(0).repeat(self.num_heads,1,1)\n+        mask1 = torch.einsum(\"mui,mvj->muvij\", intra_mask, alibi_1d_intra).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n+        mask2 = torch.einsum(\"mui,mvj->muvij\", inter_mask, alibi_1d_inter).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n         return (mask1+mask2).unsqueeze(0)\n \n     def forward(self, queries, keys, values, attn_mask, n_vars=None, n_tokens=None, tau=None, delta=None):\n         B, L, H, E = queries.shape\n"
                },
                {
                    "date": 1733060991647,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -85,9 +85,9 @@\n     def get_alibi_2d(self, n_vars: int, n_tokens: int, device):\n         alibi_1d_intra = self.get_alibi_1d(n_tokens,device).squeeze(0)\n         alibi_1d_inter = torch.sqrt(1+alibi_1d_intra**2)\n         intra_mask = torch.diag(torch.ones(n_vars)).to(device).unsqueeze(0).repeat(self.num_heads,1,1)\n-        inter_mask = torch.tril(torch.ones((n_vars,n_vars)),diagnal=-1).to(device).unsqueeze(0).repeat(self.num_heads,1,1)\n+        inter_mask = torch.tril(torch.ones((n_vars,n_vars)),diagonal=-1).to(device).unsqueeze(0).repeat(self.num_heads,1,1)\n         mask1 = torch.einsum(\"mui,mvj->muvij\", intra_mask, alibi_1d_intra).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n         mask2 = torch.einsum(\"mui,mvj->muvij\", inter_mask, alibi_1d_inter).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n         return (mask1+mask2).unsqueeze(0)\n \n"
                },
                {
                    "date": 1733108283610,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -72,11 +72,11 @@\n             closest_power_of_2 = 2**math.floor(math.log2(n))  #when the number of heads is not a power of 2, we use this workaround. \n             return get_slopes_power_of_2(closest_power_of_2) + self.get_slopes(2*closest_power_of_2)[0::2][:n-closest_power_of_2]\n \n \n-    def get_relative_positions(self,seq_len: int,device) -> torch.tensor:\n-        x = torch.arange(seq_len)[None, :].to(device)\n-        y = torch.arange(seq_len)[:, None].to(device)\n+    def get_relative_positions(self,pos_ids) -> torch.tensor:\n+        x = pos_ids[None, :]\n+        y = pos_ids[:, None]\n         return x - y\n     \n     def get_alibi_1d(self, seq_len: int, device):\n         mask = self.slopes[:,None,None].to(device) * self.get_relative_positions(seq_len,device).unsqueeze(0)\n"
                },
                {
                    "date": 1733108312866,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -77,10 +77,10 @@\n         x = pos_ids[None, :]\n         y = pos_ids[:, None]\n         return x - y\n     \n-    def get_alibi_1d(self, seq_len: int, device):\n-        mask = self.slopes[:,None,None].to(device) * self.get_relative_positions(seq_len,device).unsqueeze(0)\n+    def get_alibi_1d(self, pos_ids, device):\n+        mask = self.slopes[:,None,None].to(device) * self.get_relative_positions(pos_ids).unsqueeze(0)\n         return torch.tril(mask).unsqueeze(0)\n     \n     def get_alibi_2d(self, n_vars: int, n_tokens: int, device):\n         alibi_1d_intra = self.get_alibi_1d(n_tokens,device).squeeze(0)\n"
                },
                {
                    "date": 1733108356701,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -79,18 +79,18 @@\n         return x - y\n     \n     def get_alibi_1d(self, pos_ids, device):\n         mask = self.slopes[:,None,None].to(device) * self.get_relative_positions(pos_ids).unsqueeze(0)\n-        return torch.tril(mask).unsqueeze(0)\n+        return torch.tril(mask)\n     \n     def get_alibi_2d(self, n_vars: int, n_tokens: int, device):\n         alibi_1d_intra = self.get_alibi_1d(n_tokens,device).squeeze(0)\n         alibi_1d_inter = torch.sqrt(1+alibi_1d_intra**2)\n         intra_mask = torch.diag(torch.ones(n_vars)).to(device).unsqueeze(0).repeat(self.num_heads,1,1)\n         inter_mask = torch.tril(torch.ones((n_vars,n_vars)),diagonal=-1).to(device).unsqueeze(0).repeat(self.num_heads,1,1)\n         mask1 = torch.einsum(\"mui,mvj->muvij\", intra_mask, alibi_1d_intra).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n         mask2 = torch.einsum(\"mui,mvj->muvij\", inter_mask, alibi_1d_inter).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n-        return (mask1+mask2).unsqueeze(0)\n+        return (mask1+mask2)\n \n     def forward(self, queries, keys, values, attn_mask, n_vars=None, n_tokens=None, tau=None, delta=None):\n         B, L, H, E = queries.shape\n         _, S, _, D = values.shape\n@@ -106,11 +106,11 @@\n             # self.apply_rope(queries, keys, seq_id)\n             # queries, keys = self.qk_proj(\n             #     queries, keys, query_id=seq_id, kv_id=seq_id)\n         if n_vars>1:\n-            alibi = self.get_alibi_2d(n_vars, n_tokens, queries.device)\n+            alibi = self.get_alibi_2d(n_vars, n_tokens, queries.device).unsqueeze(0)\n         else:\n-            alibi = self.get_alibi_1d(n_tokens, queries.device)\n+            alibi = self.get_alibi_1d(n_tokens, queries.device).unsqueeze(0)\n \n         scores = torch.einsum(\"bhle,bhse->bhls\", queries, keys) + alibi\n \n         if self.mask_flag:\n"
                },
                {
                    "date": 1733108382435,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -81,18 +81,18 @@\n     def get_alibi_1d(self, pos_ids, device):\n         mask = self.slopes[:,None,None].to(device) * self.get_relative_positions(pos_ids).unsqueeze(0)\n         return torch.tril(mask)\n     \n-    def get_alibi_2d(self, n_vars: int, n_tokens: int, device):\n+    def get_alibi_2d(self, n_vars: int, pos_ids: int, device):\n         alibi_1d_intra = self.get_alibi_1d(n_tokens,device).squeeze(0)\n         alibi_1d_inter = torch.sqrt(1+alibi_1d_intra**2)\n         intra_mask = torch.diag(torch.ones(n_vars)).to(device).unsqueeze(0).repeat(self.num_heads,1,1)\n         inter_mask = torch.tril(torch.ones((n_vars,n_vars)),diagonal=-1).to(device).unsqueeze(0).repeat(self.num_heads,1,1)\n         mask1 = torch.einsum(\"mui,mvj->muvij\", intra_mask, alibi_1d_intra).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n         mask2 = torch.einsum(\"mui,mvj->muvij\", inter_mask, alibi_1d_inter).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n         return (mask1+mask2)\n \n-    def forward(self, queries, keys, values, attn_mask, n_vars=None, n_tokens=None, tau=None, delta=None):\n+    def forward(self, queries, keys, values, attn_mask, n_vars=None, pos_ids=None, tau=None, delta=None):\n         B, L, H, E = queries.shape\n         _, S, _, D = values.shape\n         scale = self.scale or 1. / sqrt(E)\n         \n@@ -106,11 +106,11 @@\n             # self.apply_rope(queries, keys, seq_id)\n             # queries, keys = self.qk_proj(\n             #     queries, keys, query_id=seq_id, kv_id=seq_id)\n         if n_vars>1:\n-            alibi = self.get_alibi_2d(n_vars, n_tokens, queries.device).unsqueeze(0)\n+            alibi = self.get_alibi_2d(n_vars, pos_ids, queries.device).unsqueeze(0)\n         else:\n-            alibi = self.get_alibi_1d(n_tokens, queries.device).unsqueeze(0)\n+            alibi = self.get_alibi_1d(pos_ids, queries.device).unsqueeze(0)\n \n         scores = torch.einsum(\"bhle,bhse->bhls\", queries, keys) + alibi\n \n         if self.mask_flag:\n"
                },
                {
                    "date": 1733108397041,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -82,9 +82,9 @@\n         mask = self.slopes[:,None,None].to(device) * self.get_relative_positions(pos_ids).unsqueeze(0)\n         return torch.tril(mask)\n     \n     def get_alibi_2d(self, n_vars: int, pos_ids: int, device):\n-        alibi_1d_intra = self.get_alibi_1d(n_tokens,device).squeeze(0)\n+        alibi_1d_intra = self.get_alibi_1d(pos_ids,device)\n         alibi_1d_inter = torch.sqrt(1+alibi_1d_intra**2)\n         intra_mask = torch.diag(torch.ones(n_vars)).to(device).unsqueeze(0).repeat(self.num_heads,1,1)\n         inter_mask = torch.tril(torch.ones((n_vars,n_vars)),diagonal=-1).to(device).unsqueeze(0).repeat(self.num_heads,1,1)\n         mask1 = torch.einsum(\"mui,mvj->muvij\", intra_mask, alibi_1d_intra).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n"
                },
                {
                    "date": 1733108417316,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -82,8 +82,9 @@\n         mask = self.slopes[:,None,None].to(device) * self.get_relative_positions(pos_ids).unsqueeze(0)\n         return torch.tril(mask)\n     \n     def get_alibi_2d(self, n_vars: int, pos_ids: int, device):\n+        n_tokens = pos_ids.shape[0]\n         alibi_1d_intra = self.get_alibi_1d(pos_ids,device)\n         alibi_1d_inter = torch.sqrt(1+alibi_1d_intra**2)\n         intra_mask = torch.diag(torch.ones(n_vars)).to(device).unsqueeze(0).repeat(self.num_heads,1,1)\n         inter_mask = torch.tril(torch.ones((n_vars,n_vars)),diagonal=-1).to(device).unsqueeze(0).repeat(self.num_heads,1,1)\n"
                },
                {
                    "date": 1733109292831,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -223,9 +223,9 @@\n             keys,\n             values,\n             attn_mask,\n             n_vars=n_vars,\n-            n_tokens=n_tokens,\n+            pos_ids=n_tokens,\n             tau=tau,\n             delta=delta\n         )\n         out = out.view(B, L, -1)\n"
                },
                {
                    "date": 1733122633136,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -84,9 +84,9 @@\n     \n     def get_alibi_2d(self, n_vars: int, pos_ids: int, device):\n         n_tokens = pos_ids.shape[0]\n         alibi_1d_intra = self.get_alibi_1d(pos_ids,device)\n-        alibi_1d_inter = torch.sqrt(1+alibi_1d_intra**2)\n+        alibi_1d_inter = torch.sqrt(0.5+alibi_1d_intra**2)\n         intra_mask = torch.diag(torch.ones(n_vars)).to(device).unsqueeze(0).repeat(self.num_heads,1,1)\n         inter_mask = torch.tril(torch.ones((n_vars,n_vars)),diagonal=-1).to(device).unsqueeze(0).repeat(self.num_heads,1,1)\n         mask1 = torch.einsum(\"mui,mvj->muvij\", intra_mask, alibi_1d_intra).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n         mask2 = torch.einsum(\"mui,mvj->muvij\", inter_mask, alibi_1d_inter).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n"
                },
                {
                    "date": 1733132656204,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -84,9 +84,9 @@\n     \n     def get_alibi_2d(self, n_vars: int, pos_ids: int, device):\n         n_tokens = pos_ids.shape[0]\n         alibi_1d_intra = self.get_alibi_1d(pos_ids,device)\n-        alibi_1d_inter = torch.sqrt(0.5+alibi_1d_intra**2)\n+        alibi_1d_inter = torch.sqrt(1+self.alpha+alibi_1d_intra**2)\n         intra_mask = torch.diag(torch.ones(n_vars)).to(device).unsqueeze(0).repeat(self.num_heads,1,1)\n         inter_mask = torch.tril(torch.ones((n_vars,n_vars)),diagonal=-1).to(device).unsqueeze(0).repeat(self.num_heads,1,1)\n         mask1 = torch.einsum(\"mui,mvj->muvij\", intra_mask, alibi_1d_intra).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n         mask2 = torch.einsum(\"mui,mvj->muvij\", inter_mask, alibi_1d_inter).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n"
                },
                {
                    "date": 1733132686267,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -25,8 +25,9 @@\n         # )\n         self.dropout = nn.Dropout(attention_dropout)\n         self.num_heads = num_heads\n         self.slopes = torch.Tensor(self.get_slopes(num_heads))\n+        self.alpha = nn.Parameter(torch.randn(1))\n     \n     def _set_cos_sin_cache(self, seq_len, device, dtype):\n         self.max_seq_len_cached = seq_len\n         t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n"
                },
                {
                    "date": 1733132763639,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -85,9 +85,9 @@\n     \n     def get_alibi_2d(self, n_vars: int, pos_ids: int, device):\n         n_tokens = pos_ids.shape[0]\n         alibi_1d_intra = self.get_alibi_1d(pos_ids,device)\n-        alibi_1d_inter = torch.sqrt(1+self.alpha+alibi_1d_intra**2)\n+        alibi_1d_inter = torch.sqrt((1+self.alpha)**2+alibi_1d_intra**2)\n         intra_mask = torch.diag(torch.ones(n_vars)).to(device).unsqueeze(0).repeat(self.num_heads,1,1)\n         inter_mask = torch.tril(torch.ones((n_vars,n_vars)),diagonal=-1).to(device).unsqueeze(0).repeat(self.num_heads,1,1)\n         mask1 = torch.einsum(\"mui,mvj->muvij\", intra_mask, alibi_1d_intra).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n         mask2 = torch.einsum(\"mui,mvj->muvij\", inter_mask, alibi_1d_inter).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n"
                },
                {
                    "date": 1733132776088,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -85,8 +85,9 @@\n     \n     def get_alibi_2d(self, n_vars: int, pos_ids: int, device):\n         n_tokens = pos_ids.shape[0]\n         alibi_1d_intra = self.get_alibi_1d(pos_ids,device)\n+        print(self.alpha)\n         alibi_1d_inter = torch.sqrt((1+self.alpha)**2+alibi_1d_intra**2)\n         intra_mask = torch.diag(torch.ones(n_vars)).to(device).unsqueeze(0).repeat(self.num_heads,1,1)\n         inter_mask = torch.tril(torch.ones((n_vars,n_vars)),diagonal=-1).to(device).unsqueeze(0).repeat(self.num_heads,1,1)\n         mask1 = torch.einsum(\"mui,mvj->muvij\", intra_mask, alibi_1d_intra).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n"
                },
                {
                    "date": 1733132860796,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -86,9 +86,9 @@\n     def get_alibi_2d(self, n_vars: int, pos_ids: int, device):\n         n_tokens = pos_ids.shape[0]\n         alibi_1d_intra = self.get_alibi_1d(pos_ids,device)\n         print(self.alpha)\n-        alibi_1d_inter = torch.sqrt((1+self.alpha)**2+alibi_1d_intra**2)\n+        alibi_1d_inter = torch.sqrt(self.alpha+alibi_1d_intra**2)\n         intra_mask = torch.diag(torch.ones(n_vars)).to(device).unsqueeze(0).repeat(self.num_heads,1,1)\n         inter_mask = torch.tril(torch.ones((n_vars,n_vars)),diagonal=-1).to(device).unsqueeze(0).repeat(self.num_heads,1,1)\n         mask1 = torch.einsum(\"mui,mvj->muvij\", intra_mask, alibi_1d_intra).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n         mask2 = torch.einsum(\"mui,mvj->muvij\", inter_mask, alibi_1d_inter).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n"
                },
                {
                    "date": 1733132871948,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -25,9 +25,9 @@\n         # )\n         self.dropout = nn.Dropout(attention_dropout)\n         self.num_heads = num_heads\n         self.slopes = torch.Tensor(self.get_slopes(num_heads))\n-        self.alpha = nn.Parameter(torch.randn(1))\n+        self.alpha = nn.Parameter(torch.ones(1))\n     \n     def _set_cos_sin_cache(self, seq_len, device, dtype):\n         self.max_seq_len_cached = seq_len\n         t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n"
                },
                {
                    "date": 1733132959299,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -25,9 +25,9 @@\n         # )\n         self.dropout = nn.Dropout(attention_dropout)\n         self.num_heads = num_heads\n         self.slopes = torch.Tensor(self.get_slopes(num_heads))\n-        self.alpha = nn.Parameter(torch.ones(1))\n+        self.alpha = nn.Parameter(torch.ones(1)*1.5)\n     \n     def _set_cos_sin_cache(self, seq_len, device, dtype):\n         self.max_seq_len_cached = seq_len\n         t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n"
                },
                {
                    "date": 1733132992449,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -25,9 +25,9 @@\n         # )\n         self.dropout = nn.Dropout(attention_dropout)\n         self.num_heads = num_heads\n         self.slopes = torch.Tensor(self.get_slopes(num_heads))\n-        self.alpha = nn.Parameter(torch.ones(1)*1.5)\n+        self.alpha = nn.Parameter(torch.ones(1)*1.1)\n     \n     def _set_cos_sin_cache(self, seq_len, device, dtype):\n         self.max_seq_len_cached = seq_len\n         t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n"
                },
                {
                    "date": 1733133039644,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -85,9 +85,8 @@\n     \n     def get_alibi_2d(self, n_vars: int, pos_ids: int, device):\n         n_tokens = pos_ids.shape[0]\n         alibi_1d_intra = self.get_alibi_1d(pos_ids,device)\n-        print(self.alpha)\n         alibi_1d_inter = torch.sqrt(self.alpha+alibi_1d_intra**2)\n         intra_mask = torch.diag(torch.ones(n_vars)).to(device).unsqueeze(0).repeat(self.num_heads,1,1)\n         inter_mask = torch.tril(torch.ones((n_vars,n_vars)),diagonal=-1).to(device).unsqueeze(0).repeat(self.num_heads,1,1)\n         mask1 = torch.einsum(\"mui,mvj->muvij\", intra_mask, alibi_1d_intra).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n"
                },
                {
                    "date": 1733133214394,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -25,9 +25,9 @@\n         # )\n         self.dropout = nn.Dropout(attention_dropout)\n         self.num_heads = num_heads\n         self.slopes = torch.Tensor(self.get_slopes(num_heads))\n-        self.alpha = nn.Parameter(torch.ones(1)*1.1)\n+        self.alpha = nn.Parameter(torch.randn(1))\n     \n     def _set_cos_sin_cache(self, seq_len, device, dtype):\n         self.max_seq_len_cached = seq_len\n         t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n"
                },
                {
                    "date": 1733133244216,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -25,9 +25,9 @@\n         # )\n         self.dropout = nn.Dropout(attention_dropout)\n         self.num_heads = num_heads\n         self.slopes = torch.Tensor(self.get_slopes(num_heads))\n-        self.alpha = nn.Parameter(torch.randn(1))\n+        self.alpha = nn.Parameter(torch.abs(torch.randn(1)))\n     \n     def _set_cos_sin_cache(self, seq_len, device, dtype):\n         self.max_seq_len_cached = seq_len\n         t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n"
                },
                {
                    "date": 1733133266929,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -85,9 +85,9 @@\n     \n     def get_alibi_2d(self, n_vars: int, pos_ids: int, device):\n         n_tokens = pos_ids.shape[0]\n         alibi_1d_intra = self.get_alibi_1d(pos_ids,device)\n-        alibi_1d_inter = torch.sqrt(self.alpha+alibi_1d_intra**2)\n+        alibi_1d_inter = torch.sqrt((self.alpha)**2+alibi_1d_intra**2)\n         intra_mask = torch.diag(torch.ones(n_vars)).to(device).unsqueeze(0).repeat(self.num_heads,1,1)\n         inter_mask = torch.tril(torch.ones((n_vars,n_vars)),diagonal=-1).to(device).unsqueeze(0).repeat(self.num_heads,1,1)\n         mask1 = torch.einsum(\"mui,mvj->muvij\", intra_mask, alibi_1d_intra).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n         mask2 = torch.einsum(\"mui,mvj->muvij\", inter_mask, alibi_1d_inter).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n"
                },
                {
                    "date": 1733133273648,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -85,9 +85,9 @@\n     \n     def get_alibi_2d(self, n_vars: int, pos_ids: int, device):\n         n_tokens = pos_ids.shape[0]\n         alibi_1d_intra = self.get_alibi_1d(pos_ids,device)\n-        alibi_1d_inter = torch.sqrt((self.alpha)**2+alibi_1d_intra**2)\n+        alibi_1d_inter = torch.sqrt(torch.abs(self.alpha)**2+alibi_1d_intra**2)\n         intra_mask = torch.diag(torch.ones(n_vars)).to(device).unsqueeze(0).repeat(self.num_heads,1,1)\n         inter_mask = torch.tril(torch.ones((n_vars,n_vars)),diagonal=-1).to(device).unsqueeze(0).repeat(self.num_heads,1,1)\n         mask1 = torch.einsum(\"mui,mvj->muvij\", intra_mask, alibi_1d_intra).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n         mask2 = torch.einsum(\"mui,mvj->muvij\", inter_mask, alibi_1d_inter).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n"
                },
                {
                    "date": 1733133279910,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -85,8 +85,9 @@\n     \n     def get_alibi_2d(self, n_vars: int, pos_ids: int, device):\n         n_tokens = pos_ids.shape[0]\n         alibi_1d_intra = self.get_alibi_1d(pos_ids,device)\n+        print(self.alpha)\n         alibi_1d_inter = torch.sqrt(torch.abs(self.alpha)**2+alibi_1d_intra**2)\n         intra_mask = torch.diag(torch.ones(n_vars)).to(device).unsqueeze(0).repeat(self.num_heads,1,1)\n         inter_mask = torch.tril(torch.ones((n_vars,n_vars)),diagonal=-1).to(device).unsqueeze(0).repeat(self.num_heads,1,1)\n         mask1 = torch.einsum(\"mui,mvj->muvij\", intra_mask, alibi_1d_intra).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n"
                },
                {
                    "date": 1733133371386,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -85,9 +85,8 @@\n     \n     def get_alibi_2d(self, n_vars: int, pos_ids: int, device):\n         n_tokens = pos_ids.shape[0]\n         alibi_1d_intra = self.get_alibi_1d(pos_ids,device)\n-        print(self.alpha)\n         alibi_1d_inter = torch.sqrt(torch.abs(self.alpha)**2+alibi_1d_intra**2)\n         intra_mask = torch.diag(torch.ones(n_vars)).to(device).unsqueeze(0).repeat(self.num_heads,1,1)\n         inter_mask = torch.tril(torch.ones((n_vars,n_vars)),diagonal=-1).to(device).unsqueeze(0).repeat(self.num_heads,1,1)\n         mask1 = torch.einsum(\"mui,mvj->muvij\", intra_mask, alibi_1d_intra).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n"
                },
                {
                    "date": 1733133397978,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -85,9 +85,9 @@\n     \n     def get_alibi_2d(self, n_vars: int, pos_ids: int, device):\n         n_tokens = pos_ids.shape[0]\n         alibi_1d_intra = self.get_alibi_1d(pos_ids,device)\n-        alibi_1d_inter = torch.sqrt(torch.abs(self.alpha)**2+alibi_1d_intra**2)\n+        alibi_1d_inter = torch.sqrt((self.alpha)**2+alibi_1d_intra**2)\n         intra_mask = torch.diag(torch.ones(n_vars)).to(device).unsqueeze(0).repeat(self.num_heads,1,1)\n         inter_mask = torch.tril(torch.ones((n_vars,n_vars)),diagonal=-1).to(device).unsqueeze(0).repeat(self.num_heads,1,1)\n         mask1 = torch.einsum(\"mui,mvj->muvij\", intra_mask, alibi_1d_intra).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n         mask2 = torch.einsum(\"mui,mvj->muvij\", inter_mask, alibi_1d_inter).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n"
                },
                {
                    "date": 1733148741847,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -85,9 +85,9 @@\n     \n     def get_alibi_2d(self, n_vars: int, pos_ids: int, device):\n         n_tokens = pos_ids.shape[0]\n         alibi_1d_intra = self.get_alibi_1d(pos_ids,device)\n-        alibi_1d_inter = torch.sqrt((self.alpha)**2+alibi_1d_intra**2)\n+        alibi_1d_inter = torch.sqrt(1.1+alibi_1d_intra**2)\n         intra_mask = torch.diag(torch.ones(n_vars)).to(device).unsqueeze(0).repeat(self.num_heads,1,1)\n         inter_mask = torch.tril(torch.ones((n_vars,n_vars)),diagonal=-1).to(device).unsqueeze(0).repeat(self.num_heads,1,1)\n         mask1 = torch.einsum(\"mui,mvj->muvij\", intra_mask, alibi_1d_intra).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n         mask2 = torch.einsum(\"mui,mvj->muvij\", inter_mask, alibi_1d_inter).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n"
                },
                {
                    "date": 1733148752108,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -25,9 +25,9 @@\n         # )\n         self.dropout = nn.Dropout(attention_dropout)\n         self.num_heads = num_heads\n         self.slopes = torch.Tensor(self.get_slopes(num_heads))\n-        self.alpha = nn.Parameter(torch.abs(torch.randn(1)))\n+        # self.alpha = nn.Parameter(torch.abs(torch.randn(1)))\n     \n     def _set_cos_sin_cache(self, seq_len, device, dtype):\n         self.max_seq_len_cached = seq_len\n         t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n"
                },
                {
                    "date": 1733305479136,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -92,29 +92,29 @@\n         mask1 = torch.einsum(\"mui,mvj->muvij\", intra_mask, alibi_1d_intra).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n         mask2 = torch.einsum(\"mui,mvj->muvij\", inter_mask, alibi_1d_inter).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n         return (mask1+mask2)\n \n-    def forward(self, queries, keys, values, attn_mask, n_vars=None, pos_ids=None, tau=None, delta=None):\n+    def forward(self, queries, keys, values, attn_mask, n_vars=None, n_tokens=None, tau=None, delta=None):\n         B, L, H, E = queries.shape\n         _, S, _, D = values.shape\n         scale = self.scale or 1. / sqrt(E)\n         \n         queries = queries.permute(0, 2, 1, 3)\n         keys = keys.permute(0, 2, 1, 3)\n         \n-        # if n_vars is not None and n_tokens is not None:\n+        if n_vars==1:\n             # seq_id = repeat(torch.arange(n_tokens), 'n -> c n', c=n_vars).reshape(-1)\n             # seq_id = repeat(seq_id, 'n -> b h n', b=B, h=H)\n             \n             # self.apply_rope(queries, keys, seq_id)\n             # queries, keys = self.qk_proj(\n             #     queries, keys, query_id=seq_id, kv_id=seq_id)\n-        if n_vars>1:\n-            alibi = self.get_alibi_2d(n_vars, pos_ids, queries.device).unsqueeze(0)\n-        else:\n-            alibi = self.get_alibi_1d(pos_ids, queries.device).unsqueeze(0)\n+        # if n_vars>1:\n+        #     alibi = self.get_alibi_2d(n_vars, n_tokens, queries.device).unsqueeze(0)\n+        # else:\n+        #     alibi = self.get_alibi_1d(n_tokens, queries.device).unsqueeze(0)\n \n-        scores = torch.einsum(\"bhle,bhse->bhls\", queries, keys) + alibi\n+        scores = torch.einsum(\"bhle,bhse->bhls\", queries, keys)# + alibi\n \n         if self.mask_flag:\n             if attn_mask is None:\n                 attn_mask = TriangularCausalMask(B, L, device=queries.device)\n@@ -224,9 +224,9 @@\n             keys,\n             values,\n             attn_mask,\n             n_vars=n_vars,\n-            pos_ids=n_tokens,\n+            n_tokens=n_tokens,\n             tau=tau,\n             delta=delta\n         )\n         out = out.view(B, L, -1)\n"
                },
                {
                    "date": 1733305508271,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -101,14 +101,14 @@\n         queries = queries.permute(0, 2, 1, 3)\n         keys = keys.permute(0, 2, 1, 3)\n         \n         if n_vars==1:\n-            # seq_id = repeat(torch.arange(n_tokens), 'n -> c n', c=n_vars).reshape(-1)\n-            # seq_id = repeat(seq_id, 'n -> b h n', b=B, h=H)\n+            seq_id = repeat(torch.arange(len(n_tokens)), 'n -> c n', c=n_vars).reshape(-1)\n+            seq_id = repeat(seq_id, 'n -> b h n', b=B, h=H)\n             \n             # self.apply_rope(queries, keys, seq_id)\n-            # queries, keys = self.qk_proj(\n-            #     queries, keys, query_id=seq_id, kv_id=seq_id)\n+            queries, keys = self.qk_proj(\n+                queries, keys, query_id=seq_id, kv_id=seq_id)\n         # if n_vars>1:\n         #     alibi = self.get_alibi_2d(n_vars, n_tokens, queries.device).unsqueeze(0)\n         # else:\n         #     alibi = self.get_alibi_1d(n_tokens, queries.device).unsqueeze(0)\n"
                },
                {
                    "date": 1733306503523,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -100,21 +100,23 @@\n         \n         queries = queries.permute(0, 2, 1, 3)\n         keys = keys.permute(0, 2, 1, 3)\n         \n-        if n_vars==1:\n-            seq_id = repeat(torch.arange(len(n_tokens)), 'n -> c n', c=n_vars).reshape(-1)\n-            seq_id = repeat(seq_id, 'n -> b h n', b=B, h=H)\n+        # if n_vars==1:\n+        #     seq_id = repeat(torch.arange(len(n_tokens)), 'n -> c n', c=n_vars).reshape(-1)\n+        #     seq_id = repeat(seq_id, 'n -> b h n', b=B, h=H)\n             \n-            # self.apply_rope(queries, keys, seq_id)\n-            queries, keys = self.qk_proj(\n-                queries, keys, query_id=seq_id, kv_id=seq_id)\n-        # if n_vars>1:\n-        #     alibi = self.get_alibi_2d(n_vars, n_tokens, queries.device).unsqueeze(0)\n-        # else:\n-        #     alibi = self.get_alibi_1d(n_tokens, queries.device).unsqueeze(0)\n+        #     # self.apply_rope(queries, keys, seq_id)\n+        #     queries, keys = self.qk_proj(\n+        #         queries, keys, query_id=seq_id, kv_id=seq_id)\n+        if n_vars>1:\n+            scores = torch.einsum(\"bhle,bhse->bhls\", queries, keys)\n+            # alibi = self.get_alibi_2d(n_vars, n_tokens, queries.device).unsqueeze(0)\n+        else:\n+            alibi = self.get_alibi_1d(n_tokens, queries.device).unsqueeze(0)\n+            scores = torch.einsum(\"bhle,bhse->bhls\", queries, keys) + alibi\n \n-        scores = torch.einsum(\"bhle,bhse->bhls\", queries, keys)# + alibi\n+        # scores = torch.einsum(\"bhle,bhse->bhls\", queries, keys)# + alibi\n \n         if self.mask_flag:\n             if attn_mask is None:\n                 attn_mask = TriangularCausalMask(B, L, device=queries.device)\n"
                },
                {
                    "date": 1733312096576,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -85,9 +85,9 @@\n     \n     def get_alibi_2d(self, n_vars: int, pos_ids: int, device):\n         n_tokens = pos_ids.shape[0]\n         alibi_1d_intra = self.get_alibi_1d(pos_ids,device)\n-        alibi_1d_inter = torch.sqrt(1.1+alibi_1d_intra**2)\n+        alibi_1d_inter = torch.sqrt(1.1+alibi_1d_intra**2)*torch.sign(alibi_1d_intra)\n         intra_mask = torch.diag(torch.ones(n_vars)).to(device).unsqueeze(0).repeat(self.num_heads,1,1)\n         inter_mask = torch.tril(torch.ones((n_vars,n_vars)),diagonal=-1).to(device).unsqueeze(0).repeat(self.num_heads,1,1)\n         mask1 = torch.einsum(\"mui,mvj->muvij\", intra_mask, alibi_1d_intra).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n         mask2 = torch.einsum(\"mui,mvj->muvij\", inter_mask, alibi_1d_inter).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n"
                },
                {
                    "date": 1733316182567,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -108,15 +108,13 @@\n         #     # self.apply_rope(queries, keys, seq_id)\n         #     queries, keys = self.qk_proj(\n         #         queries, keys, query_id=seq_id, kv_id=seq_id)\n         if n_vars>1:\n-            scores = torch.einsum(\"bhle,bhse->bhls\", queries, keys)\n-            # alibi = self.get_alibi_2d(n_vars, n_tokens, queries.device).unsqueeze(0)\n+            alibi = self.get_alibi_2d(n_vars, n_tokens, queries.device).unsqueeze(0)\n         else:\n             alibi = self.get_alibi_1d(n_tokens, queries.device).unsqueeze(0)\n-            scores = torch.einsum(\"bhle,bhse->bhls\", queries, keys) + alibi\n \n-        # scores = torch.einsum(\"bhle,bhse->bhls\", queries, keys)# + alibi\n+        scores = torch.einsum(\"bhle,bhse->bhls\", queries, keys) + alibi\n \n         if self.mask_flag:\n             if attn_mask is None:\n                 attn_mask = TriangularCausalMask(B, L, device=queries.device)\n"
                }
            ],
            "date": 1730855017210,
            "name": "Commit-0",
            "content": "import torch\nimport torch.nn as nn\nimport numpy as np\nfrom math import sqrt\nfrom utils.masking import TriangularCausalMask, TimerMultivariateMask, TimerCovariateMask\nfrom einops import repeat\nfrom layers.Attn_Projection import QueryKeyProjection, RotaryProjection\nfrom layers.Attn_Bias import BinaryAttentionBias\n\n\nclass FullAttention(nn.Module):\n    def __init__(self, mask_flag=True, scale=None, attention_dropout=0.1, output_attention=False):\n        super(FullAttention, self).__init__()\n        self.scale = scale\n        self.mask_flag = mask_flag\n        self.output_attention = output_attention\n        self.dropout = nn.Dropout(attention_dropout)\n\n    def forward(self, queries, keys, values, attn_mask, n_vars=None, n_tokens=None, tau=None, delta=None):\n        B, L, H, E = queries.shape\n        _, S, _, D = values.shape\n        scale = self.scale or 1. / sqrt(E)\n\n        scores = torch.einsum(\"blhe,bshe->bhls\", queries, keys)\n\n        if self.mask_flag:\n            if attn_mask is None:\n                attn_mask = TriangularCausalMask(B, L, device=queries.device)\n\n            scores.masked_fill_(attn_mask.mask, -np.inf)\n\n        A = self.dropout(torch.softmax(scale * scores, dim=-1))\n        V = torch.einsum(\"bhls,bshd->blhd\", A, values)\n\n        if self.output_attention:\n            return V.contiguous(), A\n        else:\n            return V.contiguous(), None\n\n\nclass TimeAttention(nn.Module):\n    def __init__(self, mask_flag=True, scale=None, attention_dropout=0.1, output_attention=False, d_model=512, num_heads=8, max_len=100, covariate=False, flash_attention=False):\n        super(TimeAttention, self).__init__()\n        self.scale = scale\n        self.mask_flag = mask_flag\n        self.output_attention = output_attention\n        self.dropout = nn.Dropout(attention_dropout)\n        self.covariate = covariate\n        self.flash_attention = flash_attention\n        self.qk_proj = QueryKeyProjection(dim=d_model, num_heads=num_heads, proj_layer=RotaryProjection, kwargs=dict(max_len=max_len),\n                                          partial_factor=(0.0, 0.5),)\n        self.attn_bias = BinaryAttentionBias(dim=d_model, num_heads=num_heads)\n\n    def forward(self, queries, keys, values, attn_mask, n_vars, n_tokens, tau=None, delta=None):\n        B, L, H, E = queries.shape\n        _, S, _, D = values.shape\n\n        # [B, H, L, E]\n        queries = queries.permute(0, 2, 1, 3)\n        keys = keys.permute(0, 2, 1, 3)\n        if self.flash_attention:\n            values = values.permute(0, 2, 1, 3)\n\n        seq_id = torch.arange(n_tokens * n_vars)\n        seq_id = repeat(seq_id, 'n -> b h n', b=B, h=H)\n\n        queries, keys = self.qk_proj(\n            queries, keys, query_id=seq_id, kv_id=seq_id)\n\n        scale = self.scale or 1. / sqrt(E)\n\n        var_id = repeat(torch.arange(n_vars),\n                        'C -> (C n_tokens)', n_tokens=n_tokens)\n        var_id = repeat(var_id, 'L -> b h L', b=B, h=1).to(queries.device)\n\n        attn_bias = self.attn_bias(var_id, var_id)\n\n        if self.mask_flag:\n            if attn_mask is None:\n                if self.covariate:\n                    attn_mask = TimerCovariateMask(\n                        B, n_vars, n_tokens, device=queries.device)\n                else:\n                    attn_mask = TimerMultivariateMask(\n                        B, n_vars, n_tokens, device=queries.device)\n            attn_mask = attn_bias.masked_fill(attn_mask.mask, float(\"-inf\"))\n        else:\n            attn_mask = attn_bias\n\n        if self.flash_attention:\n            V = torch.nn.functional.scaled_dot_product_attention(\n                queries, keys, values, attn_mask)\n        else:\n            scores = torch.einsum(\"bhle,bhse->bhls\", queries, keys)\n            scores += attn_mask\n            \n            A = self.dropout(torch.softmax(scale * scores, dim=-1))\n            V = torch.einsum(\"bhls,bshd->blhd\", A, values)\n\n        if self.output_attention:\n            return V.contiguous(), None\n        else:\n            return V.contiguous(), None\n\nclass HybridAttention(nn.Module):\n    def __init__(self, mask_flag=True, scale=None, attention_dropout=0.1, output_attention=False, d_model=512, num_heads=8, max_len=100, covariate=False, flash_attention=False):\n        super(HybridAttention, self).__init__()\n        self.scale = scale\n        self.mask_flag = mask_flag\n        self.output_attention = output_attention\n        self.dropout = nn.Dropout(attention_dropout)\n        self.covariate = covariate\n        self.flash_attention = flash_attention\n        self.qk_proj = QueryKeyProjection(dim=d_model, num_heads=num_heads, proj_layer=RotaryProjection, kwargs=dict(max_len=max_len),\n                                          partial_factor=(0.0, 0.5),)\n        self.attn_bias = BinaryAttentionBias(dim=d_model, num_heads=num_heads)\n\n    def forward(self, queries, keys, values, attn_mask, n_vars, n_tokens, tau=None, delta=None):\n        B, L, H, E = queries.shape\n        _, S, _, D = values.shape\n\n        # [B, H, L, E]\n        queries = queries.permute(0, 2, 1, 3)\n        keys = keys.permute(0, 2, 1, 3)\n        if self.flash_attention:\n            values = values.permute(0, 2, 1, 3)\n\n        seq_id = torch.arange(n_tokens * n_vars)\n        seq_id = repeat(seq_id, 'n -> b h n', b=B, h=H)\n\n        queries, keys = self.qk_proj(\n            queries, keys, query_id=seq_id, kv_id=seq_id)\n\n        scale = self.scale or 1. / sqrt(E)\n\n        var_id = repeat(torch.arange(n_vars),\n                        'C -> (C n_tokens)', n_tokens=n_tokens)\n        var_id = repeat(var_id, 'L -> b h L', b=B, h=1).to(queries.device)\n\n        attn_bias = self.attn_bias(var_id, var_id)\n\n        if self.mask_flag:\n            if attn_mask is None:\n                if self.covariate:\n                    attn_mask = TimerCovariateMask(\n                        B, n_vars, n_tokens, device=queries.device)\n                else:\n                    attn_mask = TimerMultivariateMask(\n                        B, n_vars, n_tokens, device=queries.device)\n            attn_mask = attn_bias.masked_fill(attn_mask.mask, float(\"-inf\"))\n        else:\n            attn_mask = attn_bias\n\n        if self.flash_attention:\n            V = torch.nn.functional.scaled_dot_product_attention(\n                queries, keys, values, attn_mask)\n        else:\n            scores = torch.einsum(\"bhle,bhse->bhls\", queries, keys)\n            scores += attn_mask\n            \n            A = self.dropout(torch.softmax(scale * scores, dim=-1))\n            V = torch.einsum(\"bhls,bshd->blhd\", A, values)\n\n        if self.output_attention:\n            return V.contiguous(), None\n        else:\n            return V.contiguous(), None\n\n\nclass AttentionLayer(nn.Module):\n    def __init__(self, attention, d_model, n_heads, d_keys=None, d_values=None):\n        super(AttentionLayer, self).__init__()\n\n        d_keys = d_keys or (d_model // n_heads)\n        d_values = d_values or (d_model // n_heads)\n\n        self.inner_attention = attention\n        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n        self.value_projection = nn.Linear(d_model, d_values * n_heads)\n        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n        self.n_heads = n_heads\n\n    def forward(self, queries, keys, values, attn_mask, n_vars=None, n_tokens=None, tau=None, delta=None):\n        B, L, _ = queries.shape\n        _, S, _ = keys.shape\n        H = self.n_heads\n\n        queries = self.query_projection(queries).view(B, L, H, -1)\n        keys = self.key_projection(keys).view(B, S, H, -1)\n        values = self.value_projection(values).view(B, S, H, -1)\n\n        out, attn = self.inner_attention(\n            queries,\n            keys,\n            values,\n            attn_mask,\n            n_vars=n_vars,\n            n_tokens=n_tokens,\n            tau=tau,\n            delta=delta\n        )\n        out = out.view(B, L, -1)\n\n        return self.out_projection(out), attn\n"
        }
    ]
}