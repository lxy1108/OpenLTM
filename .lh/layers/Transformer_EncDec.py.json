{
    "sourceFile": "layers/Transformer_EncDec.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 95,
            "patches": [
                {
                    "date": 1730121378851,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1730121490374,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -123,8 +123,9 @@\n         self.norm = norm_layer\n \n     def forward(self, x, attn_mask=None, tau=None, delta=None):\n         # x [B, L, D]\n+        B,M,L,D = x.shape\n         attns = []\n         if self.conv_layers is not None:\n             for i, (attn_layer, conv_layer) in enumerate(zip(self.attn_layers, self.conv_layers)):\n                 delta = delta if i == 0 else None\n@@ -134,11 +135,18 @@\n                 attns.append(attn)\n             x, attn = self.attn_layers[-1](x, tau=tau, delta=None)\n             attns.append(attn)\n         else:\n-            for attn_layer in self.attn_layers:\n-                x, attn = attn_layer(\n-                    x, attn_mask=attn_mask, tau=tau, delta=delta)\n+            for i, attn_layer in enumerate(self.attn_layers):\n+                if i%2==0:\n+                    x = x.reshape(B*M,L,D)\n+                    x, attn = attn_layer(\n+                        x, attn_mask=attn_mask, tau=tau, delta=delta)\n+                else:\n+                    x = x.reshape(B,M*L,D)\n+                    x, attn = attn_layer(\n+                        x, attn_mask=attn_mask, tau=tau, delta=delta)\n+                x = x.reshape(B,M,L,D)\n                 attns.append(attn)\n \n         if self.norm is not None:\n             x = self.norm(x)\n"
                },
                {
                    "date": 1730121569384,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -138,12 +138,13 @@\n         else:\n             for i, attn_layer in enumerate(self.attn_layers):\n                 if i%2==0:\n                     x = x.reshape(B*M,L,D)\n-                    x, attn = attn_layer(\n-                        x, attn_mask=attn_mask, tau=tau, delta=delta)\n+                    attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n+                    x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n                     x = x.reshape(B,M*L,D)\n+                    attn_mask = TimerMultivariateMask(B,M,L,device=x.device)\n                     x, attn = attn_layer(\n                         x, attn_mask=attn_mask, tau=tau, delta=delta)\n                 x = x.reshape(B,M,L,D)\n                 attns.append(attn)\n"
                },
                {
                    "date": 1730121603127,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,8 +1,8 @@\n import torch.nn as nn\n import torch.nn.functional as F\n+from utils.masking import TriangularCausalMask, TimerMultivariateMask\n \n-\n class ConvLayer(nn.Module):\n     def __init__(self, c_in):\n         super(ConvLayer, self).__init__()\n         self.downConv = nn.Conv1d(in_channels=c_in,\n"
                },
                {
                    "date": 1730421895551,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -136,9 +136,9 @@\n             x, attn = self.attn_layers[-1](x, tau=tau, delta=None)\n             attns.append(attn)\n         else:\n             for i, attn_layer in enumerate(self.attn_layers):\n-                if i%2==0:\n+                if i%2==1:\n                     x = x.reshape(B*M,L,D)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n"
                },
                {
                    "date": 1730421939426,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -136,9 +136,9 @@\n             x, attn = self.attn_layers[-1](x, tau=tau, delta=None)\n             attns.append(attn)\n         else:\n             for i, attn_layer in enumerate(self.attn_layers):\n-                if i%2==1:\n+                if i!=len(self.attn_layers)-1:\n                     x = x.reshape(B*M,L,D)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n"
                },
                {
                    "date": 1730422198656,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -136,9 +136,9 @@\n             x, attn = self.attn_layers[-1](x, tau=tau, delta=None)\n             attns.append(attn)\n         else:\n             for i, attn_layer in enumerate(self.attn_layers):\n-                if i!=len(self.attn_layers)-1:\n+                if i%2==0:\n                     x = x.reshape(B*M,L,D)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n"
                },
                {
                    "date": 1730422963947,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -136,9 +136,9 @@\n             x, attn = self.attn_layers[-1](x, tau=tau, delta=None)\n             attns.append(attn)\n         else:\n             for i, attn_layer in enumerate(self.attn_layers):\n-                if i%2==0:\n+                if i!=len(self.attn_layers)-1:\n                     x = x.reshape(B*M,L,D)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n"
                },
                {
                    "date": 1730423067767,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -136,9 +136,10 @@\n             x, attn = self.attn_layers[-1](x, tau=tau, delta=None)\n             attns.append(attn)\n         else:\n             for i, attn_layer in enumerate(self.attn_layers):\n-                if i!=len(self.attn_layers)-1:\n+                if i%2==0:\n+                # if i!=len(self.attn_layers)-1:\n                     x = x.reshape(B*M,L,D)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n"
                },
                {
                    "date": 1730431474029,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -136,10 +136,10 @@\n             x, attn = self.attn_layers[-1](x, tau=tau, delta=None)\n             attns.append(attn)\n         else:\n             for i, attn_layer in enumerate(self.attn_layers):\n-                if i%2==0:\n-                # if i!=len(self.attn_layers)-1:\n+                # if i%2==0:\n+                if i!=len(self.attn_layers)-1:\n                     x = x.reshape(B*M,L,D)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n"
                },
                {
                    "date": 1730431488472,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -137,9 +137,9 @@\n             attns.append(attn)\n         else:\n             for i, attn_layer in enumerate(self.attn_layers):\n                 # if i%2==0:\n-                if i!=len(self.attn_layers)-1:\n+                if i!=len(self.attn_layers):\n                     x = x.reshape(B*M,L,D)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n"
                },
                {
                    "date": 1730431630120,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -137,9 +137,9 @@\n             attns.append(attn)\n         else:\n             for i, attn_layer in enumerate(self.attn_layers):\n                 # if i%2==0:\n-                if i!=len(self.attn_layers):\n+                if i!=len(self.attn_layers)-1:\n                     x = x.reshape(B*M,L,D)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n"
                },
                {
                    "date": 1730431864111,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -137,9 +137,9 @@\n             attns.append(attn)\n         else:\n             for i, attn_layer in enumerate(self.attn_layers):\n                 # if i%2==0:\n-                if i!=len(self.attn_layers)-1:\n+                if i!=len(self.attn_layers):\n                     x = x.reshape(B*M,L,D)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n"
                },
                {
                    "date": 1730439481783,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -137,9 +137,9 @@\n             attns.append(attn)\n         else:\n             for i, attn_layer in enumerate(self.attn_layers):\n                 # if i%2==0:\n-                if i!=len(self.attn_layers):\n+                if i!=len(self.attn_layers)-1:\n                     x = x.reshape(B*M,L,D)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n"
                },
                {
                    "date": 1730772845900,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -137,9 +137,9 @@\n             attns.append(attn)\n         else:\n             for i, attn_layer in enumerate(self.attn_layers):\n                 # if i%2==0:\n-                if i!=len(self.attn_layers)-1:\n+                if i!=len(self.attn_layers):\n                     x = x.reshape(B*M,L,D)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n"
                },
                {
                    "date": 1730772852589,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -137,9 +137,9 @@\n             attns.append(attn)\n         else:\n             for i, attn_layer in enumerate(self.attn_layers):\n                 # if i%2==0:\n-                if i!=len(self.attn_layers):\n+                if i!=len(self.attn_layers)-1:\n                     x = x.reshape(B*M,L,D)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n"
                },
                {
                    "date": 1730781322866,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -136,10 +136,10 @@\n             x, attn = self.attn_layers[-1](x, tau=tau, delta=None)\n             attns.append(attn)\n         else:\n             for i, attn_layer in enumerate(self.attn_layers):\n-                # if i%2==0:\n-                if i!=len(self.attn_layers)-1:\n+                if i%2==0:\n+                # if i!=len(self.attn_layers)-1:\n                     x = x.reshape(B*M,L,D)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n"
                },
                {
                    "date": 1730793840192,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -136,9 +136,9 @@\n             x, attn = self.attn_layers[-1](x, tau=tau, delta=None)\n             attns.append(attn)\n         else:\n             for i, attn_layer in enumerate(self.attn_layers):\n-                if i%2==0:\n+                if i%2==1:\n                 # if i!=len(self.attn_layers)-1:\n                     x = x.reshape(B*M,L,D)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n"
                },
                {
                    "date": 1730805080763,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,7 +1,7 @@\n import torch.nn as nn\n import torch.nn.functional as F\n-from utils.masking import TriangularCausalMask, TimerMultivariateMask\n+from utils.masking import TriangularCausalMask, TimerMultivariateMask, TimerMultivariateCrossMask\n \n class ConvLayer(nn.Module):\n     def __init__(self, c_in):\n         super(ConvLayer, self).__init__()\n@@ -143,9 +143,9 @@\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n                     x = x.reshape(B,M*L,D)\n-                    attn_mask = TimerMultivariateMask(B,M,L,device=x.device)\n+                    attn_mask = TimerMultivariateCrossMask(B,M,L,device=x.device)\n                     x, attn = attn_layer(\n                         x, attn_mask=attn_mask, tau=tau, delta=delta)\n                 x = x.reshape(B,M,L,D)\n                 attns.append(attn)\n"
                },
                {
                    "date": 1730807926171,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -136,10 +136,10 @@\n             x, attn = self.attn_layers[-1](x, tau=tau, delta=None)\n             attns.append(attn)\n         else:\n             for i, attn_layer in enumerate(self.attn_layers):\n-                if i%2==1:\n-                # if i!=len(self.attn_layers)-1:\n+                # if i%2==1:\n+                if i!=len(self.attn_layers)-1:\n                     x = x.reshape(B*M,L,D)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n"
                },
                {
                    "date": 1730809667968,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -136,16 +136,16 @@\n             x, attn = self.attn_layers[-1](x, tau=tau, delta=None)\n             attns.append(attn)\n         else:\n             for i, attn_layer in enumerate(self.attn_layers):\n-                # if i%2==1:\n-                if i!=len(self.attn_layers)-1:\n+                if i%2==1:\n+                # if i!=len(self.attn_layers)-1:\n                     x = x.reshape(B*M,L,D)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n                     x = x.reshape(B,M*L,D)\n-                    attn_mask = TimerMultivariateCrossMask(B,M,L,device=x.device)\n+                    attn_mask = TimerMultivariateMask(B,M,L,device=x.device)\n                     x, attn = attn_layer(\n                         x, attn_mask=attn_mask, tau=tau, delta=delta)\n                 x = x.reshape(B,M,L,D)\n                 attns.append(attn)\n"
                },
                {
                    "date": 1730815245802,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -136,16 +136,16 @@\n             x, attn = self.attn_layers[-1](x, tau=tau, delta=None)\n             attns.append(attn)\n         else:\n             for i, attn_layer in enumerate(self.attn_layers):\n-                if i%2==1:\n-                # if i!=len(self.attn_layers)-1:\n+                # if i%2==1:\n+                if i!=len(self.attn_layers)-1:\n                     x = x.reshape(B*M,L,D)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n                     x = x.reshape(B,M*L,D)\n-                    attn_mask = TimerMultivariateMask(B,M,L,device=x.device)\n+                    attn_mask = TimerMultivariateCrossMask(B,M,L,device=x.device)\n                     x, attn = attn_layer(\n                         x, attn_mask=attn_mask, tau=tau, delta=delta)\n                 x = x.reshape(B,M,L,D)\n                 attns.append(attn)\n"
                },
                {
                    "date": 1730817595889,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -136,10 +136,10 @@\n             x, attn = self.attn_layers[-1](x, tau=tau, delta=None)\n             attns.append(attn)\n         else:\n             for i, attn_layer in enumerate(self.attn_layers):\n-                # if i%2==1:\n-                if i!=len(self.attn_layers)-1:\n+                if i%2==1:\n+                # if i!=len(self.attn_layers)-1:\n                     x = x.reshape(B*M,L,D)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n"
                },
                {
                    "date": 1730817602921,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -143,9 +143,9 @@\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n                     x = x.reshape(B,M*L,D)\n-                    attn_mask = TimerMultivariateCrossMask(B,M,L,device=x.device)\n+                    attn_mask = TimerMultivariateMask(B,M,L,device=x.device)\n                     x, attn = attn_layer(\n                         x, attn_mask=attn_mask, tau=tau, delta=delta)\n                 x = x.reshape(B,M,L,D)\n                 attns.append(attn)\n"
                },
                {
                    "date": 1730818103789,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -145,9 +145,9 @@\n                 else:\n                     x = x.reshape(B,M*L,D)\n                     attn_mask = TimerMultivariateMask(B,M,L,device=x.device)\n                     x, attn = attn_layer(\n-                        x, attn_mask=attn_mask, tau=tau, delta=delta)\n+                        x, M, L, attn_mask=attn_mask, tau=tau, delta=delta)\n                 x = x.reshape(B,M,L,D)\n                 attns.append(attn)\n \n         if self.norm is not None:\n"
                },
                {
                    "date": 1730818254607,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -136,9 +136,9 @@\n             x, attn = self.attn_layers[-1](x, tau=tau, delta=None)\n             attns.append(attn)\n         else:\n             for i, attn_layer in enumerate(self.attn_layers):\n-                if i%2==1:\n+                if i%2==0:\n                 # if i!=len(self.attn_layers)-1:\n                     x = x.reshape(B*M,L,D)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n"
                },
                {
                    "date": 1730818336757,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -136,9 +136,9 @@\n             x, attn = self.attn_layers[-1](x, tau=tau, delta=None)\n             attns.append(attn)\n         else:\n             for i, attn_layer in enumerate(self.attn_layers):\n-                if i%2==0:\n+                if i%2==1:\n                 # if i!=len(self.attn_layers)-1:\n                     x = x.reshape(B*M,L,D)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n"
                },
                {
                    "date": 1730853474985,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -136,9 +136,9 @@\n             x, attn = self.attn_layers[-1](x, tau=tau, delta=None)\n             attns.append(attn)\n         else:\n             for i, attn_layer in enumerate(self.attn_layers):\n-                if i%2==1:\n+                if i<3:\n                 # if i!=len(self.attn_layers)-1:\n                     x = x.reshape(B*M,L,D)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n"
                },
                {
                    "date": 1730879448351,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -136,9 +136,9 @@\n             x, attn = self.attn_layers[-1](x, tau=tau, delta=None)\n             attns.append(attn)\n         else:\n             for i, attn_layer in enumerate(self.attn_layers):\n-                if i<3:\n+                if i%2==1:\n                 # if i!=len(self.attn_layers)-1:\n                     x = x.reshape(B*M,L,D)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n"
                },
                {
                    "date": 1730879475538,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -143,9 +143,9 @@\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n                     x = x.reshape(B,M*L,D)\n-                    attn_mask = TimerMultivariateMask(B,M,L,device=x.device)\n+                    attn_mask = TimerMultivariateCrossMask(B,M,L,device=x.device)\n                     x, attn = attn_layer(\n                         x, M, L, attn_mask=attn_mask, tau=tau, delta=delta)\n                 x = x.reshape(B,M,L,D)\n                 attns.append(attn)\n"
                },
                {
                    "date": 1730879671958,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -145,9 +145,9 @@\n                 else:\n                     x = x.reshape(B,M*L,D)\n                     attn_mask = TimerMultivariateCrossMask(B,M,L,device=x.device)\n                     x, attn = attn_layer(\n-                        x, M, L, attn_mask=attn_mask, tau=tau, delta=delta)\n+                        x, attn_mask=attn_mask, tau=tau, delta=delta)\n                 x = x.reshape(B,M,L,D)\n                 attns.append(attn)\n \n         if self.norm is not None:\n"
                },
                {
                    "date": 1730882476075,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -143,9 +143,9 @@\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n                     x = x.reshape(B,M*L,D)\n-                    attn_mask = TimerMultivariateCrossMask(B,M,L,device=x.device)\n+                    attn_mask = TimerMultivariateMask(B,M,L,device=x.device)\n                     x, attn = attn_layer(\n                         x, attn_mask=attn_mask, tau=tau, delta=delta)\n                 x = x.reshape(B,M,L,D)\n                 attns.append(attn)\n"
                },
                {
                    "date": 1730883040854,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -140,9 +140,9 @@\n                 if i%2==1:\n                 # if i!=len(self.attn_layers)-1:\n                     x = x.reshape(B*M,L,D)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n-                    x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n+                    x, attn = attn_layer(x, n_vars=1, n_tokens=L, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n                     x = x.reshape(B,M*L,D)\n                     attn_mask = TimerMultivariateMask(B,M,L,device=x.device)\n                     x, attn = attn_layer(\n"
                },
                {
                    "date": 1730883203915,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -36,9 +36,9 @@\n         self.norm2 = nn.LayerNorm(d_model)\n         self.dropout = nn.Dropout(dropout)\n         self.activation = F.relu if activation == \"relu\" else F.gelu\n \n-    def forward(self, x, attn_mask=None, tau=None, delta=None):\n+    def forward(self, x, n_vars=None, n_tokens=None, attn_mask=None, tau=None, delta=None):\n         new_x, attn = self.attention(\n             x, x, x,\n             attn_mask=attn_mask,\n             tau=tau, delta=delta\n"
                },
                {
                    "date": 1730883218461,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -39,8 +39,10 @@\n \n     def forward(self, x, n_vars=None, n_tokens=None, attn_mask=None, tau=None, delta=None):\n         new_x, attn = self.attention(\n             x, x, x,\n+            n_vars = n_vars,\n+            n_tokens = n_tokens,\n             attn_mask=attn_mask,\n             tau=tau, delta=delta\n         )\n         x = x + self.dropout(new_x)\n"
                },
                {
                    "date": 1730884037827,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -147,9 +147,9 @@\n                 else:\n                     x = x.reshape(B,M*L,D)\n                     attn_mask = TimerMultivariateMask(B,M,L,device=x.device)\n                     x, attn = attn_layer(\n-                        x, attn_mask=attn_mask, tau=tau, delta=delta)\n+                        x, attn_mask=attn_mask, n_vars=1, n_tokens=L, tau=tau, delta=delta)\n                 x = x.reshape(B,M,L,D)\n                 attns.append(attn)\n \n         if self.norm is not None:\n"
                },
                {
                    "date": 1730884654136,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -147,9 +147,9 @@\n                 else:\n                     x = x.reshape(B,M*L,D)\n                     attn_mask = TimerMultivariateMask(B,M,L,device=x.device)\n                     x, attn = attn_layer(\n-                        x, attn_mask=attn_mask, n_vars=1, n_tokens=L, tau=tau, delta=delta)\n+                        x, attn_mask=attn_mask, tau=tau, delta=delta)\n                 x = x.reshape(B,M,L,D)\n                 attns.append(attn)\n \n         if self.norm is not None:\n"
                },
                {
                    "date": 1730888230434,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -147,9 +147,9 @@\n                 else:\n                     x = x.reshape(B,M*L,D)\n                     attn_mask = TimerMultivariateMask(B,M,L,device=x.device)\n                     x, attn = attn_layer(\n-                        x, attn_mask=attn_mask, tau=tau, delta=delta)\n+                        x,  n_vars=M, n_tokens=L, attn_mask=attn_mask, tau=tau, delta=delta)\n                 x = x.reshape(B,M,L,D)\n                 attns.append(attn)\n \n         if self.norm is not None:\n"
                },
                {
                    "date": 1730888662508,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -147,9 +147,9 @@\n                 else:\n                     x = x.reshape(B,M*L,D)\n                     attn_mask = TimerMultivariateMask(B,M,L,device=x.device)\n                     x, attn = attn_layer(\n-                        x,  n_vars=M, n_tokens=L, attn_mask=attn_mask, tau=tau, delta=delta)\n+                        x,  n_vars=None, n_tokens=None, attn_mask=attn_mask, tau=tau, delta=delta)\n                 x = x.reshape(B,M,L,D)\n                 attns.append(attn)\n \n         if self.norm is not None:\n"
                },
                {
                    "date": 1730941626535,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -147,9 +147,9 @@\n                 else:\n                     x = x.reshape(B,M*L,D)\n                     attn_mask = TimerMultivariateMask(B,M,L,device=x.device)\n                     x, attn = attn_layer(\n-                        x,  n_vars=None, n_tokens=None, attn_mask=attn_mask, tau=tau, delta=delta)\n+                        x,  n_vars=M, n_tokens=L, attn_mask=attn_mask, tau=tau, delta=delta)\n                 x = x.reshape(B,M,L,D)\n                 attns.append(attn)\n \n         if self.norm is not None:\n"
                },
                {
                    "date": 1730941852106,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -147,9 +147,9 @@\n                 else:\n                     x = x.reshape(B,M*L,D)\n                     attn_mask = TimerMultivariateMask(B,M,L,device=x.device)\n                     x, attn = attn_layer(\n-                        x,  n_vars=M, n_tokens=L, attn_mask=attn_mask, tau=tau, delta=delta)\n+                        x,  n_vars=None, n_tokens=None, attn_mask=attn_mask, tau=tau, delta=delta)\n                 x = x.reshape(B,M,L,D)\n                 attns.append(attn)\n \n         if self.norm is not None:\n"
                },
                {
                    "date": 1731639482946,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -144,12 +144,16 @@\n                     x = x.reshape(B*M,L,D)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, n_vars=1, n_tokens=L, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n-                    x = x.reshape(B,M*L,D)\n+                    x_cross = x[:,:,[0,3,6],:].reshape(B,M*3,D)\n+                    x_cross = x_cross.reshape(B,M,3,D)\n+                    # x = x.reshape(B,M*L,D)\n                     attn_mask = TimerMultivariateMask(B,M,L,device=x.device)\n                     x, attn = attn_layer(\n                         x,  n_vars=None, n_tokens=None, attn_mask=attn_mask, tau=tau, delta=delta)\n+                    # \n+                    # x[:,:,[0,3,6],:] = x_cross\n                 x = x.reshape(B,M,L,D)\n                 attns.append(attn)\n \n         if self.norm is not None:\n"
                },
                {
                    "date": 1731639495573,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -144,14 +144,14 @@\n                     x = x.reshape(B*M,L,D)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, n_vars=1, n_tokens=L, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n-                    x_cross = x[:,:,[0,3,6],:].reshape(B,M*3,D)\n-                    x_cross = x_cross.reshape(B,M,3,D)\n+                    x_cross = x[:,:,[3,6],:].reshape(B,M*2,D)\n+                    x_cross = x_cross.reshape(B,M,2,D)\n                     # x = x.reshape(B,M*L,D)\n-                    attn_mask = TimerMultivariateMask(B,M,L,device=x.device)\n+                    attn_mask = TimerMultivariateMask(B,M,2,device=x.device)\n                     x, attn = attn_layer(\n-                        x,  n_vars=None, n_tokens=None, attn_mask=attn_mask, tau=tau, delta=delta)\n+                        x_cross,  n_vars=None, n_tokens=None, attn_mask=attn_mask, tau=tau, delta=delta)\n                     # \n                     # x[:,:,[0,3,6],:] = x_cross\n                 x = x.reshape(B,M,L,D)\n                 attns.append(attn)\n"
                },
                {
                    "date": 1731639599867,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -148,12 +148,12 @@\n                     x_cross = x[:,:,[3,6],:].reshape(B,M*2,D)\n                     x_cross = x_cross.reshape(B,M,2,D)\n                     # x = x.reshape(B,M*L,D)\n                     attn_mask = TimerMultivariateMask(B,M,2,device=x.device)\n-                    x, attn = attn_layer(\n+                    x_cross, attn = attn_layer(\n                         x_cross,  n_vars=None, n_tokens=None, attn_mask=attn_mask, tau=tau, delta=delta)\n                     # \n-                    # x[:,:,[0,3,6],:] = x_cross\n+                    x[:,:,[3,6],:] = x_cross\n                 x = x.reshape(B,M,L,D)\n                 attns.append(attn)\n \n         if self.norm is not None:\n"
                },
                {
                    "date": 1731639663738,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -145,14 +145,14 @@\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, n_vars=1, n_tokens=L, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n                     x_cross = x[:,:,[3,6],:].reshape(B,M*2,D)\n-                    x_cross = x_cross.reshape(B,M,2,D)\n                     # x = x.reshape(B,M*L,D)\n                     attn_mask = TimerMultivariateMask(B,M,2,device=x.device)\n                     x_cross, attn = attn_layer(\n                         x_cross,  n_vars=None, n_tokens=None, attn_mask=attn_mask, tau=tau, delta=delta)\n                     # \n+                    x_cross = x_cross.reshape(B,M,2,D)\n                     x[:,:,[3,6],:] = x_cross\n                 x = x.reshape(B,M,L,D)\n                 attns.append(attn)\n \n"
                },
                {
                    "date": 1731648615212,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -144,16 +144,16 @@\n                     x = x.reshape(B*M,L,D)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, n_vars=1, n_tokens=L, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n-                    x_cross = x[:,:,[3,6],:].reshape(B,M*2,D)\n-                    # x = x.reshape(B,M*L,D)\n-                    attn_mask = TimerMultivariateMask(B,M,2,device=x.device)\n-                    x_cross, attn = attn_layer(\n-                        x_cross,  n_vars=None, n_tokens=None, attn_mask=attn_mask, tau=tau, delta=delta)\n+                    # x_cross = x[:,:,[3,6],:].reshape(B,M*2,D)\n+                    x = x.reshape(B,M*L,D)\n+                    attn_mask = TimerMultivariateMask(B,M, L,device=x.device)\n+                    x, attn = attn_layer(\n+                        x,  n_vars=None, n_tokens=None, attn_mask=attn_mask, tau=tau, delta=delta)\n                     # \n-                    x_cross = x_cross.reshape(B,M,2,D)\n-                    x[:,:,[3,6],:] = x_cross\n+                    # x_cross = x_cross.reshape(B,M,2,D)\n+                    # x[:,:,[3,6],:] = x_cross\n                 x = x.reshape(B,M,L,D)\n                 attns.append(attn)\n \n         if self.norm is not None:\n"
                },
                {
                    "date": 1731660695385,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -144,16 +144,16 @@\n                     x = x.reshape(B*M,L,D)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, n_vars=1, n_tokens=L, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n-                    # x_cross = x[:,:,[3,6],:].reshape(B,M*2,D)\n-                    x = x.reshape(B,M*L,D)\n-                    attn_mask = TimerMultivariateMask(B,M, L,device=x.device)\n-                    x, attn = attn_layer(\n-                        x,  n_vars=None, n_tokens=None, attn_mask=attn_mask, tau=tau, delta=delta)\n+                    x_cross = x[:,:,[3,6],:].reshape(B,M*2,D)\n+                    # x = x.reshape(B,M*L,D)\n+                    attn_mask = TimerMultivariateMask(B,M, 2,device=x.device)\n+                    x_cross, attn = attn_layer(\n+                        x_cross,  n_vars=None, n_tokens=None, attn_mask=attn_mask, tau=tau, delta=delta)\n                     # \n-                    # x_cross = x_cross.reshape(B,M,2,D)\n-                    # x[:,:,[3,6],:] = x_cross\n+                    x_cross = x_cross.reshape(B,M,2,D)\n+                    x[:,:,[3,6],:] = x_cross\n                 x = x.reshape(B,M,L,D)\n                 attns.append(attn)\n \n         if self.norm is not None:\n"
                },
                {
                    "date": 1731675738088,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -144,16 +144,16 @@\n                     x = x.reshape(B*M,L,D)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, n_vars=1, n_tokens=L, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n-                    x_cross = x[:,:,[3,6],:].reshape(B,M*2,D)\n-                    # x = x.reshape(B,M*L,D)\n-                    attn_mask = TimerMultivariateMask(B,M, 2,device=x.device)\n-                    x_cross, attn = attn_layer(\n-                        x_cross,  n_vars=None, n_tokens=None, attn_mask=attn_mask, tau=tau, delta=delta)\n+                    # x_cross = x[:,:,[3,6],:].reshape(B,M*2,D)\n+                    x = x.reshape(B,M*L,D)\n+                    attn_mask = TimerMultivariateMask(B,M, L,device=x.device)\n+                    x, attn = attn_layer(\n+                        x,  n_vars=None, n_tokens=None, attn_mask=attn_mask, tau=tau, delta=delta)\n                     # \n-                    x_cross = x_cross.reshape(B,M,2,D)\n-                    x[:,:,[3,6],:] = x_cross\n+                    # x_cross = x_cross.reshape(B,M,2,D)\n+                    # x[:,:,[3,6],:] = x_cross\n                 x = x.reshape(B,M,L,D)\n                 attns.append(attn)\n \n         if self.norm is not None:\n"
                },
                {
                    "date": 1731679197267,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -123,9 +123,9 @@\n         self.conv_layers = nn.ModuleList(\n             conv_layers) if conv_layers is not None else None\n         self.norm = norm_layer\n \n-    def forward(self, x, attn_mask=None, tau=None, delta=None):\n+    def forward(self, x, mask=None, tau=None, delta=None):\n         # x [B, L, D]\n         B,M,L,D = x.shape\n         attns = []\n         if self.conv_layers is not None:\n@@ -146,9 +146,9 @@\n                     x, attn = attn_layer(x, n_vars=1, n_tokens=L, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n                     # x_cross = x[:,:,[3,6],:].reshape(B,M*2,D)\n                     x = x.reshape(B,M*L,D)\n-                    attn_mask = TimerMultivariateMask(B,M, L,device=x.device)\n+                    attn_mask = TimerMultivariateMask(B,M, L,mask = mask, device=x.device)\n                     x, attn = attn_layer(\n                         x,  n_vars=None, n_tokens=None, attn_mask=attn_mask, tau=tau, delta=delta)\n                     # \n                     # x_cross = x_cross.reshape(B,M,2,D)\n"
                },
                {
                    "date": 1732257318577,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -144,9 +144,9 @@\n                     x = x.reshape(B*M,L,D)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, n_vars=1, n_tokens=L, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n-                    # x_cross = x[:,:,[3,6],:].reshape(B,M*2,D)\n+                    x_cross = x[:,:,[1,3,5,7],:].reshape(B,M*2,D)\n                     x = x.reshape(B,M*L,D)\n                     attn_mask = TimerMultivariateMask(B,M, L,mask = mask, device=x.device)\n                     x, attn = attn_layer(\n                         x,  n_vars=None, n_tokens=None, attn_mask=attn_mask, tau=tau, delta=delta)\n"
                },
                {
                    "date": 1732257326053,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -144,11 +144,11 @@\n                     x = x.reshape(B*M,L,D)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, n_vars=1, n_tokens=L, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n-                    x_cross = x[:,:,[1,3,5,7],:].reshape(B,M*2,D)\n+                    x_cross = x[:,:,[1,3,5,7],:].reshape(B,M*4,D)\n                     x = x.reshape(B,M*L,D)\n-                    attn_mask = TimerMultivariateMask(B,M, L,mask = mask, device=x.device)\n+                    attn_mask = TimerMultivariateMask(B,M, 4,mask = mask, device=x.device)\n                     x, attn = attn_layer(\n                         x,  n_vars=None, n_tokens=None, attn_mask=attn_mask, tau=tau, delta=delta)\n                     # \n                     # x_cross = x_cross.reshape(B,M,2,D)\n"
                },
                {
                    "date": 1732257360659,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -145,15 +145,15 @@\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, n_vars=1, n_tokens=L, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n                     x_cross = x[:,:,[1,3,5,7],:].reshape(B,M*4,D)\n-                    x = x.reshape(B,M*L,D)\n+                    # x = x.reshape(B,M*L,D)\n                     attn_mask = TimerMultivariateMask(B,M, 4,mask = mask, device=x.device)\n-                    x, attn = attn_layer(\n-                        x,  n_vars=None, n_tokens=None, attn_mask=attn_mask, tau=tau, delta=delta)\n+                    x_cross, attn = attn_layer(\n+                        x_cross,  n_vars=None, n_tokens=None, attn_mask=attn_mask, tau=tau, delta=delta)\n                     # \n-                    # x_cross = x_cross.reshape(B,M,2,D)\n-                    # x[:,:,[3,6],:] = x_cross\n+                    x_cross = x_cross.reshape(B,M,2,D)\n+                    x[:,:,[1,3,5,7],:] = x_cross\n                 x = x.reshape(B,M,L,D)\n                 attns.append(attn)\n \n         if self.norm is not None:\n"
                },
                {
                    "date": 1732282434207,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -144,16 +144,16 @@\n                     x = x.reshape(B*M,L,D)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, n_vars=1, n_tokens=L, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n-                    x_cross = x[:,:,[1,3,5,7],:].reshape(B,M*4,D)\n-                    # x = x.reshape(B,M*L,D)\n-                    attn_mask = TimerMultivariateMask(B,M, 4,mask = mask, device=x.device)\n-                    x_cross, attn = attn_layer(\n-                        x_cross,  n_vars=None, n_tokens=None, attn_mask=attn_mask, tau=tau, delta=delta)\n+                    # x_cross = x[:,:,[1,3,5,7],:].reshape(B,M*4,D)\n+                    x = x.reshape(B,M*L,D)\n+                    attn_mask = TimerMultivariateMask(B,M, L,mask = mask, device=x.device)\n+                    x, attn = attn_layer(\n+                        x,  n_vars=None, n_tokens=None, attn_mask=attn_mask, tau=tau, delta=delta)\n                     # \n-                    x_cross = x_cross.reshape(B,M,2,D)\n-                    x[:,:,[1,3,5,7],:] = x_cross\n+                    # x_cross = x_cross.reshape(B,M,4,D)\n+                    # x[:,:,[1,3,5,7],:] = x_cross\n                 x = x.reshape(B,M,L,D)\n                 attns.append(attn)\n \n         if self.norm is not None:\n"
                },
                {
                    "date": 1732802855402,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -144,16 +144,16 @@\n                     x = x.reshape(B*M,L,D)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, n_vars=1, n_tokens=L, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n-                    # x_cross = x[:,:,[1,3,5,7],:].reshape(B,M*4,D)\n-                    x = x.reshape(B,M*L,D)\n-                    attn_mask = TimerMultivariateMask(B,M, L,mask = mask, device=x.device)\n-                    x, attn = attn_layer(\n-                        x,  n_vars=None, n_tokens=None, attn_mask=attn_mask, tau=tau, delta=delta)\n+                    x_cross = x[:,:,[1,3,5,7],:].reshape(B,M*4,D)\n+                    # x = x.reshape(B,M*L,D)\n+                    attn_mask = TimerMultivariateMask(B,M, 4,mask = mask, device=x.device)\n+                    x_cross, attn = attn_layer(\n+                        x_cross,  n_vars=None, n_tokens=None, attn_mask=attn_mask, tau=tau, delta=delta)\n                     # \n-                    # x_cross = x_cross.reshape(B,M,4,D)\n-                    # x[:,:,[1,3,5,7],:] = x_cross\n+                    x_cross = x_cross.reshape(B,M,4,D)\n+                    x[:,:,[1,3,5,7],:] = x_cross\n                 x = x.reshape(B,M,L,D)\n                 attns.append(attn)\n \n         if self.norm is not None:\n"
                },
                {
                    "date": 1732889902590,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -144,16 +144,16 @@\n                     x = x.reshape(B*M,L,D)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, n_vars=1, n_tokens=L, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n-                    x_cross = x[:,:,[1,3,5,7],:].reshape(B,M*4,D)\n+                    x_cross = x[:,:,[0,2,4,6],:].reshape(B,M*4,D)\n                     # x = x.reshape(B,M*L,D)\n                     attn_mask = TimerMultivariateMask(B,M, 4,mask = mask, device=x.device)\n                     x_cross, attn = attn_layer(\n                         x_cross,  n_vars=None, n_tokens=None, attn_mask=attn_mask, tau=tau, delta=delta)\n                     # \n                     x_cross = x_cross.reshape(B,M,4,D)\n-                    x[:,:,[1,3,5,7],:] = x_cross\n+                    x[:,:,[0,2,4,6],:] = x_cross\n                 x = x.reshape(B,M,L,D)\n                 attns.append(attn)\n \n         if self.norm is not None:\n"
                },
                {
                    "date": 1732943165417,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -144,9 +144,10 @@\n                     x = x.reshape(B*M,L,D)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, n_vars=1, n_tokens=L, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n-                    x_cross = x[:,:,[0,2,4,6],:].reshape(B,M*4,D)\n+                    sel = list(range(3,7))\n+                    x_cross = x[:,:,sel,:].reshape(B,M*len(sel),D)\n                     # x = x.reshape(B,M*L,D)\n                     attn_mask = TimerMultivariateMask(B,M, 4,mask = mask, device=x.device)\n                     x_cross, attn = attn_layer(\n                         x_cross,  n_vars=None, n_tokens=None, attn_mask=attn_mask, tau=tau, delta=delta)\n"
                },
                {
                    "date": 1732943177350,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -147,14 +147,14 @@\n                 else:\n                     sel = list(range(3,7))\n                     x_cross = x[:,:,sel,:].reshape(B,M*len(sel),D)\n                     # x = x.reshape(B,M*L,D)\n-                    attn_mask = TimerMultivariateMask(B,M, 4,mask = mask, device=x.device)\n+                    attn_mask = TimerMultivariateMask(B,M, len(sel),mask = mask, device=x.device)\n                     x_cross, attn = attn_layer(\n                         x_cross,  n_vars=None, n_tokens=None, attn_mask=attn_mask, tau=tau, delta=delta)\n                     # \n-                    x_cross = x_cross.reshape(B,M,4,D)\n-                    x[:,:,[0,2,4,6],:] = x_cross\n+                    x_cross = x_cross.reshape(B,M,len(sel),D)\n+                    x[:,:,sel,:] = x_cross\n                 x = x.reshape(B,M,L,D)\n                 attns.append(attn)\n \n         if self.norm is not None:\n"
                },
                {
                    "date": 1733036066651,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -144,9 +144,9 @@\n                     x = x.reshape(B*M,L,D)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, n_vars=1, n_tokens=L, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n-                    sel = list(range(3,7))\n+                    sel = list(range(0,8,2))\n                     x_cross = x[:,:,sel,:].reshape(B,M*len(sel),D)\n                     # x = x.reshape(B,M*L,D)\n                     attn_mask = TimerMultivariateMask(B,M, len(sel),mask = mask, device=x.device)\n                     x_cross, attn = attn_layer(\n"
                },
                {
                    "date": 1733052076505,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,7 +1,8 @@\n import torch.nn as nn\n import torch.nn.functional as F\n from utils.masking import TriangularCausalMask, TimerMultivariateMask, TimerMultivariateCrossMask\n+import math\n \n class ConvLayer(nn.Module):\n     def __init__(self, c_in):\n         super(ConvLayer, self).__init__()\n@@ -114,9 +115,21 @@\n         if self.norm is not None:\n             x = self.norm(x)\n \n         return x, attns\n+    \n+def get_slopes(n):\n+    def get_slopes_power_of_2(n):\n+        start = (2**(-2**-(math.log2(n)-3)))\n+        ratio = start\n+        return [start*ratio**i for i in range(n)]\n \n+    if math.log2(n).is_integer():\n+        return get_slopes_power_of_2(n)                   #In the paper, we only train models that have 2^a heads for some a. This function has\n+    else:                                                 #some good properties that only occur when the input is a power of 2. To maintain that even\n+        closest_power_of_2 = 2**math.floor(math.log2(n))  #when the number of heads is not a power of 2, we use this workaround. \n+        return get_slopes_power_of_2(closest_power_of_2) + get_slopes(2*closest_power_of_2)[0::2][:n-closest_power_of_2]\n+\n class AlterEncoder(nn.Module):\n     def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n         super(AlterEncoder, self).__init__()\n         self.attn_layers = nn.ModuleList(attn_layers)\n"
                },
                {
                    "date": 1733052661942,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,8 +1,9 @@\n import torch.nn as nn\n import torch.nn.functional as F\n from utils.masking import TriangularCausalMask, TimerMultivariateMask, TimerMultivariateCrossMask\n import math\n+import torch\n \n class ConvLayer(nn.Module):\n     def __init__(self, c_in):\n         super(ConvLayer, self).__init__()\n@@ -136,8 +137,14 @@\n         self.conv_layers = nn.ModuleList(\n             conv_layers) if conv_layers is not None else None\n         self.norm = norm_layer\n \n+    def get_relative_positions(seq_len: int) -> torch.tensor:\n+        x = torch.arange(seq_len)[None, :]\n+        y = torch.arange(seq_len)[:, None]\n+        return x - y\n+\n+\n     def forward(self, x, mask=None, tau=None, delta=None):\n         # x [B, L, D]\n         B,M,L,D = x.shape\n         attns = []\n@@ -154,9 +161,9 @@\n             for i, attn_layer in enumerate(self.attn_layers):\n                 if i%2==1:\n                 # if i!=len(self.attn_layers)-1:\n                     x = x.reshape(B*M,L,D)\n-                    attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n+                    attn_mask = TriangularCausalMask(B*M, L, device=x.device) + (self.m * get_relative_positions(seq_len)).unsqueeze(0)\n                     x, attn = attn_layer(x, n_vars=1, n_tokens=L, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n                     sel = list(range(0,8,2))\n                     x_cross = x[:,:,sel,:].reshape(B,M*len(sel),D)\n"
                },
                {
                    "date": 1733052678146,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -137,9 +137,9 @@\n         self.conv_layers = nn.ModuleList(\n             conv_layers) if conv_layers is not None else None\n         self.norm = norm_layer\n \n-    def get_relative_positions(seq_len: int) -> torch.tensor:\n+    def get_relative_positions(self,seq_len: int) -> torch.tensor:\n         x = torch.arange(seq_len)[None, :]\n         y = torch.arange(seq_len)[:, None]\n         return x - y\n \n@@ -161,9 +161,9 @@\n             for i, attn_layer in enumerate(self.attn_layers):\n                 if i%2==1:\n                 # if i!=len(self.attn_layers)-1:\n                     x = x.reshape(B*M,L,D)\n-                    attn_mask = TriangularCausalMask(B*M, L, device=x.device) + (self.m * get_relative_positions(seq_len)).unsqueeze(0)\n+                    attn_mask = TriangularCausalMask(B*M, L, device=x.device) + (self.m * self.get_relative_positions(L)).unsqueeze(0)\n                     x, attn = attn_layer(x, n_vars=1, n_tokens=L, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n                     sel = list(range(0,8,2))\n                     x_cross = x[:,:,sel,:].reshape(B,M*len(sel),D)\n"
                },
                {
                    "date": 1733052748791,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -117,28 +117,31 @@\n             x = self.norm(x)\n \n         return x, attns\n     \n-def get_slopes(n):\n-    def get_slopes_power_of_2(n):\n-        start = (2**(-2**-(math.log2(n)-3)))\n-        ratio = start\n-        return [start*ratio**i for i in range(n)]\n \n-    if math.log2(n).is_integer():\n-        return get_slopes_power_of_2(n)                   #In the paper, we only train models that have 2^a heads for some a. This function has\n-    else:                                                 #some good properties that only occur when the input is a power of 2. To maintain that even\n-        closest_power_of_2 = 2**math.floor(math.log2(n))  #when the number of heads is not a power of 2, we use this workaround. \n-        return get_slopes_power_of_2(closest_power_of_2) + get_slopes(2*closest_power_of_2)[0::2][:n-closest_power_of_2]\n-\n class AlterEncoder(nn.Module):\n-    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n+    def __init__(self, attn_layers, conv_layers=None, norm_layer=None, num_layers=8):\n         super(AlterEncoder, self).__init__()\n         self.attn_layers = nn.ModuleList(attn_layers)\n         self.conv_layers = nn.ModuleList(\n             conv_layers) if conv_layers is not None else None\n         self.norm = norm_layer\n+        self.slopes = self.get_slopes(attn_layers[0])\n+        \n+    def get_slopes(self,n):\n+        def get_slopes_power_of_2(n):\n+            start = (2**(-2**-(math.log2(n)-3)))\n+            ratio = start\n+            return [start*ratio**i for i in range(n)]\n \n+        if math.log2(n).is_integer():\n+            return get_slopes_power_of_2(n)                   #In the paper, we only train models that have 2^a heads for some a. This function has\n+        else:                                                 #some good properties that only occur when the input is a power of 2. To maintain that even\n+            closest_power_of_2 = 2**math.floor(math.log2(n))  #when the number of heads is not a power of 2, we use this workaround. \n+            return get_slopes_power_of_2(closest_power_of_2) + get_slopes(2*closest_power_of_2)[0::2][:n-closest_power_of_2]\n+\n+\n     def get_relative_positions(self,seq_len: int) -> torch.tensor:\n         x = torch.arange(seq_len)[None, :]\n         y = torch.arange(seq_len)[:, None]\n         return x - y\n"
                },
                {
                    "date": 1733052760878,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -119,15 +119,15 @@\n         return x, attns\n     \n \n class AlterEncoder(nn.Module):\n-    def __init__(self, attn_layers, conv_layers=None, norm_layer=None, num_layers=8):\n+    def __init__(self, attn_layers, conv_layers=None, norm_layer=None, num_headss=8):\n         super(AlterEncoder, self).__init__()\n         self.attn_layers = nn.ModuleList(attn_layers)\n         self.conv_layers = nn.ModuleList(\n             conv_layers) if conv_layers is not None else None\n         self.norm = norm_layer\n-        self.slopes = self.get_slopes(attn_layers[0])\n+        self.slopes = self.get_slopes(num_headss)\n         \n     def get_slopes(self,n):\n         def get_slopes_power_of_2(n):\n             start = (2**(-2**-(math.log2(n)-3)))\n"
                },
                {
                    "date": 1733052777278,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -137,9 +137,9 @@\n         if math.log2(n).is_integer():\n             return get_slopes_power_of_2(n)                   #In the paper, we only train models that have 2^a heads for some a. This function has\n         else:                                                 #some good properties that only occur when the input is a power of 2. To maintain that even\n             closest_power_of_2 = 2**math.floor(math.log2(n))  #when the number of heads is not a power of 2, we use this workaround. \n-            return get_slopes_power_of_2(closest_power_of_2) + get_slopes(2*closest_power_of_2)[0::2][:n-closest_power_of_2]\n+            return get_slopes_power_of_2(closest_power_of_2) + self.get_slopes(2*closest_power_of_2)[0::2][:n-closest_power_of_2]\n \n \n     def get_relative_positions(self,seq_len: int) -> torch.tensor:\n         x = torch.arange(seq_len)[None, :]\n@@ -164,9 +164,9 @@\n             for i, attn_layer in enumerate(self.attn_layers):\n                 if i%2==1:\n                 # if i!=len(self.attn_layers)-1:\n                     x = x.reshape(B*M,L,D)\n-                    attn_mask = TriangularCausalMask(B*M, L, device=x.device) + (self.m * self.get_relative_positions(L)).unsqueeze(0)\n+                    attn_mask = TriangularCausalMask(B*M, L, device=x.device) + (self.slopes * self.get_relative_positions(L)).unsqueeze(0)\n                     x, attn = attn_layer(x, n_vars=1, n_tokens=L, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n                     sel = list(range(0,8,2))\n                     x_cross = x[:,:,sel,:].reshape(B,M*len(sel),D)\n"
                },
                {
                    "date": 1733052833187,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -144,10 +144,12 @@\n     def get_relative_positions(self,seq_len: int) -> torch.tensor:\n         x = torch.arange(seq_len)[None, :]\n         y = torch.arange(seq_len)[:, None]\n         return x - y\n+    \n+    def get_alibi_1d(self, seq_len: int):\n+        return (self.slopes * self.get_relative_positions(seq_len)).unsqueeze(0)\n \n-\n     def forward(self, x, mask=None, tau=None, delta=None):\n         # x [B, L, D]\n         B,M,L,D = x.shape\n         attns = []\n@@ -164,9 +166,9 @@\n             for i, attn_layer in enumerate(self.attn_layers):\n                 if i%2==1:\n                 # if i!=len(self.attn_layers)-1:\n                     x = x.reshape(B*M,L,D)\n-                    attn_mask = TriangularCausalMask(B*M, L, device=x.device) + (self.slopes * self.get_relative_positions(L)).unsqueeze(0)\n+                    attn_mask = TriangularCausalMask(B*M, L, device=x.device) + self.get_alibi_1d(L)\n                     x, attn = attn_layer(x, n_vars=1, n_tokens=L, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n                     sel = list(range(0,8,2))\n                     x_cross = x[:,:,sel,:].reshape(B,M*len(sel),D)\n"
                },
                {
                    "date": 1733052841335,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -147,8 +147,11 @@\n         return x - y\n     \n     def get_alibi_1d(self, seq_len: int):\n         return (self.slopes * self.get_relative_positions(seq_len)).unsqueeze(0)\n+    \n+    def get_alibi_1d(self, seq_len: int):\n+        return (self.slopes * self.get_relative_positions(seq_len)).unsqueeze(0)\n \n     def forward(self, x, mask=None, tau=None, delta=None):\n         # x [B, L, D]\n         B,M,L,D = x.shape\n"
                },
                {
                    "date": 1733052884598,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -148,9 +148,10 @@\n     \n     def get_alibi_1d(self, seq_len: int):\n         return (self.slopes * self.get_relative_positions(seq_len)).unsqueeze(0)\n     \n-    def get_alibi_1d(self, seq_len: int):\n+    def get_alibi_2d(self, seq_len: int, n_vars: int):\n+        alibi_1d = self.get_alibi_1d(seq_len)\n         return (self.slopes * self.get_relative_positions(seq_len)).unsqueeze(0)\n \n     def forward(self, x, mask=None, tau=None, delta=None):\n         # x [B, L, D]\n"
                },
                {
                    "date": 1733053004548,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -149,9 +149,10 @@\n     def get_alibi_1d(self, seq_len: int):\n         return (self.slopes * self.get_relative_positions(seq_len)).unsqueeze(0)\n     \n     def get_alibi_2d(self, seq_len: int, n_vars: int):\n-        alibi_1d = self.get_alibi_1d(seq_len)\n+        alibi_1d_intra = self.get_alibi_1d(seq_len)\n+        alibi_1d_inter = torch.sqrt(1+alibi_1d_intra**2)\n         return (self.slopes * self.get_relative_positions(seq_len)).unsqueeze(0)\n \n     def forward(self, x, mask=None, tau=None, delta=None):\n         # x [B, L, D]\n"
                },
                {
                    "date": 1733053083943,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -148,11 +148,13 @@\n     \n     def get_alibi_1d(self, seq_len: int):\n         return (self.slopes * self.get_relative_positions(seq_len)).unsqueeze(0)\n     \n-    def get_alibi_2d(self, seq_len: int, n_vars: int):\n-        alibi_1d_intra = self.get_alibi_1d(seq_len)\n+    def get_alibi_2d(self, n_vars: int, n_tokens: int):\n+        alibi_1d_intra = self.get_alibi_1d(n_tokens)\n         alibi_1d_inter = torch.sqrt(1+alibi_1d_intra**2)\n+        mask = torch.einsum(\"ui,vj->uvij\", self._mask1, self._mask2).reshape(n_vars*n_tokens,n_vars*n_tokens)\n+\n         return (self.slopes * self.get_relative_positions(seq_len)).unsqueeze(0)\n \n     def forward(self, x, mask=None, tau=None, delta=None):\n         # x [B, L, D]\n"
                },
                {
                    "date": 1733053530154,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -119,15 +119,16 @@\n         return x, attns\n     \n \n class AlterEncoder(nn.Module):\n-    def __init__(self, attn_layers, conv_layers=None, norm_layer=None, num_headss=8):\n+    def __init__(self, attn_layers, conv_layers=None, norm_layer=None, num_heads=8):\n         super(AlterEncoder, self).__init__()\n         self.attn_layers = nn.ModuleList(attn_layers)\n         self.conv_layers = nn.ModuleList(\n             conv_layers) if conv_layers is not None else None\n         self.norm = norm_layer\n-        self.slopes = self.get_slopes(num_headss)\n+        self.num_heads = num_heads\n+        self.slopes = self.get_slopes(num_heads)\n         \n     def get_slopes(self,n):\n         def get_slopes_power_of_2(n):\n             start = (2**(-2**-(math.log2(n)-3)))\n@@ -151,12 +152,14 @@\n     \n     def get_alibi_2d(self, n_vars: int, n_tokens: int):\n         alibi_1d_intra = self.get_alibi_1d(n_tokens)\n         alibi_1d_inter = torch.sqrt(1+alibi_1d_intra**2)\n-        mask = torch.einsum(\"ui,vj->uvij\", self._mask1, self._mask2).reshape(n_vars*n_tokens,n_vars*n_tokens)\n+        intra_mask = torch.diag(torch.ones(n_tokens))\n+        inter_mask = torch.triu(torch.ones((n_tokens,n_tokens)))\n+        mask1 = torch.einsum(\"mui,vj->muvij\", alibi_1d_intra, intra_mask).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n+        mask2 = torch.einsum(\"mui,vj->muvij\", alibi_1d_inter, inter_mask).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n+        return mask1+mask2\n \n-        return (self.slopes * self.get_relative_positions(seq_len)).unsqueeze(0)\n-\n     def forward(self, x, mask=None, tau=None, delta=None):\n         # x [B, L, D]\n         B,M,L,D = x.shape\n         attns = []\n"
                },
                {
                    "date": 1733053765770,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -147,18 +147,20 @@\n         y = torch.arange(seq_len)[:, None]\n         return x - y\n     \n     def get_alibi_1d(self, seq_len: int):\n-        return (self.slopes * self.get_relative_positions(seq_len)).unsqueeze(0)\n+        mask = self.slopes[:,None,None] * self.get_relative_positions(seq_len)\n+        mask = torch.triu(mask).unsqueeze(0)\n+        return ().unsqueeze(0)\n     \n     def get_alibi_2d(self, n_vars: int, n_tokens: int):\n         alibi_1d_intra = self.get_alibi_1d(n_tokens)\n         alibi_1d_inter = torch.sqrt(1+alibi_1d_intra**2)\n         intra_mask = torch.diag(torch.ones(n_tokens))\n         inter_mask = torch.triu(torch.ones((n_tokens,n_tokens)))\n         mask1 = torch.einsum(\"mui,vj->muvij\", alibi_1d_intra, intra_mask).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n         mask2 = torch.einsum(\"mui,vj->muvij\", alibi_1d_inter, inter_mask).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n-        return mask1+mask2\n+        return (mask1+mask2).unsqueeze(0)\n \n     def forward(self, x, mask=None, tau=None, delta=None):\n         # x [B, L, D]\n         B,M,L,D = x.shape\n"
                },
                {
                    "date": 1733053775685,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -147,9 +147,9 @@\n         y = torch.arange(seq_len)[:, None]\n         return x - y\n     \n     def get_alibi_1d(self, seq_len: int):\n-        mask = self.slopes[:,None,None] * self.get_relative_positions(seq_len)\n+        mask = self.slopes[:,None,None] * self.get_relative_positions(seq_len).unsqueeze(0)\n         mask = torch.triu(mask).unsqueeze(0)\n         return ().unsqueeze(0)\n     \n     def get_alibi_2d(self, n_vars: int, n_tokens: int):\n"
                },
                {
                    "date": 1733053793451,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -148,10 +148,9 @@\n         return x - y\n     \n     def get_alibi_1d(self, seq_len: int):\n         mask = self.slopes[:,None,None] * self.get_relative_positions(seq_len).unsqueeze(0)\n-        mask = torch.triu(mask).unsqueeze(0)\n-        return ().unsqueeze(0)\n+        return torch.triu(mask).unsqueeze(0)\n     \n     def get_alibi_2d(self, n_vars: int, n_tokens: int):\n         alibi_1d_intra = self.get_alibi_1d(n_tokens)\n         alibi_1d_inter = torch.sqrt(1+alibi_1d_intra**2)\n"
                },
                {
                    "date": 1733053855455,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -177,15 +177,17 @@\n             for i, attn_layer in enumerate(self.attn_layers):\n                 if i%2==1:\n                 # if i!=len(self.attn_layers)-1:\n                     x = x.reshape(B*M,L,D)\n-                    attn_mask = TriangularCausalMask(B*M, L, device=x.device) + self.get_alibi_1d(L)\n+                    attn_mask = self.get_alibi_1d(L)\n+                    # attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, n_vars=1, n_tokens=L, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n                     sel = list(range(0,8,2))\n                     x_cross = x[:,:,sel,:].reshape(B,M*len(sel),D)\n                     # x = x.reshape(B,M*L,D)\n-                    attn_mask = TimerMultivariateMask(B,M, len(sel),mask = mask, device=x.device)\n+                    # attn_mask = TimerMultivariateMask(B,M, len(sel),mask = mask, device=x.device)\n+                    attn_mask = self.get_alibi_2d(M,len(sel))\n                     x_cross, attn = attn_layer(\n                         x_cross,  n_vars=None, n_tokens=None, attn_mask=attn_mask, tau=tau, delta=delta)\n                     # \n                     x_cross = x_cross.reshape(B,M,len(sel),D)\n"
                },
                {
                    "date": 1733058771899,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,9 +1,7 @@\n import torch.nn as nn\n import torch.nn.functional as F\n from utils.masking import TriangularCausalMask, TimerMultivariateMask, TimerMultivariateCrossMask\n-import math\n-import torch\n \n class ConvLayer(nn.Module):\n     def __init__(self, c_in):\n         super(ConvLayer, self).__init__()\n@@ -128,38 +126,9 @@\n         self.norm = norm_layer\n         self.num_heads = num_heads\n         self.slopes = self.get_slopes(num_heads)\n         \n-    def get_slopes(self,n):\n-        def get_slopes_power_of_2(n):\n-            start = (2**(-2**-(math.log2(n)-3)))\n-            ratio = start\n-            return [start*ratio**i for i in range(n)]\n-\n-        if math.log2(n).is_integer():\n-            return get_slopes_power_of_2(n)                   #In the paper, we only train models that have 2^a heads for some a. This function has\n-        else:                                                 #some good properties that only occur when the input is a power of 2. To maintain that even\n-            closest_power_of_2 = 2**math.floor(math.log2(n))  #when the number of heads is not a power of 2, we use this workaround. \n-            return get_slopes_power_of_2(closest_power_of_2) + self.get_slopes(2*closest_power_of_2)[0::2][:n-closest_power_of_2]\n-\n-\n-    def get_relative_positions(self,seq_len: int) -> torch.tensor:\n-        x = torch.arange(seq_len)[None, :]\n-        y = torch.arange(seq_len)[:, None]\n-        return x - y\n     \n-    def get_alibi_1d(self, seq_len: int):\n-        mask = self.slopes[:,None,None] * self.get_relative_positions(seq_len).unsqueeze(0)\n-        return torch.triu(mask).unsqueeze(0)\n-    \n-    def get_alibi_2d(self, n_vars: int, n_tokens: int):\n-        alibi_1d_intra = self.get_alibi_1d(n_tokens)\n-        alibi_1d_inter = torch.sqrt(1+alibi_1d_intra**2)\n-        intra_mask = torch.diag(torch.ones(n_tokens))\n-        inter_mask = torch.triu(torch.ones((n_tokens,n_tokens)))\n-        mask1 = torch.einsum(\"mui,vj->muvij\", alibi_1d_intra, intra_mask).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n-        mask2 = torch.einsum(\"mui,vj->muvij\", alibi_1d_inter, inter_mask).reshape(self.num_heads,n_vars*n_tokens,n_vars*n_tokens)\n-        return (mask1+mask2).unsqueeze(0)\n \n     def forward(self, x, mask=None, tau=None, delta=None):\n         # x [B, L, D]\n         B,M,L,D = x.shape\n@@ -177,17 +146,17 @@\n             for i, attn_layer in enumerate(self.attn_layers):\n                 if i%2==1:\n                 # if i!=len(self.attn_layers)-1:\n                     x = x.reshape(B*M,L,D)\n-                    attn_mask = self.get_alibi_1d(L)\n-                    # attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n+                    # attn_mask = self.get_alibi_1d(L)\n+                    attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, n_vars=1, n_tokens=L, attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n                     sel = list(range(0,8,2))\n                     x_cross = x[:,:,sel,:].reshape(B,M*len(sel),D)\n                     # x = x.reshape(B,M*L,D)\n-                    # attn_mask = TimerMultivariateMask(B,M, len(sel),mask = mask, device=x.device)\n-                    attn_mask = self.get_alibi_2d(M,len(sel))\n+                    attn_mask = TimerMultivariateMask(B,M, len(sel),mask = mask, device=x.device)\n+                    # attn_mask = self.get_alibi_2d(M,len(sel))\n                     x_cross, attn = attn_layer(\n                         x_cross,  n_vars=None, n_tokens=None, attn_mask=attn_mask, tau=tau, delta=delta)\n                     # \n                     x_cross = x_cross.reshape(B,M,len(sel),D)\n"
                },
                {
                    "date": 1733058966039,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -124,10 +124,10 @@\n         self.conv_layers = nn.ModuleList(\n             conv_layers) if conv_layers is not None else None\n         self.norm = norm_layer\n         self.num_heads = num_heads\n-        self.slopes = self.get_slopes(num_heads)\n         \n+        \n     \n \n     def forward(self, x, mask=None, tau=None, delta=None):\n         # x [B, L, D]\n"
                },
                {
                    "date": 1733058971402,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -117,15 +117,14 @@\n         return x, attns\n     \n \n class AlterEncoder(nn.Module):\n-    def __init__(self, attn_layers, conv_layers=None, norm_layer=None, num_heads=8):\n+    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n         super(AlterEncoder, self).__init__()\n         self.attn_layers = nn.ModuleList(attn_layers)\n         self.conv_layers = nn.ModuleList(\n             conv_layers) if conv_layers is not None else None\n         self.norm = norm_layer\n-        self.num_heads = num_heads\n         \n         \n     \n \n"
                },
                {
                    "date": 1733059677383,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -155,9 +155,9 @@\n                     # x = x.reshape(B,M*L,D)\n                     attn_mask = TimerMultivariateMask(B,M, len(sel),mask = mask, device=x.device)\n                     # attn_mask = self.get_alibi_2d(M,len(sel))\n                     x_cross, attn = attn_layer(\n-                        x_cross,  n_vars=None, n_tokens=None, attn_mask=attn_mask, tau=tau, delta=delta)\n+                        x_cross,  n_vars=M, n_tokens=len(sel), attn_mask=attn_mask, tau=tau, delta=delta)\n                     # \n                     x_cross = x_cross.reshape(B,M,len(sel),D)\n                     x[:,:,sel,:] = x_cross\n                 x = x.reshape(B,M,L,D)\n"
                },
                {
                    "date": 1733108444768,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -147,9 +147,9 @@\n                 # if i!=len(self.attn_layers)-1:\n                     x = x.reshape(B*M,L,D)\n                     # attn_mask = self.get_alibi_1d(L)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n-                    x, attn = attn_layer(x, n_vars=1, n_tokens=L, attn_mask=attn_mask, tau=tau, delta=delta)\n+                    x, attn = attn_layer(x, n_vars=1, n_tokens=torch.arange(L), attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n                     sel = list(range(0,8,2))\n                     x_cross = x[:,:,sel,:].reshape(B,M*len(sel),D)\n                     # x = x.reshape(B,M*L,D)\n"
                },
                {
                    "date": 1733108461228,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -147,9 +147,9 @@\n                 # if i!=len(self.attn_layers)-1:\n                     x = x.reshape(B*M,L,D)\n                     # attn_mask = self.get_alibi_1d(L)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n-                    x, attn = attn_layer(x, n_vars=1, n_tokens=torch.arange(L), attn_mask=attn_mask, tau=tau, delta=delta)\n+                    x, attn = attn_layer(x, n_vars=1, n_tokens=torch.arange(L,device=x.device), attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n                     sel = list(range(0,8,2))\n                     x_cross = x[:,:,sel,:].reshape(B,M*len(sel),D)\n                     # x = x.reshape(B,M*L,D)\n"
                },
                {
                    "date": 1733108560942,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,8 +1,8 @@\n import torch.nn as nn\n import torch.nn.functional as F\n from utils.masking import TriangularCausalMask, TimerMultivariateMask, TimerMultivariateCrossMask\n-\n+import torch\n class ConvLayer(nn.Module):\n     def __init__(self, c_in):\n         super(ConvLayer, self).__init__()\n         self.downConv = nn.Conv1d(in_channels=c_in,\n@@ -149,9 +149,9 @@\n                     # attn_mask = self.get_alibi_1d(L)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, n_vars=1, n_tokens=torch.arange(L,device=x.device), attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n-                    sel = list(range(0,8,2))\n+                    sel = list(range(0,7,2))#0,2,4,6;1,3,5,7\n                     x_cross = x[:,:,sel,:].reshape(B,M*len(sel),D)\n                     # x = x.reshape(B,M*L,D)\n                     attn_mask = TimerMultivariateMask(B,M, len(sel),mask = mask, device=x.device)\n                     # attn_mask = self.get_alibi_2d(M,len(sel))\n"
                },
                {
                    "date": 1733108585730,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -155,9 +155,9 @@\n                     # x = x.reshape(B,M*L,D)\n                     attn_mask = TimerMultivariateMask(B,M, len(sel),mask = mask, device=x.device)\n                     # attn_mask = self.get_alibi_2d(M,len(sel))\n                     x_cross, attn = attn_layer(\n-                        x_cross,  n_vars=M, n_tokens=len(sel), attn_mask=attn_mask, tau=tau, delta=delta)\n+                        x_cross,  n_vars=M, n_tokens=torch.Tensor(sel,device=x.device), attn_mask=attn_mask, tau=tau, delta=delta)\n                     # \n                     x_cross = x_cross.reshape(B,M,len(sel),D)\n                     x[:,:,sel,:] = x_cross\n                 x = x.reshape(B,M,L,D)\n"
                },
                {
                    "date": 1733108847475,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -155,9 +155,9 @@\n                     # x = x.reshape(B,M*L,D)\n                     attn_mask = TimerMultivariateMask(B,M, len(sel),mask = mask, device=x.device)\n                     # attn_mask = self.get_alibi_2d(M,len(sel))\n                     x_cross, attn = attn_layer(\n-                        x_cross,  n_vars=M, n_tokens=torch.Tensor(sel,device=x.device), attn_mask=attn_mask, tau=tau, delta=delta)\n+                        x_cross,  n_vars=M, n_tokens=torch.Tensor(sel).to(x.device), attn_mask=attn_mask, tau=tau, delta=delta)\n                     # \n                     x_cross = x_cross.reshape(B,M,len(sel),D)\n                     x[:,:,sel,:] = x_cross\n                 x = x.reshape(B,M,L,D)\n"
                },
                {
                    "date": 1733285475291,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -149,9 +149,9 @@\n                     # attn_mask = self.get_alibi_1d(L)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, n_vars=1, n_tokens=torch.arange(L,device=x.device), attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n-                    sel = list(range(0,7,2))#0,2,4,6;1,3,5,7\n+                    sel = list(range(0,7,1))#0,2,4,6;1,3,5,7\n                     x_cross = x[:,:,sel,:].reshape(B,M*len(sel),D)\n                     # x = x.reshape(B,M*L,D)\n                     attn_mask = TimerMultivariateMask(B,M, len(sel),mask = mask, device=x.device)\n                     # attn_mask = self.get_alibi_2d(M,len(sel))\n"
                },
                {
                    "date": 1733294607505,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -149,9 +149,9 @@\n                     # attn_mask = self.get_alibi_1d(L)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, n_vars=1, n_tokens=torch.arange(L,device=x.device), attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n-                    sel = list(range(0,7,1))#0,2,4,6;1,3,5,7\n+                    sel = list(range(0,7,2))#0,2,4,6;1,3,5,7\n                     x_cross = x[:,:,sel,:].reshape(B,M*len(sel),D)\n                     # x = x.reshape(B,M*L,D)\n                     attn_mask = TimerMultivariateMask(B,M, len(sel),mask = mask, device=x.device)\n                     # attn_mask = self.get_alibi_2d(M,len(sel))\n"
                },
                {
                    "date": 1733294617099,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -142,9 +142,9 @@\n             x, attn = self.attn_layers[-1](x, tau=tau, delta=None)\n             attns.append(attn)\n         else:\n             for i, attn_layer in enumerate(self.attn_layers):\n-                if i%2==1:\n+                if i%2==-1:\n                 # if i!=len(self.attn_layers)-1:\n                     x = x.reshape(B*M,L,D)\n                     # attn_mask = self.get_alibi_1d(L)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n"
                },
                {
                    "date": 1733305429628,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -149,9 +149,9 @@\n                     # attn_mask = self.get_alibi_1d(L)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, n_vars=1, n_tokens=torch.arange(L,device=x.device), attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n-                    sel = list(range(0,7,2))#0,2,4,6;1,3,5,7\n+                    sel = list(range(0,7,1))#0,2,4,6;1,3,5,7\n                     x_cross = x[:,:,sel,:].reshape(B,M*len(sel),D)\n                     # x = x.reshape(B,M*L,D)\n                     attn_mask = TimerMultivariateMask(B,M, len(sel),mask = mask, device=x.device)\n                     # attn_mask = self.get_alibi_2d(M,len(sel))\n"
                },
                {
                    "date": 1733305436972,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -142,9 +142,9 @@\n             x, attn = self.attn_layers[-1](x, tau=tau, delta=None)\n             attns.append(attn)\n         else:\n             for i, attn_layer in enumerate(self.attn_layers):\n-                if i%2==-1:\n+                if i%2==1:\n                 # if i!=len(self.attn_layers)-1:\n                     x = x.reshape(B*M,L,D)\n                     # attn_mask = self.get_alibi_1d(L)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n"
                },
                {
                    "date": 1733322331474,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -149,9 +149,9 @@\n                     # attn_mask = self.get_alibi_1d(L)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, n_vars=1, n_tokens=torch.arange(L,device=x.device), attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n-                    sel = list(range(0,7,1))#0,2,4,6;1,3,5,7\n+                    sel = list(range(0,7,2))#0,2,4,6;1,3,5,7\n                     x_cross = x[:,:,sel,:].reshape(B,M*len(sel),D)\n                     # x = x.reshape(B,M*L,D)\n                     attn_mask = TimerMultivariateMask(B,M, len(sel),mask = mask, device=x.device)\n                     # attn_mask = self.get_alibi_2d(M,len(sel))\n"
                },
                {
                    "date": 1733388901647,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -149,9 +149,9 @@\n                     # attn_mask = self.get_alibi_1d(L)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, n_vars=1, n_tokens=torch.arange(L,device=x.device), attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n-                    sel = list(range(0,7,2))#0,2,4,6;1,3,5,7\n+                    sel = list(range(0,8,2))#0,2,4,6;1,3,5,7\n                     x_cross = x[:,:,sel,:].reshape(B,M*len(sel),D)\n                     # x = x.reshape(B,M*L,D)\n                     attn_mask = TimerMultivariateMask(B,M, len(sel),mask = mask, device=x.device)\n                     # attn_mask = self.get_alibi_2d(M,len(sel))\n"
                },
                {
                    "date": 1733388956523,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -149,9 +149,9 @@\n                     # attn_mask = self.get_alibi_1d(L)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, n_vars=1, n_tokens=torch.arange(L,device=x.device), attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n-                    sel = list(range(0,8,2))#0,2,4,6;1,3,5,7\n+                    sel = list(range(1,8,2))#0,2,4,6;1,3,5,7\n                     x_cross = x[:,:,sel,:].reshape(B,M*len(sel),D)\n                     # x = x.reshape(B,M*L,D)\n                     attn_mask = TimerMultivariateMask(B,M, len(sel),mask = mask, device=x.device)\n                     # attn_mask = self.get_alibi_2d(M,len(sel))\n"
                },
                {
                    "date": 1733454471825,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -149,9 +149,9 @@\n                     # attn_mask = self.get_alibi_1d(L)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, n_vars=1, n_tokens=torch.arange(L,device=x.device), attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n-                    sel = list(range(1,8,2))#0,2,4,6;1,3,5,7\n+                    sel = list(range(0,7,2))#0,2,4,6;1,3,5,7\n                     x_cross = x[:,:,sel,:].reshape(B,M*len(sel),D)\n                     # x = x.reshape(B,M*L,D)\n                     attn_mask = TimerMultivariateMask(B,M, len(sel),mask = mask, device=x.device)\n                     # attn_mask = self.get_alibi_2d(M,len(sel))\n"
                },
                {
                    "date": 1733459917665,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -149,9 +149,10 @@\n                     # attn_mask = self.get_alibi_1d(L)\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, n_vars=1, n_tokens=torch.arange(L,device=x.device), attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n-                    sel = list(range(0,7,2))#0,2,4,6;1,3,5,7\n+                    # sel = list(range(0,7,2))#0,2,4,6;1,3,5,7\n+                    sel = random.sample(list(range(7)), 4)\n                     x_cross = x[:,:,sel,:].reshape(B,M*len(sel),D)\n                     # x = x.reshape(B,M*L,D)\n                     attn_mask = TimerMultivariateMask(B,M, len(sel),mask = mask, device=x.device)\n                     # attn_mask = self.get_alibi_2d(M,len(sel))\n"
                },
                {
                    "date": 1733459924076,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,8 +1,9 @@\n import torch.nn as nn\n import torch.nn.functional as F\n from utils.masking import TriangularCausalMask, TimerMultivariateMask, TimerMultivariateCrossMask\n import torch\n+import random\n class ConvLayer(nn.Module):\n     def __init__(self, c_in):\n         super(ConvLayer, self).__init__()\n         self.downConv = nn.Conv1d(in_channels=c_in,\n"
                },
                {
                    "date": 1733469142415,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -151,9 +151,12 @@\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, n_vars=1, n_tokens=torch.arange(L,device=x.device), attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n                     # sel = list(range(0,7,2))#0,2,4,6;1,3,5,7\n-                    sel = random.sample(list(range(7)), 4)\n+                    if self.training:\n+                        sel = random.sample(list(range(7)), 4)\n+                    else:\n+                        sel = list(range(7))\n                     x_cross = x[:,:,sel,:].reshape(B,M*len(sel),D)\n                     # x = x.reshape(B,M*L,D)\n                     attn_mask = TimerMultivariateMask(B,M, len(sel),mask = mask, device=x.device)\n                     # attn_mask = self.get_alibi_2d(M,len(sel))\n"
                },
                {
                    "date": 1733469666311,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -151,11 +151,13 @@\n                     attn_mask = TriangularCausalMask(B*M, L, device=x.device)\n                     x, attn = attn_layer(x, n_vars=1, n_tokens=torch.arange(L,device=x.device), attn_mask=attn_mask, tau=tau, delta=delta)\n                 else:\n                     # sel = list(range(0,7,2))#0,2,4,6;1,3,5,7\n-                    if self.training:\n-                        sel = random.sample(list(range(7)), 4)\n-                    else:\n+                    if i==0:\n+                        sel = [0,2,4,6]\n+                    elif i==2:\n+                        sel = [1,3,5,6]\n+                    elif i==4:\n                         sel = list(range(7))\n                     x_cross = x[:,:,sel,:].reshape(B,M*len(sel),D)\n                     # x = x.reshape(B,M*L,D)\n                     attn_mask = TimerMultivariateMask(B,M, len(sel),mask = mask, device=x.device)\n"
                }
            ],
            "date": 1730121378851,
            "name": "Commit-0",
            "content": "import torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass ConvLayer(nn.Module):\n    def __init__(self, c_in):\n        super(ConvLayer, self).__init__()\n        self.downConv = nn.Conv1d(in_channels=c_in,\n                                  out_channels=c_in,\n                                  kernel_size=3,\n                                  padding=2,\n                                  padding_mode='circular')\n        self.norm = nn.BatchNorm1d(c_in)\n        self.activation = nn.ELU()\n        self.maxPool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n\n    def forward(self, x):\n        x = self.downConv(x.permute(0, 2, 1))\n        x = self.norm(x)\n        x = self.activation(x)\n        x = self.maxPool(x)\n        x = x.transpose(1, 2)\n        return x\n\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, attention, d_model, d_ff=None, dropout=0.1, activation=\"relu\"):\n        super(EncoderLayer, self).__init__()\n        d_ff = d_ff or 4 * d_model\n        self.attention = attention\n        self.conv1 = nn.Conv1d(in_channels=d_model,\n                               out_channels=d_ff, kernel_size=1)\n        self.conv2 = nn.Conv1d(\n            in_channels=d_ff, out_channels=d_model, kernel_size=1)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n        self.activation = F.relu if activation == \"relu\" else F.gelu\n\n    def forward(self, x, attn_mask=None, tau=None, delta=None):\n        new_x, attn = self.attention(\n            x, x, x,\n            attn_mask=attn_mask,\n            tau=tau, delta=delta\n        )\n        x = x + self.dropout(new_x)\n\n        y = x = self.norm1(x)\n        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n        y = self.dropout(self.conv2(y).transpose(-1, 1))\n\n        return self.norm2(x + y), attn\n\n\nclass TimerLayer(nn.Module):\n    def __init__(self, attention, d_model, d_ff=None, dropout=0.1, activation=\"relu\"):\n        super(TimerLayer, self).__init__()\n        d_ff = d_ff or 4 * d_model\n        self.attention = attention\n        self.conv1 = nn.Conv1d(in_channels=d_model,\n                               out_channels=d_ff, kernel_size=1)\n        self.conv2 = nn.Conv1d(\n            in_channels=d_ff, out_channels=d_model, kernel_size=1)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n        self.activation = F.relu if activation == \"relu\" else F.gelu\n\n    def forward(self, x, n_vars, n_tokens, attn_mask=None, tau=None, delta=None):\n        new_x, attn = self.attention(\n            x, x, x,\n            n_vars=n_vars,\n            n_tokens=n_tokens,\n            attn_mask=attn_mask,\n            tau=tau, delta=delta\n        )\n        x = x + self.dropout(new_x)\n\n        y = x = self.norm1(x)\n        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n        y = self.dropout(self.conv2(y).transpose(-1, 1))\n\n        return self.norm2(x + y), attn\n\n\nclass Encoder(nn.Module):\n    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n        super(Encoder, self).__init__()\n        self.attn_layers = nn.ModuleList(attn_layers)\n        self.conv_layers = nn.ModuleList(\n            conv_layers) if conv_layers is not None else None\n        self.norm = norm_layer\n\n    def forward(self, x, attn_mask=None, tau=None, delta=None):\n        # x [B, L, D]\n        attns = []\n        if self.conv_layers is not None:\n            for i, (attn_layer, conv_layer) in enumerate(zip(self.attn_layers, self.conv_layers)):\n                delta = delta if i == 0 else None\n                x, attn = attn_layer(\n                    x, attn_mask=attn_mask, tau=tau, delta=delta)\n                x = conv_layer(x)\n                attns.append(attn)\n            x, attn = self.attn_layers[-1](x, tau=tau, delta=None)\n            attns.append(attn)\n        else:\n            for attn_layer in self.attn_layers:\n                x, attn = attn_layer(\n                    x, attn_mask=attn_mask, tau=tau, delta=delta)\n                attns.append(attn)\n\n        if self.norm is not None:\n            x = self.norm(x)\n\n        return x, attns\n\nclass AlterEncoder(nn.Module):\n    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n        super(AlterEncoder, self).__init__()\n        self.attn_layers = nn.ModuleList(attn_layers)\n        self.conv_layers = nn.ModuleList(\n            conv_layers) if conv_layers is not None else None\n        self.norm = norm_layer\n\n    def forward(self, x, attn_mask=None, tau=None, delta=None):\n        # x [B, L, D]\n        attns = []\n        if self.conv_layers is not None:\n            for i, (attn_layer, conv_layer) in enumerate(zip(self.attn_layers, self.conv_layers)):\n                delta = delta if i == 0 else None\n                x, attn = attn_layer(\n                    x, attn_mask=attn_mask, tau=tau, delta=delta)\n                x = conv_layer(x)\n                attns.append(attn)\n            x, attn = self.attn_layers[-1](x, tau=tau, delta=None)\n            attns.append(attn)\n        else:\n            for attn_layer in self.attn_layers:\n                x, attn = attn_layer(\n                    x, attn_mask=attn_mask, tau=tau, delta=delta)\n                attns.append(attn)\n\n        if self.norm is not None:\n            x = self.norm(x)\n\n        return x, attns\n\nclass DecoderLayer(nn.Module):\n    def __init__(self, self_attention, cross_attention, d_model, d_ff=None,\n                 dropout=0.1, activation=\"relu\"):\n        super(DecoderLayer, self).__init__()\n        d_ff = d_ff or 4 * d_model\n        self.self_attention = self_attention\n        self.cross_attention = cross_attention\n        self.conv1 = nn.Conv1d(in_channels=d_model,\n                               out_channels=d_ff, kernel_size=1)\n        self.conv2 = nn.Conv1d(\n            in_channels=d_ff, out_channels=d_model, kernel_size=1)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n        self.activation = F.relu if activation == \"relu\" else F.gelu\n\n    def forward(self, x, cross, x_mask=None, cross_mask=None, tau=None, delta=None):\n        x = x + self.dropout(self.self_attention(\n            x, x, x,\n            attn_mask=x_mask,\n            tau=tau, delta=None\n        )[0])\n        x = self.norm1(x)\n\n        x = x + self.dropout(self.cross_attention(\n            x, cross, cross,\n            attn_mask=cross_mask,\n            tau=tau, delta=delta\n        )[0])\n\n        y = x = self.norm2(x)\n        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n        y = self.dropout(self.conv2(y).transpose(-1, 1))\n\n        return self.norm3(x + y)\n\n\nclass Decoder(nn.Module):\n    def __init__(self, layers, norm_layer=None, projection=None):\n        super(Decoder, self).__init__()\n        self.layers = nn.ModuleList(layers)\n        self.norm = norm_layer\n        self.projection = projection\n\n    def forward(self, x, cross, x_mask=None, cross_mask=None, tau=None, delta=None):\n        for layer in self.layers:\n            x = layer(x, cross, x_mask=x_mask,\n                      cross_mask=cross_mask, tau=tau, delta=delta)\n\n        if self.norm is not None:\n            x = self.norm(x)\n\n        if self.projection is not None:\n            x = self.projection(x)\n        return x\n\n\nclass TimerBlock(nn.Module):\n    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n        super(TimerBlock, self).__init__()\n        self.attn_layers = nn.ModuleList(attn_layers)\n        self.conv_layers = nn.ModuleList(\n            conv_layers) if conv_layers is not None else None\n        self.norm = norm_layer\n\n    def forward(self, x, n_vars, n_tokens, attn_mask=None, tau=None, delta=None):\n        # x [B, L, D]\n        attns = []\n        if self.conv_layers is not None:\n            for i, (attn_layer, conv_layer) in enumerate(zip(self.attn_layers, self.conv_layers)):\n                delta = delta if i == 0 else None\n                x, attn = attn_layer(\n                    x, attn_mask=attn_mask, tau=tau, delta=delta)\n                x = conv_layer(x)\n                attns.append(attn)\n            x, attn = self.attn_layers[-1](x, n_vars,\n                                           n_tokens, tau=tau, delta=None)\n            attns.append(attn)\n        else:\n            for attn_layer in self.attn_layers:\n                x, attn = attn_layer(x, n_vars, n_tokens,\n                                     attn_mask=attn_mask, tau=tau, delta=delta)\n                attns.append(attn)\n\n        if self.norm is not None:\n            x = self.norm(x)\n\n        return x, attns\n"
        }
    ]
}