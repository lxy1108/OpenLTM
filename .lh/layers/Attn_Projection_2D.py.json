{
    "sourceFile": "layers/Attn_Projection_2D.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1732973516291,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1732973516291,
            "name": "Commit-0",
            "content": "import abc\nfrom functools import cached_property\nimport torch\nfrom einops import einsum, rearrange, repeat\nfrom torch import nn\n\n\nclass Projection(nn.Module, abc.ABC):\n    def __init__(self, proj_width: int, num_heads: int, **kwargs):\n        super().__init__()\n        self.proj_width = proj_width\n        self.num_heads = num_heads\n\n    @abc.abstractmethod\n    def forward(self, x, seq_id): ...\n\n\nclass RotaryProjection(Projection):\n    def __init__(self, *, proj_width: int, num_heads: int, max_len: int = 512, base: int = 10000):\n        super().__init__(proj_width, num_heads)\n        assert (\n            self.proj_width % 2 == 0\n        ), f\"proj_width must be even, got {self.proj_width}\"\n        self.register_buffer(\n            \"theta\",\n            1.0\n            / torch.pow(\n                base,\n                torch.arange(0, self.proj_width, 4, dtype=torch.float)\n                / self.proj_width,\n            ),\n            persistent=False,\n        )\n        self.register_buffer(\"cos\", None, persistent=False)\n        self.register_buffer(\"sin\", None, persistent=False)\n        self._init_freq(max_len=max_len)\n\n    def _init_freq(self, max_len: int):\n        if self.cos is None or self.cos.size(-2) < max_len:\n            position = torch.arange(\n                max_len, device=self.theta.device, dtype=self.theta.dtype\n            )\n            m_theta = einsum(position, self.theta,\n                             \"length, width -> length width\")\n            m_theta = repeat(m_theta, \"length width -> length (width 2)\")\n            self.register_buffer(\"cos\", torch.cos(m_theta), persistent=False)\n            self.register_buffer(\"sin\", torch.sin(m_theta), persistent=False)\n\n    @staticmethod\n    def _rotate(x):\n        x1, x2 = rearrange(x, \"... (dim r) -> r ... dim\", r=2)\n        return rearrange([-x2, x1], \"r ... dim -> ... (dim r)\", r=2)  # noqa\n\n    def forward(self, x, seq_id):\n        self._init_freq(max_len=seq_id.max() + 1)\n        rot_cos = self.cos[seq_id]\n        rot_sin = self.sin[seq_id]\n        return rot_cos * x + rot_sin * self._rotate(x)\n\n\nclass QueryKeyProjection(nn.Module):\n    def __init__(self, dim: int, num_heads: int, proj_layer, kwargs=None, partial_factor=None):\n        super().__init__()\n        if partial_factor is not None:\n            assert (\n                0.0 <= partial_factor[0] < partial_factor[1] <= 1.0\n            ), f\"got {partial_factor[0]}, {partial_factor[1]}\"\n        assert num_heads > 0 and dim % num_heads == 0\n\n        self.head_dim = dim // num_heads\n        self.partial_factor = partial_factor\n        self.query_proj = proj_layer(\n            proj_width=self.proj_width,\n            num_heads=num_heads,\n            **(kwargs or {}),\n        )\n        self.key_proj = self.query_proj\n\n    @cached_property\n    def proj_width(self) -> int:\n        if self.partial_factor is None:\n            return self.head_dim\n        return int(self.head_dim * (self.partial_factor[1] - self.partial_factor[0]))\n\n    @cached_property\n    def split_sizes(self):\n        if self.partial_factor is None:\n            return 0, self.head_dim, 0\n        return (\n            int(self.partial_factor[0] * self.head_dim),\n            self.proj_width,\n            int((1.0 - self.partial_factor[1]) * self.head_dim),\n        )\n\n    def forward(self, query, key, query_id, kv_id):\n        if self.partial_factor is not None:\n            queries = list(query.split(self.split_sizes, dim=-1))\n            keys = list(key.split(self.split_sizes, dim=-1))\n            queries[1] = self.query_proj(queries[1], seq_id=query_id)\n            keys[1] = self.key_proj(keys[1], seq_id=kv_id)\n            query = torch.cat(queries, dim=-1)\n            key = torch.cat(keys, dim=-1)\n        else:\n            query = self.query_proj(query, seq_id=query_id)\n            key = self.key_proj(key, seq_id=kv_id)\n        return query, key\n"
        }
    ]
}