{
    "sourceFile": "utils/tools.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1730423037589,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1730423037589,
            "name": "Commit-0",
            "content": "import numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch.distributed as dist\nplt.switch_backend('agg')\n\n\ndef adjust_learning_rate(optimizer, epoch, args):\n    if args.lradj == 'type1':\n        lr_adjust = {epoch: args.learning_rate * (0.1 ** epoch)}\n    elif args.lradj == 'type2':\n        lr_adjust = {epoch: args.learning_rate * (0.5 ** epoch)}\n    elif args.lradj == 'type3':\n        lr_adjust = {epoch: args.learning_rate * (0.9 ** epoch)}\n    elif args.lradj == 'type4':\n        lr_adjust = {epoch: args.learning_rate}\n\n\n    if epoch in lr_adjust.keys():\n        lr = lr_adjust[epoch]\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = lr\n        print('Updating learning rate to {}'.format(lr))\n\n\nclass EarlyStopping:\n    def __init__(self, args, verbose=False, delta=0):\n        self.patience = args.patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.inf\n        self.delta = delta\n        self.dp = args.dp\n        self.ddp = args.ddp\n        if self.ddp:\n            self.local_rank = args.local_rank\n        else:\n            self.local_rank = None\n\n    def __call__(self, val_loss, model, path):\n        score = -val_loss\n        if self.best_score is None:\n            self.best_score = score\n            if self.verbose:\n                print(\n                    f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).')\n            self.val_loss_min = val_loss\n            if self.ddp:\n                if self.local_rank == 0:\n                    self.save_checkpoint(val_loss, model, path)\n                dist.barrier()\n            else:\n                self.save_checkpoint(val_loss, model, path)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            print(\n                f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            if self.ddp:\n                if self.local_rank == 0:\n                    self.save_checkpoint(val_loss, model, path)\n                dist.barrier()\n            else:\n                self.save_checkpoint(val_loss, model, path)\n            if self.verbose:\n                print(\n                    f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).')\n            self.val_loss_min = val_loss\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model, path):\n        if self.dp:\n            model = model.module\n        param_grad_dic = {\n            k: v.requires_grad for (k, v) in model.named_parameters()\n        }\n        state_dict = model.state_dict()\n        for k in list(state_dict.keys()):\n            if k in param_grad_dic.keys() and not param_grad_dic[k]:\n                # delete parameters that do not require gradient\n                del state_dict[k]\n        torch.save(state_dict, path + '/' + f'checkpoint.pth')\n\ndef visual(true, preds=None, name='./pic/test.pdf'):\n    \"\"\"\n    Results visualization\n    \"\"\"\n    plt.figure()\n    plt.plot(true, label='GroundTruth', linewidth=2)\n    if preds is not None:\n        plt.plot(preds, label='Prediction', linewidth=2)\n    plt.legend()\n    plt.savefig(name, bbox_inches='tight')"
        }
    ]
}