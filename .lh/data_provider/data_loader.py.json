{
    "sourceFile": "data_provider/data_loader.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 105,
            "patches": [
                {
                    "date": 1731680707908,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1731680717554,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -500,84 +500,9 @@\n \n     def inverse_transform(self, data):\n         return self.scaler.inverse_transform(data)\n \n-\"\"\"\n-All single-variate series in UTSD are divided into (input-output) windows with a uniform length based on S3.\n-\"\"\"\n-class UTSDataset(Dataset):\n-    def __init__(self, subset_name=r'UTSD-1G', flag='train', split=0.9,\n-                 input_len=None, output_len=None, scale=True, stride=1):\n-        self.input_len = input_len\n-        self.output_len = output_len\n-        self.seq_len = input_len + output_len\n-        assert flag in ['train', 'val']\n-        assert split >= 0 and split <=1.0\n-        type_map = {'train': 0, 'val': 1, 'test': 2}\n-        self.set_type = type_map[flag]\n-        self.flag = flag\n-        self.scale = scale\n-        self.split = split\n-        self.stride = stride\n \n-        self.data_list = []\n-        self.n_window_list = []\n-\n-        self.subset_name = subset_name\n-        self.__read_data__()\n-\n-    def __read_data__(self):\n-        dataset = datasets.load_dataset(\"thuml/UTSD\", self.subset_name, split='train')\n-        # split='train' contains all the time series, which have not been divided into splits, \n-        # you can split them by yourself, or use our default split as train:val = 9:1\n-        print('Indexing dataset...')\n-        for item in tqdm(dataset):\n-            self.scaler = StandardScaler()\n-            data = item['target']\n-            data = np.array(data).reshape(-1, 1)\n-            num_train = int(len(data) * self.split)\n-            border1s = [0, num_train - self.seq_len]\n-            border2s = [num_train, len(data)]\n-\n-            border1 = border1s[self.set_type]\n-            border2 = border2s[self.set_type]\n-\n-            if self.scale:\n-                train_data = data[border1s[0]:border2s[0]]\n-                self.scaler.fit(train_data)\n-                data = self.scaler.transform(data)\n-\n-            data = data[border1:border2]\n-            n_window = (len(data) - self.seq_len) // self.stride + 1\n-            if n_window < 1:\n-                continue\n-\n-            self.data_list.append(data)\n-            self.n_window_list.append(n_window if len(self.n_window_list) == 0 else self.n_window_list[-1] + n_window)\n-\n-\n-    def __getitem__(self, index):\n-        # you can wirte your own processing code here\n-        dataset_index = 0\n-        while index >= self.n_window_list[dataset_index]:\n-            dataset_index += 1\n-\n-        index = index - self.n_window_list[dataset_index - 1] if dataset_index > 0 else index\n-        n_timepoint = (len(self.data_list[dataset_index]) - self.seq_len) // self.stride + 1\n-\n-        s_begin = index % n_timepoint\n-        s_begin = self.stride * s_begin\n-        s_end = s_begin + self.seq_len\n-        p_begin = s_end\n-        p_end = p_begin + self.output_len\n-        seq_x = self.data_list[dataset_index][s_begin:s_end, :]\n-        seq_y = self.data_list[dataset_index][p_begin:p_end, :]\n-\n-        return seq_x, seq_y\n-\n-    def __len__(self):\n-        return self.n_window_list[-1]\n-\n # Download link: https://huggingface.co/datasets/thuml/UTSD\n class UTSD(Dataset):\n     def __init__(self, root_path, flag='train', size=None, data_path='ETTh1.csv', scale=True, nonautoregressive=False, stride=1, split=0.9, test_flag='T'):\n         self.seq_len = size[0]\n"
                },
                {
                    "date": 1731680735706,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -556,10 +556,10 @@\n             self.data_list.append(data)\n             n_window = n_timepoint * n_var\n             self.n_window_list.append(n_window if len(\n                 self.n_window_list) == 0 else self.n_window_list[-1] + n_window)\n-print(\"Total number of windows in merged dataset: \",\n-        self.n_window_list[-1])\n+        print(\"Total number of windows in merged dataset: \",\n+                self.n_window_list[-1])\n \n     def __getitem__(self, index):\n         assert index >= 0\n         # find the location of one dataset by the index\n"
                },
                {
                    "date": 1731680842539,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -503,9 +503,9 @@\n \n \n # Download link: https://huggingface.co/datasets/thuml/UTSD\n class UTSD(Dataset):\n-    def __init__(self, root_path, flag='train', size=None, data_path='ETTh1.csv', scale=True, nonautoregressive=False, stride=1, split=0.9, test_flag='T'):\n+    def __init__(self, root_path, flag='train', size=None, data_path='ETTh1.csv', scale=True, nonautoregressive=False, stride=1, split=0.9, test_flag='T', subset_ratio=0):\n         self.seq_len = size[0]\n         self.input_token_len = size[1]\n         self.output_token_len = size[2]\n         self.context_len = self.seq_len + self.output_token_len\n"
                },
                {
                    "date": 1731680859605,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -503,9 +503,9 @@\n \n \n # Download link: https://huggingface.co/datasets/thuml/UTSD\n class UTSD(Dataset):\n-    def __init__(self, root_path, flag='train', size=None, data_path='ETTh1.csv', scale=True, nonautoregressive=False, stride=1, split=0.9, test_flag='T', subset_ratio=0):\n+    def __init__(self, root_path, flag='train', size=None, data_path='ETTh1.csv', scale=True, nonautoregressive=False, stride=1, split=0.9, test_flag='T', subset_rand_ratio=0):\n         self.seq_len = size[0]\n         self.input_token_len = size[1]\n         self.output_token_len = size[2]\n         self.context_len = self.seq_len + self.output_token_len\n"
                },
                {
                    "date": 1731680886928,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -522,9 +522,9 @@\n         self.root_path = root_path\n         self.__confirm_data__()\n \n     def __confirm_data__(self):\n-        dataset = datasets.load_dataset(\"thuml/UTSD\", self.subset_name, split='train')\n+        dataset = datasets.load_dataset(\"thuml/UTSD\", \"UTSD-1G\", split='train')\n         # split='train' contains all the time series, which have not been divided into splits, \n         # you can split them by yourself, or use our default split as train:val = 9:1\n         print('Indexing dataset...')\n         for item in tqdm(dataset):\n"
                },
                {
                    "date": 1731681067545,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -553,9 +553,9 @@\n             n_timepoint = (\n                 len(data) - self.context_len) // self.stride + 1\n             n_var = data.shape[1]\n             self.data_list.append(data)\n-            n_window = n_timepoint * n_var\n+            n_window = n_timepoint #* n_var\n             self.n_window_list.append(n_window if len(\n                 self.n_window_list) == 0 else self.n_window_list[-1] + n_window)\n         print(\"Total number of windows in merged dataset: \",\n                 self.n_window_list[-1])\n"
                },
                {
                    "date": 1731681083032,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -579,12 +579,10 @@\n         s_end = s_begin + self.seq_len\n         r_begin = s_begin + self.input_token_len\n         r_end = s_end + self.output_token_len\n \n-        seq_x = self.data_list[dataset_index][s_begin:s_end,\n-                                              c_begin:c_begin + 1]\n-        seq_y = self.data_list[dataset_index][r_begin:r_end,\n-                                              c_begin:c_begin + 1]\n+        seq_x = self.data_list[dataset_index][s_begin:s_end]\n+        seq_y = self.data_list[dataset_index][r_begin:r_end]\n         seq_x_mark = torch.zeros((seq_x.shape[0], 1))\n         seq_y_mark = torch.zeros((seq_x.shape[0], 1))\n         return seq_x, seq_y, seq_x_mark, seq_y_mark\n \n"
                },
                {
                    "date": 1731681207638,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -526,9 +526,11 @@\n         dataset = datasets.load_dataset(\"thuml/UTSD\", \"UTSD-1G\", split='train')\n         # split='train' contains all the time series, which have not been divided into splits, \n         # you can split them by yourself, or use our default split as train:val = 9:1\n         print('Indexing dataset...')\n-        for item in tqdm(dataset):\n+        for i,item in tqdm(enumerate(dataset)):\n+            if i>10:\n+                break\n             self.scaler = StandardScaler()\n             data = item['target']\n             data = np.array(data).reshape(-1, 1)\n             num_train = int(len(data) * self.split)\n@@ -572,9 +574,9 @@\n                                1] if dataset_index > 0 else index\n         n_timepoint = (\n             len(self.data_list[dataset_index]) - self.context_len) // self.stride + 1\n \n-        c_begin = index // n_timepoint  # select variable\n+        # c_begin = index // n_timepoint  # select variable\n         s_begin = index % n_timepoint  # select start timestamp\n         s_begin = self.stride * s_begin\n         s_end = s_begin + self.seq_len\n         r_begin = s_begin + self.input_token_len\n"
                },
                {
                    "date": 1731681370058,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -527,9 +527,9 @@\n         # split='train' contains all the time series, which have not been divided into splits, \n         # you can split them by yourself, or use our default split as train:val = 9:1\n         print('Indexing dataset...')\n         for i,item in tqdm(enumerate(dataset)):\n-            if i>10:\n+            if i>100:\n                 break\n             self.scaler = StandardScaler()\n             data = item['target']\n             data = np.array(data).reshape(-1, 1)\n"
                },
                {
                    "date": 1731681393520,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -527,9 +527,9 @@\n         # split='train' contains all the time series, which have not been divided into splits, \n         # you can split them by yourself, or use our default split as train:val = 9:1\n         print('Indexing dataset...')\n         for i,item in tqdm(enumerate(dataset)):\n-            if i>100:\n+            if i>1000:\n                 break\n             self.scaler = StandardScaler()\n             data = item['target']\n             data = np.array(data).reshape(-1, 1)\n"
                },
                {
                    "date": 1731681427889,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -537,8 +537,9 @@\n             num_test = int(len(data) * (1 - self.split) / 2)\n             num_vali = len(data) - num_train - num_test\n             if num_train < self.context_len:\n                 continue\n+            print(i)\n             border1s = [0, num_train - self.seq_len, len(data) - num_test - self.seq_len]\n             border2s = [num_train, num_train + num_vali, len(data)]\n \n             border1 = border1s[self.set_type]\n"
                },
                {
                    "date": 1731681434119,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -527,9 +527,9 @@\n         # split='train' contains all the time series, which have not been divided into splits, \n         # you can split them by yourself, or use our default split as train:val = 9:1\n         print('Indexing dataset...')\n         for i,item in tqdm(enumerate(dataset)):\n-            if i>1000:\n+            if i>5000:\n                 break\n             self.scaler = StandardScaler()\n             data = item['target']\n             data = np.array(data).reshape(-1, 1)\n"
                },
                {
                    "date": 1731681485380,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -537,9 +537,8 @@\n             num_test = int(len(data) * (1 - self.split) / 2)\n             num_vali = len(data) - num_train - num_test\n             if num_train < self.context_len:\n                 continue\n-            print(i)\n             border1s = [0, num_train - self.seq_len, len(data) - num_test - self.seq_len]\n             border2s = [num_train, num_train + num_vali, len(data)]\n \n             border1 = border1s[self.set_type]\n"
                },
                {
                    "date": 1731681496418,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -527,9 +527,9 @@\n         # split='train' contains all the time series, which have not been divided into splits, \n         # you can split them by yourself, or use our default split as train:val = 9:1\n         print('Indexing dataset...')\n         for i,item in tqdm(enumerate(dataset)):\n-            if i>5000:\n+            if i>4000:\n                 break\n             self.scaler = StandardScaler()\n             data = item['target']\n             data = np.array(data).reshape(-1, 1)\n@@ -558,8 +558,9 @@\n             self.data_list.append(data)\n             n_window = n_timepoint #* n_var\n             self.n_window_list.append(n_window if len(\n                 self.n_window_list) == 0 else self.n_window_list[-1] + n_window)\n+        print(self.n_window_list)\n         print(\"Total number of windows in merged dataset: \",\n                 self.n_window_list[-1])\n \n     def __getitem__(self, index):\n"
                },
                {
                    "date": 1731681710720,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -535,9 +535,9 @@\n             data = np.array(data).reshape(-1, 1)\n             num_train = int(len(data) * self.split)\n             num_test = int(len(data) * (1 - self.split) / 2)\n             num_vali = len(data) - num_train - num_test\n-            if num_train < self.context_len:\n+            if num_train < self.context_len or num_test < self.context_len or num_vali < self.context_len:\n                 continue\n             border1s = [0, num_train - self.seq_len, len(data) - num_test - self.seq_len]\n             border2s = [num_train, num_train + num_vali, len(data)]\n \n"
                },
                {
                    "date": 1731681770377,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -503,9 +503,9 @@\n \n \n # Download link: https://huggingface.co/datasets/thuml/UTSD\n class UTSD(Dataset):\n-    def __init__(self, root_path, flag='train', size=None, data_path='ETTh1.csv', scale=True, nonautoregressive=False, stride=1, split=0.9, test_flag='T', subset_rand_ratio=0):\n+    def __init__(self, root_path, flag='train', size=None, data_path='ETTh1.csv', scale=True, nonautoregressive=False, stride=1, split=0.7, test_flag='T', subset_rand_ratio=0):\n         self.seq_len = size[0]\n         self.input_token_len = size[1]\n         self.output_token_len = size[2]\n         self.context_len = self.seq_len + self.output_token_len\n"
                },
                {
                    "date": 1731681795231,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -527,9 +527,9 @@\n         # split='train' contains all the time series, which have not been divided into splits, \n         # you can split them by yourself, or use our default split as train:val = 9:1\n         print('Indexing dataset...')\n         for i,item in tqdm(enumerate(dataset)):\n-            if i>4000:\n+            if i>5000:\n                 break\n             self.scaler = StandardScaler()\n             data = item['target']\n             data = np.array(data).reshape(-1, 1)\n"
                },
                {
                    "date": 1731681820370,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -527,10 +527,8 @@\n         # split='train' contains all the time series, which have not been divided into splits, \n         # you can split them by yourself, or use our default split as train:val = 9:1\n         print('Indexing dataset...')\n         for i,item in tqdm(enumerate(dataset)):\n-            if i>5000:\n-                break\n             self.scaler = StandardScaler()\n             data = item['target']\n             data = np.array(data).reshape(-1, 1)\n             num_train = int(len(data) * self.split)\n"
                },
                {
                    "date": 1731681887145,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -526,9 +526,9 @@\n         dataset = datasets.load_dataset(\"thuml/UTSD\", \"UTSD-1G\", split='train')\n         # split='train' contains all the time series, which have not been divided into splits, \n         # you can split them by yourself, or use our default split as train:val = 9:1\n         print('Indexing dataset...')\n-        for i,item in tqdm(enumerate(dataset)):\n+        for item in tqdm(dataset):\n             self.scaler = StandardScaler()\n             data = item['target']\n             data = np.array(data).reshape(-1, 1)\n             num_train = int(len(data) * self.split)\n@@ -556,9 +556,8 @@\n             self.data_list.append(data)\n             n_window = n_timepoint #* n_var\n             self.n_window_list.append(n_window if len(\n                 self.n_window_list) == 0 else self.n_window_list[-1] + n_window)\n-        print(self.n_window_list)\n         print(\"Total number of windows in merged dataset: \",\n                 self.n_window_list[-1])\n \n     def __getitem__(self, index):\n"
                },
                {
                    "date": 1731682226261,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -526,9 +526,11 @@\n         dataset = datasets.load_dataset(\"thuml/UTSD\", \"UTSD-1G\", split='train')\n         # split='train' contains all the time series, which have not been divided into splits, \n         # you can split them by yourself, or use our default split as train:val = 9:1\n         print('Indexing dataset...')\n-        for item in tqdm(dataset):\n+        for i,item in tqdm(enumerate(dataset)):\n+            if i>20000:\n+                break\n             self.scaler = StandardScaler()\n             data = item['target']\n             data = np.array(data).reshape(-1, 1)\n             num_train = int(len(data) * self.split)\n"
                },
                {
                    "date": 1731682458409,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -527,9 +527,9 @@\n         # split='train' contains all the time series, which have not been divided into splits, \n         # you can split them by yourself, or use our default split as train:val = 9:1\n         print('Indexing dataset...')\n         for i,item in tqdm(enumerate(dataset)):\n-            if i>20000:\n+            if i>10000:\n                 break\n             self.scaler = StandardScaler()\n             data = item['target']\n             data = np.array(data).reshape(-1, 1)\n"
                },
                {
                    "date": 1731682489692,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -527,9 +527,9 @@\n         # split='train' contains all the time series, which have not been divided into splits, \n         # you can split them by yourself, or use our default split as train:val = 9:1\n         print('Indexing dataset...')\n         for i,item in tqdm(enumerate(dataset)):\n-            if i>10000:\n+            if i>15000:\n                 break\n             self.scaler = StandardScaler()\n             data = item['target']\n             data = np.array(data).reshape(-1, 1)\n"
                },
                {
                    "date": 1731682522810,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -527,9 +527,9 @@\n         # split='train' contains all the time series, which have not been divided into splits, \n         # you can split them by yourself, or use our default split as train:val = 9:1\n         print('Indexing dataset...')\n         for i,item in tqdm(enumerate(dataset)):\n-            if i>15000:\n+            if i>20000:\n                 break\n             self.scaler = StandardScaler()\n             data = item['target']\n             data = np.array(data).reshape(-1, 1)\n"
                },
                {
                    "date": 1731727470368,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -583,8 +583,12 @@\n         r_end = s_end + self.output_token_len\n \n         seq_x = self.data_list[dataset_index][s_begin:s_end]\n         seq_y = self.data_list[dataset_index][r_begin:r_end]\n+        seq_y = torch.tensor(seq_y)\n+            seq_y = seq_y.unfold(dimension=0, size=self.output_token_len,\n+                                 step=self.input_token_len).permute(0, 2, 1)\n+            seq_y = seq_y.reshape(seq_y.shape[0] * seq_y.shape[1], -1)\n         seq_x_mark = torch.zeros((seq_x.shape[0], 1))\n         seq_y_mark = torch.zeros((seq_x.shape[0], 1))\n         return seq_x, seq_y, seq_x_mark, seq_y_mark\n \n"
                },
                {
                    "date": 1731727661897,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -527,10 +527,8 @@\n         # split='train' contains all the time series, which have not been divided into splits, \n         # you can split them by yourself, or use our default split as train:val = 9:1\n         print('Indexing dataset...')\n         for i,item in tqdm(enumerate(dataset)):\n-            if i>20000:\n-                break\n             self.scaler = StandardScaler()\n             data = item['target']\n             data = np.array(data).reshape(-1, 1)\n             num_train = int(len(data) * self.split)\n@@ -584,11 +582,11 @@\n \n         seq_x = self.data_list[dataset_index][s_begin:s_end]\n         seq_y = self.data_list[dataset_index][r_begin:r_end]\n         seq_y = torch.tensor(seq_y)\n-            seq_y = seq_y.unfold(dimension=0, size=self.output_token_len,\n-                                 step=self.input_token_len).permute(0, 2, 1)\n-            seq_y = seq_y.reshape(seq_y.shape[0] * seq_y.shape[1], -1)\n+        seq_y = seq_y.unfold(dimension=0, size=self.output_token_len,\n+                                step=self.input_token_len).permute(0, 2, 1)\n+        seq_y = seq_y.reshape(seq_y.shape[0] * seq_y.shape[1], -1)\n         seq_x_mark = torch.zeros((seq_x.shape[0], 1))\n         seq_y_mark = torch.zeros((seq_x.shape[0], 1))\n         return seq_x, seq_y, seq_x_mark, seq_y_mark\n \n"
                },
                {
                    "date": 1731728224934,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -527,8 +527,10 @@\n         # split='train' contains all the time series, which have not been divided into splits, \n         # you can split them by yourself, or use our default split as train:val = 9:1\n         print('Indexing dataset...')\n         for i,item in tqdm(enumerate(dataset)):\n+            if i>20000:\n+                break\n             self.scaler = StandardScaler()\n             data = item['target']\n             data = np.array(data).reshape(-1, 1)\n             num_train = int(len(data) * self.split)\n"
                },
                {
                    "date": 1731729788246,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -527,10 +527,8 @@\n         # split='train' contains all the time series, which have not been divided into splits, \n         # you can split them by yourself, or use our default split as train:val = 9:1\n         print('Indexing dataset...')\n         for i,item in tqdm(enumerate(dataset)):\n-            if i>20000:\n-                break\n             self.scaler = StandardScaler()\n             data = item['target']\n             data = np.array(data).reshape(-1, 1)\n             num_train = int(len(data) * self.split)\n"
                },
                {
                    "date": 1731816609791,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -392,10 +392,10 @@\n         if not self.nonautoregressive:\n             r_begin = s_begin + self.input_token_len\n             r_end = s_end + self.output_token_len\n \n-            seq_x = self.data_x[s_begin:s_end, feat_id:feat_id+1]\n-            seq_y = self.data_y[r_begin:r_end, feat_id:feat_id+1]\n+            seq_x = self.data_x[s_begin:s_end]#, feat_id:feat_id+1]\n+            seq_y = self.data_y[r_begin:r_end]#, feat_id:feat_id+1]\n             seq_y = torch.tensor(seq_y)\n             seq_y = seq_y.unfold(dimension=0, size=self.output_token_len,\n                                  step=self.input_token_len).permute(0, 2, 1)\n             seq_y = seq_y.reshape(seq_y.shape[0] * seq_y.shape[1], -1)\n@@ -408,9 +408,9 @@\n         seq_y_mark = torch.zeros((seq_x.shape[0], 1))\n         return seq_x, seq_y, seq_x_mark, seq_y_mark\n \n     def __len__(self):\n-        return (len(self.data_x) - self.seq_len - self.output_token_len + 1) * self.enc_in\n+        return (len(self.data_x) - self.seq_len - self.output_token_len + 1)# * self.enc_in\n \n     def inverse_transform(self, data):\n         return self.scaler.inverse_transform(data)\n \n"
                },
                {
                    "date": 1731830208738,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -340,9 +340,9 @@\n         return len(self.data_x) - self.seq_len - self.output_token_len + 1\n \n \n class Dataset_ERA5_Pretrain(Dataset):\n-    def __init__(self, root_path, flag='train', size=None, data_path='ETTh1.csv', scale=True, nonautoregressive=False, test_flag='T'):\n+    def __init__(self, root_path, flag='train', size=None, data_path='ETTh1.csv', scale=True, nonautoregressive=False, test_flag='T', subset_rand_ratio):\n         self.seq_len = size[0]\n         self.input_token_len = size[1]\n         self.output_token_len = size[2]\n         self.flag = flag\n"
                },
                {
                    "date": 1731830782908,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -340,9 +340,9 @@\n         return len(self.data_x) - self.seq_len - self.output_token_len + 1\n \n \n class Dataset_ERA5_Pretrain(Dataset):\n-    def __init__(self, root_path, flag='train', size=None, data_path='ETTh1.csv', scale=True, nonautoregressive=False, test_flag='T', subset_rand_ratio):\n+    def __init__(self, root_path, flag='train', size=None, data_path='ETTh1.csv', scale=True, nonautoregressive=False, test_flag='T', subset_rand_ratio=0):\n         self.seq_len = size[0]\n         self.input_token_len = size[1]\n         self.output_token_len = size[2]\n         self.flag = flag\n@@ -354,8 +354,9 @@\n         self.scale = scale\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data_x.shape[-1]\n+        print(self.enc_in,'------------------------')\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1731831058936,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -354,9 +354,8 @@\n         self.scale = scale\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data_x.shape[-1]\n-        print(self.enc_in,'------------------------')\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n@@ -381,11 +380,14 @@\n             self.scaler.fit(train_data)\n             data = self.scaler.transform(df_data)\n         else:\n             data = df_data\n-        self.data_x = data[border1:border2]\n-        self.data_y = data[border1:border2]\n+        self.data = data[border1:border2]\n+        \n+    def __shuffle_data(self):\n+        self.data = np.apply_along_axis(np.random.shuffle, -1, self.data)\n \n+\n     def __getitem__(self, index):\n         feat_id = index // self.tot_len\n         s_begin = index % self.tot_len\n         s_end = s_begin + self.seq_len\n@@ -393,19 +395,19 @@\n         if not self.nonautoregressive:\n             r_begin = s_begin + self.input_token_len\n             r_end = s_end + self.output_token_len\n \n-            seq_x = self.data_x[s_begin:s_end]#, feat_id:feat_id+1]\n-            seq_y = self.data_y[r_begin:r_end]#, feat_id:feat_id+1]\n+            seq_x = self.data[s_begin:s_end]#, feat_id:feat_id+1]\n+            seq_y = self.data[r_begin:r_end]#, feat_id:feat_id+1]\n             seq_y = torch.tensor(seq_y)\n             seq_y = seq_y.unfold(dimension=0, size=self.output_token_len,\n                                  step=self.input_token_len).permute(0, 2, 1)\n             seq_y = seq_y.reshape(seq_y.shape[0] * seq_y.shape[1], -1)\n         else:\n             r_begin = s_end\n             r_end = r_begin + self.output_token_len\n-            seq_x = self.data_x[s_begin:s_end, feat_id:feat_id+1]\n-            seq_y = self.data_y[r_begin:r_end, feat_id:feat_id+1]\n+            seq_x = self.data[s_begin:s_end, feat_id:feat_id+1]\n+            seq_y = self.data[r_begin:r_end, feat_id:feat_id+1]\n         seq_x_mark = torch.zeros((seq_x.shape[0], 1))\n         seq_y_mark = torch.zeros((seq_x.shape[0], 1))\n         return seq_x, seq_y, seq_x_mark, seq_y_mark\n \n"
                },
                {
                    "date": 1731831069375,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -382,12 +382,11 @@\n         else:\n             data = df_data\n         self.data = data[border1:border2]\n         \n-    def __shuffle_data(self):\n+    def __shuffle_data__(self):\n         self.data = np.apply_along_axis(np.random.shuffle, -1, self.data)\n \n-\n     def __getitem__(self, index):\n         feat_id = index // self.tot_len\n         s_begin = index % self.tot_len\n         s_end = s_begin + self.seq_len\n"
                },
                {
                    "date": 1731831764821,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -353,9 +353,9 @@\n         self.data_path = data_path\n         self.scale = scale\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n-        self.enc_in = self.data_x.shape[-1]\n+        self.enc_in = 512#self.data_x.shape[-1]\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n@@ -410,9 +410,9 @@\n         seq_y_mark = torch.zeros((seq_x.shape[0], 1))\n         return seq_x, seq_y, seq_x_mark, seq_y_mark\n \n     def __len__(self):\n-        return (len(self.data_x) - self.seq_len - self.output_token_len + 1)# * self.enc_in\n+        return (len(self.data_x) - self.seq_len - self.output_token_len + 1) * self.enc_in\n \n     def inverse_transform(self, data):\n         return self.scaler.inverse_transform(data)\n \n"
                },
                {
                    "date": 1731831780514,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -394,10 +394,10 @@\n         if not self.nonautoregressive:\n             r_begin = s_begin + self.input_token_len\n             r_end = s_end + self.output_token_len\n \n-            seq_x = self.data[s_begin:s_end]#, feat_id:feat_id+1]\n-            seq_y = self.data[r_begin:r_end]#, feat_id:feat_id+1]\n+            seq_x = self.data[s_begin:s_end, feat_id:feat_id+self.enc_in]\n+            seq_y = self.data[r_begin:r_end, feat_id:feat_id+self.enc_in]\n             seq_y = torch.tensor(seq_y)\n             seq_y = seq_y.unfold(dimension=0, size=self.output_token_len,\n                                  step=self.input_token_len).permute(0, 2, 1)\n             seq_y = seq_y.reshape(seq_y.shape[0] * seq_y.shape[1], -1)\n"
                },
                {
                    "date": 1731831803308,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -353,9 +353,10 @@\n         self.data_path = data_path\n         self.scale = scale\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n-        self.enc_in = 512#self.data_x.shape[-1]\n+        self.enc_in = self.data_x.shape[-1]\n+        self.group_size = 512\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1731831814607,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -395,10 +395,10 @@\n         if not self.nonautoregressive:\n             r_begin = s_begin + self.input_token_len\n             r_end = s_end + self.output_token_len\n \n-            seq_x = self.data[s_begin:s_end, feat_id:feat_id+self.enc_in]\n-            seq_y = self.data[r_begin:r_end, feat_id:feat_id+self.enc_in]\n+            seq_x = self.data[s_begin:s_end, feat_id:feat_id+self.group_size]\n+            seq_y = self.data[r_begin:r_end, feat_id:feat_id+self.group_size]\n             seq_y = torch.tensor(seq_y)\n             seq_y = seq_y.unfold(dimension=0, size=self.output_token_len,\n                                  step=self.input_token_len).permute(0, 2, 1)\n             seq_y = seq_y.reshape(seq_y.shape[0] * seq_y.shape[1], -1)\n"
                },
                {
                    "date": 1731831826247,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -404,10 +404,10 @@\n             seq_y = seq_y.reshape(seq_y.shape[0] * seq_y.shape[1], -1)\n         else:\n             r_begin = s_end\n             r_end = r_begin + self.output_token_len\n-            seq_x = self.data[s_begin:s_end, feat_id:feat_id+1]\n-            seq_y = self.data[r_begin:r_end, feat_id:feat_id+1]\n+            seq_x = self.data[s_begin:s_end, feat_id:feat_id+self.group_size]\n+            seq_y = self.data[r_begin:r_end, feat_id:feat_id+self.group_size]\n         seq_x_mark = torch.zeros((seq_x.shape[0], 1))\n         seq_y_mark = torch.zeros((seq_x.shape[0], 1))\n         return seq_x, seq_y, seq_x_mark, seq_y_mark\n \n"
                },
                {
                    "date": 1731831837273,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -411,9 +411,9 @@\n         seq_y_mark = torch.zeros((seq_x.shape[0], 1))\n         return seq_x, seq_y, seq_x_mark, seq_y_mark\n \n     def __len__(self):\n-        return (len(self.data_x) - self.seq_len - self.output_token_len + 1) * self.enc_in\n+        return (len(self.data_x) - self.seq_len - self.output_token_len + 1) * (self.enc_in//self.group_size)\n \n     def inverse_transform(self, data):\n         return self.scaler.inverse_transform(data)\n \n"
                },
                {
                    "date": 1731831861861,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -411,9 +411,9 @@\n         seq_y_mark = torch.zeros((seq_x.shape[0], 1))\n         return seq_x, seq_y, seq_x_mark, seq_y_mark\n \n     def __len__(self):\n-        return (len(self.data_x) - self.seq_len - self.output_token_len + 1) * (self.enc_in//self.group_size)\n+        return (len(self.data) - self.seq_len - self.output_token_len + 1) * (self.enc_in//self.group_size)\n \n     def inverse_transform(self, data):\n         return self.scaler.inverse_transform(data)\n \n"
                },
                {
                    "date": 1731831872193,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -353,11 +353,11 @@\n         self.data_path = data_path\n         self.scale = scale\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n-        self.enc_in = self.data_x.shape[-1]\n+        self.enc_in = self.data.shape[-1]\n         self.group_size = 512\n-        self.tot_len = len(self.data_x) - self.seq_len - \\\n+        self.tot_len = len(self.data) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n         self.scaler = StandardScaler()\n"
                },
                {
                    "date": 1731831941830,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -382,8 +382,9 @@\n             data = self.scaler.transform(df_data)\n         else:\n             data = df_data\n         self.data = data[border1:border2]\n+        print(self.data.shape)\n         \n     def __shuffle_data__(self):\n         self.data = np.apply_along_axis(np.random.shuffle, -1, self.data)\n \n"
                },
                {
                    "date": 1731831949245,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -386,8 +386,9 @@\n         print(self.data.shape)\n         \n     def __shuffle_data__(self):\n         self.data = np.apply_along_axis(np.random.shuffle, -1, self.data)\n+        print(self.data.shape)\n \n     def __getitem__(self, index):\n         feat_id = index // self.tot_len\n         s_begin = index % self.tot_len\n"
                },
                {
                    "date": 1731832152893,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -385,9 +385,9 @@\n         self.data = data[border1:border2]\n         print(self.data.shape)\n         \n     def __shuffle_data__(self):\n-        self.data = np.apply_along_axis(np.random.shuffle, -1, self.data)\n+        self.data = np.random.Generator.shuffle(self.data, axis=1)\n         print(self.data.shape)\n \n     def __getitem__(self, index):\n         feat_id = index // self.tot_len\n"
                },
                {
                    "date": 1731832435366,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -385,8 +385,10 @@\n         self.data = data[border1:border2]\n         print(self.data.shape)\n         \n     def __shuffle_data__(self):\n+        rng = np.random.default_rng()\n+        rng.shuffle(self.data,axis=-1)\n         self.data = np.random.Generator.shuffle(self.data, axis=1)\n         print(self.data.shape)\n \n     def __getitem__(self, index):\n"
                },
                {
                    "date": 1731832473836,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -382,15 +382,12 @@\n             data = self.scaler.transform(df_data)\n         else:\n             data = df_data\n         self.data = data[border1:border2]\n-        print(self.data.shape)\n         \n     def __shuffle_data__(self):\n         rng = np.random.default_rng()\n         rng.shuffle(self.data,axis=-1)\n-        self.data = np.random.Generator.shuffle(self.data, axis=1)\n-        print(self.data.shape)\n \n     def __getitem__(self, index):\n         feat_id = index // self.tot_len\n         s_begin = index % self.tot_len\n"
                },
                {
                    "date": 1731832557997,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -354,9 +354,9 @@\n         self.scale = scale\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data.shape[-1]\n-        self.group_size = 512\n+        self.group_size = 256\n         self.tot_len = len(self.data) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732008276723,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -433,8 +433,9 @@\n         self.scale = scale\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data_x.shape[-1]\n+        self.group_num = 256\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732008312978,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -500,9 +500,9 @@\n         seq_y_mark = torch.zeros((seq_x.shape[0], 1))\n         return seq_x, seq_y, seq_x_mark, seq_y_mark\n \n     def __len__(self):\n-        return (len(self.data_x) - self.seq_len - self.output_token_len + 1) * self.enc_in\n+        return (len(self.data_x) - self.seq_len - self.output_token_len + 1) * ceil(self.enc_in/self.group_num)\n \n     def inverse_transform(self, data):\n         return self.scaler.inverse_transform(data)\n \n"
                },
                {
                    "date": 1732008514600,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -7,8 +7,9 @@\n import warnings\n warnings.filterwarnings('ignore')\n import datasets\n from tqdm import tqdm\n+import math\n \n class UnivariateDatasetBenchmark(Dataset):\n     def __init__(self, root_path, flag='train', size=None, data_path='ETTh1.csv', scale=True, nonautoregressive=False, test_flag='T', subset_rand_ratio=1.0):\n         self.seq_len = size[0]\n@@ -433,8 +434,9 @@\n         self.scale = scale\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data_x.shape[-1]\n+        print(self.enc_in)\n         self.group_num = 256\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n@@ -500,9 +502,9 @@\n         seq_y_mark = torch.zeros((seq_x.shape[0], 1))\n         return seq_x, seq_y, seq_x_mark, seq_y_mark\n \n     def __len__(self):\n-        return (len(self.data_x) - self.seq_len - self.output_token_len + 1) * ceil(self.enc_in/self.group_num)\n+        return (len(self.data_x) - self.seq_len - self.output_token_len + 1) * math.ceil(self.enc_in/self.group_num)\n \n     def inverse_transform(self, data):\n         return self.scaler.inverse_transform(data)\n \n"
                },
                {
                    "date": 1732008675171,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -420,9 +420,9 @@\n         return self.scaler.inverse_transform(data)\n \n \n class Dataset_ERA5_Pretrain_Test(Dataset):\n-    def __init__(self, root_path, flag='test', size=None, data_path='ETTh1.csv', scale=True, nonautoregressive=False, test_flag='T'):\n+    def __init__(self, root_path, flag='test', size=None, data_path='ETTh1.csv', scale=True, nonautoregressive=False, test_flag='T',subset_rand_ratio=0):\n         self.seq_len = size[0]\n         self.input_token_len = size[1]\n         self.output_token_len = size[2]\n         self.test_flag = test_flag\n"
                },
                {
                    "date": 1732008764775,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -486,10 +486,10 @@\n         if not self.nonautoregressive:\n             r_begin = s_begin + self.input_token_len\n             r_end = s_end + self.output_token_len\n \n-            seq_x = self.data_x[s_begin:s_end, feat_id:feat_id+1]\n-            seq_y = self.data_y[r_begin:r_end, feat_id:feat_id+1]\n+            seq_x = self.data_x[s_begin:s_end, feat_id:feat_id+self.group_num]\n+            seq_y = self.data_y[r_begin:r_end, feat_id:feat_id+self.group_num]\n             seq_y = torch.tensor(seq_y)\n             seq_y = seq_y.unfold(dimension=0, size=self.output_token_len,\n                                  step=self.input_token_len).permute(0, 2, 1)\n             seq_y = seq_y.reshape(seq_y.shape[0] * seq_y.shape[1], -1)\n"
                },
                {
                    "date": 1732008790367,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -435,9 +435,9 @@\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data_x.shape[-1]\n         print(self.enc_in)\n-        self.group_num = 256\n+        self.group_num = 246\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732095388042,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -435,9 +435,9 @@\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data_x.shape[-1]\n         print(self.enc_in)\n-        self.group_num = 246\n+        self.group_num = 1\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732095590021,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -435,9 +435,9 @@\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data_x.shape[-1]\n         print(self.enc_in)\n-        self.group_num = 1\n+        self.group_num = 128\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732095618951,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -435,9 +435,9 @@\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data_x.shape[-1]\n         print(self.enc_in)\n-        self.group_num = 128\n+        self.group_num = 246*2\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732095684346,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -435,9 +435,9 @@\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data_x.shape[-1]\n         print(self.enc_in)\n-        self.group_num = 246*2\n+        self.group_num = 123\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732095791373,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -435,9 +435,9 @@\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data_x.shape[-1]\n         print(self.enc_in)\n-        self.group_num = 123\n+        self.group_num = 128\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732095856396,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -435,9 +435,9 @@\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data_x.shape[-1]\n         print(self.enc_in)\n-        self.group_num = 128\n+        self.group_num = 246*2\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732107783857,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -435,9 +435,9 @@\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data_x.shape[-1]\n         print(self.enc_in)\n-        self.group_num = 246*2\n+        self.group_num = 123\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732108146230,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -435,9 +435,9 @@\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data_x.shape[-1]\n         print(self.enc_in)\n-        self.group_num = 123\n+        self.group_num = 246*4\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732110655834,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -355,9 +355,9 @@\n         self.scale = scale\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data.shape[-1]\n-        self.group_size = 256\n+        self.group_size = 512\n         self.tot_len = len(self.data) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732151164211,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -435,9 +435,9 @@\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data_x.shape[-1]\n         print(self.enc_in)\n-        self.group_num = 246*4\n+        self.group_num = 246*2\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732151172887,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -435,9 +435,9 @@\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data_x.shape[-1]\n         print(self.enc_in)\n-        self.group_num = 246*2\n+        self.group_num = 246*4\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732154559926,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -435,9 +435,9 @@\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data_x.shape[-1]\n         print(self.enc_in)\n-        self.group_num = 246*4\n+        self.group_num = 246*2\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732156999469,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -435,9 +435,9 @@\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data_x.shape[-1]\n         print(self.enc_in)\n-        self.group_num = 246*2\n+        self.group_num = 246\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732157829838,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -435,9 +435,9 @@\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data_x.shape[-1]\n         print(self.enc_in)\n-        self.group_num = 246\n+        self.group_num = 123\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732160767784,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -435,9 +435,9 @@\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data_x.shape[-1]\n         print(self.enc_in)\n-        self.group_num = 123\n+        self.group_num = 246*4\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732162104476,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -435,9 +435,9 @@\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data_x.shape[-1]\n         print(self.enc_in)\n-        self.group_num = 246*4\n+        self.group_num = 123\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732250051963,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -435,9 +435,9 @@\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data_x.shape[-1]\n         print(self.enc_in)\n-        self.group_num = 123\n+        self.group_num = 246*4\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732274203214,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -435,9 +435,9 @@\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data_x.shape[-1]\n         print(self.enc_in)\n-        self.group_num = 246*4\n+        self.group_num = 123#246*4\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732282486299,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -355,9 +355,9 @@\n         self.scale = scale\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data.shape[-1]\n-        self.group_size = 512\n+        self.group_size = 128\n         self.tot_len = len(self.data) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n@@ -435,9 +435,9 @@\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data_x.shape[-1]\n         print(self.enc_in)\n-        self.group_num = 123#246*4\n+        self.group_num = 246*4\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732283520574,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -355,9 +355,9 @@\n         self.scale = scale\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data.shape[-1]\n-        self.group_size = 128\n+        self.group_size = 256\n         self.tot_len = len(self.data) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732283639775,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -355,9 +355,9 @@\n         self.scale = scale\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data.shape[-1]\n-        self.group_size = 256\n+        self.group_size = 128\n         self.tot_len = len(self.data) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732325412845,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -355,9 +355,9 @@\n         self.scale = scale\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data.shape[-1]\n-        self.group_size = 128\n+        self.group_size = 246*2\n         self.tot_len = len(self.data) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732325450239,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -355,9 +355,9 @@\n         self.scale = scale\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data.shape[-1]\n-        self.group_size = 246*2\n+        self.group_size = 246\n         self.tot_len = len(self.data) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732325472185,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -355,9 +355,9 @@\n         self.scale = scale\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data.shape[-1]\n-        self.group_size = 246\n+        self.group_size = 123\n         self.tot_len = len(self.data) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732325481277,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -355,9 +355,9 @@\n         self.scale = scale\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data.shape[-1]\n-        self.group_size = 123\n+        self.group_size = 128\n         self.tot_len = len(self.data) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732325487092,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -435,9 +435,9 @@\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data_x.shape[-1]\n         print(self.enc_in)\n-        self.group_num = 246*4\n+        self.group_num = 246*2\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732336646098,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -435,9 +435,9 @@\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data_x.shape[-1]\n         print(self.enc_in)\n-        self.group_num = 246*2\n+        self.group_num = 246\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732343507561,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -355,9 +355,9 @@\n         self.scale = scale\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data.shape[-1]\n-        self.group_size = 128\n+        self.group_size = 1\n         self.tot_len = len(self.data) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732408972451,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -435,9 +435,9 @@\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data_x.shape[-1]\n         print(self.enc_in)\n-        self.group_num = 246\n+        self.group_num = 1\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732423030787,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -355,9 +355,9 @@\n         self.scale = scale\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data.shape[-1]\n-        self.group_size = 1\n+        self.group_size = 16\n         self.tot_len = len(self.data) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732433385864,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -435,9 +435,9 @@\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data_x.shape[-1]\n         print(self.enc_in)\n-        self.group_num = 1\n+        self.group_num = 24\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732433682052,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -435,9 +435,9 @@\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data_x.shape[-1]\n         print(self.enc_in)\n-        self.group_num = 24\n+        self.group_num = 8\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732434370596,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -435,9 +435,9 @@\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data_x.shape[-1]\n         print(self.enc_in)\n-        self.group_num = 8\n+        self.group_num = 16\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732434471709,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -435,9 +435,9 @@\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data_x.shape[-1]\n         print(self.enc_in)\n-        self.group_num = 16\n+        self.group_num = 4\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732434863360,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -435,9 +435,9 @@\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data_x.shape[-1]\n         print(self.enc_in)\n-        self.group_num = 4\n+        self.group_num = 1\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732434909613,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -435,9 +435,9 @@\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data_x.shape[-1]\n         print(self.enc_in)\n-        self.group_num = 1\n+        self.group_num = 2\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732665588550,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -355,9 +355,9 @@\n         self.scale = scale\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data.shape[-1]\n-        self.group_size = 16\n+        self.group_size = 4\n         self.tot_len = len(self.data) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732680602206,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -435,9 +435,9 @@\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data_x.shape[-1]\n         print(self.enc_in)\n-        self.group_num = 2\n+        self.group_num = 4\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732687742879,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -435,9 +435,9 @@\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data_x.shape[-1]\n         print(self.enc_in)\n-        self.group_num = 4\n+        self.group_num = 8\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732690892543,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -435,9 +435,9 @@\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data_x.shape[-1]\n         print(self.enc_in)\n-        self.group_num = 8\n+        self.group_num = 2\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732708608108,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -355,9 +355,9 @@\n         self.scale = scale\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data.shape[-1]\n-        self.group_size = 4\n+        self.group_size = 1\n         self.tot_len = len(self.data) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732774543640,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -435,9 +435,9 @@\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data_x.shape[-1]\n         print(self.enc_in)\n-        self.group_num = 2\n+        self.group_num = 1\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732802819125,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -355,9 +355,9 @@\n         self.scale = scale\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data.shape[-1]\n-        self.group_size = 1\n+        self.group_size = 512\n         self.tot_len = len(self.data) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732873311660,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -435,9 +435,9 @@\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data_x.shape[-1]\n         print(self.enc_in)\n-        self.group_num = 1\n+        self.group_num = 123\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732874209690,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -435,9 +435,9 @@\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data_x.shape[-1]\n         print(self.enc_in)\n-        self.group_num = 123\n+        self.group_num = 246*4\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1732880630773,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -435,9 +435,9 @@\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data_x.shape[-1]\n         print(self.enc_in)\n-        self.group_num = 246*4\n+        self.group_num = 1#246*4\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1733149616760,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -534,8 +534,10 @@\n         # split='train' contains all the time series, which have not been divided into splits, \n         # you can split them by yourself, or use our default split as train:val = 9:1\n         print('Indexing dataset...')\n         for i,item in tqdm(enumerate(dataset)):\n+            if i>10000:\n+                break\n             self.scaler = StandardScaler()\n             data = item['target']\n             data = np.array(data).reshape(-1, 1)\n             num_train = int(len(data) * self.split)\n"
                },
                {
                    "date": 1733149658622,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -534,10 +534,8 @@\n         # split='train' contains all the time series, which have not been divided into splits, \n         # you can split them by yourself, or use our default split as train:val = 9:1\n         print('Indexing dataset...')\n         for i,item in tqdm(enumerate(dataset)):\n-            if i>10000:\n-                break\n             self.scaler = StandardScaler()\n             data = item['target']\n             data = np.array(data).reshape(-1, 1)\n             num_train = int(len(data) * self.split)\n"
                },
                {
                    "date": 1733193141419,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -559,8 +559,9 @@\n             data = data[border1:border2]\n             n_timepoint = (\n                 len(data) - self.context_len) // self.stride + 1\n             n_var = data.shape[1]\n+            print(n_timepoint, n_var)\n             self.data_list.append(data)\n             n_window = n_timepoint #* n_var\n             self.n_window_list.append(n_window if len(\n                 self.n_window_list) == 0 else self.n_window_list[-1] + n_window)\n"
                },
                {
                    "date": 1733451756137,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -355,9 +355,9 @@\n         self.scale = scale\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data.shape[-1]\n-        self.group_size = 512\n+        self.group_size = 128\n         self.tot_len = len(self.data) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1733451778900,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -355,9 +355,9 @@\n         self.scale = scale\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data.shape[-1]\n-        self.group_size = 128\n+        self.group_size = 123\n         self.tot_len = len(self.data) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1733451908232,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -355,9 +355,9 @@\n         self.scale = scale\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data.shape[-1]\n-        self.group_size = 123\n+        self.group_size = 512\n         self.tot_len = len(self.data) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n@@ -435,9 +435,9 @@\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data_x.shape[-1]\n         print(self.enc_in)\n-        self.group_num = 1#246*4\n+        self.group_num = 123#246*4\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                },
                {
                    "date": 1733452498342,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -435,9 +435,9 @@\n         self.nonautoregressive = nonautoregressive\n         self.__read_data__()\n         self.enc_in = self.data_x.shape[-1]\n         print(self.enc_in)\n-        self.group_num = 123#246*4\n+        self.group_num = 246*4\n         self.tot_len = len(self.data_x) - self.seq_len - \\\n             self.output_token_len + 1\n \n     def __read_data__(self):\n"
                }
            ],
            "date": 1731680707908,
            "name": "Commit-0",
            "content": "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\nwarnings.filterwarnings('ignore')\nimport datasets\nfrom tqdm import tqdm\n\nclass UnivariateDatasetBenchmark(Dataset):\n    def __init__(self, root_path, flag='train', size=None, data_path='ETTh1.csv', scale=True, nonautoregressive=False, test_flag='T', subset_rand_ratio=1.0):\n        self.seq_len = size[0]\n        self.input_token_len = size[1]\n        self.output_token_len = size[2]\n        self.flag = flag\n        assert flag in ['train', 'test', 'val']\n        type_map = {'train': 0, 'val': 1, 'test': 2}\n        self.set_type = type_map[flag]\n        self.root_path = root_path\n        self.data_path = data_path\n        self.data_type = data_path.split('.')[0]\n        self.scale = scale\n        self.nonautoregressive = nonautoregressive\n        self.subset_rand_ratio = subset_rand_ratio\n        if self.set_type == 0:\n            self.internal = int(1 // self.subset_rand_ratio)\n        else:\n            self.internal = 1\n        self.__read_data__()\n\n    def __read_data__(self):\n        self.scaler = StandardScaler()\n        dataset_file_path = os.path.join(self.root_path, self.data_path)\n        if dataset_file_path.endswith('.csv'):\n            df_raw = pd.read_csv(dataset_file_path)\n        elif dataset_file_path.endswith('.txt'):\n            df_raw = []\n            with open(dataset_file_path, \"r\", encoding='utf-8') as f:\n                for line in f.readlines():\n                    line = line.strip('\\n').split(',')\n                    data_line = np.stack([float(i) for i in line])\n                    df_raw.append(data_line)\n            df_raw = np.stack(df_raw, 0)\n            df_raw = pd.DataFrame(df_raw)\n        elif dataset_file_path.endswith('.npz'):\n            data = np.load(dataset_file_path, allow_pickle=True)\n            data = data['data'][:, :, 0]\n            df_raw = pd.DataFrame(data)\n        elif dataset_file_path.endswith('.npy'):\n            data = np.load(dataset_file_path)\n            df_raw = pd.DataFrame(data)\n        else:\n            raise ValueError('Unknown data format: {}'.format(dataset_file_path))\n\n        if self.data_type == 'ETTh' or self.data_type == 'ETTh1' or self.data_type == 'ETTh2':\n            border1s = [0, 12 * 30 * 24 - self.seq_len, 12 * 30 * 24 + 4 * 30 * 24 - self.seq_len]\n            border2s = [12 * 30 * 24, 12 * 30 * 24 + 4 * 30 * 24, 12 * 30 * 24 + 8 * 30 * 24]\n        elif self.data_type == 'ETTm' or self.data_type == 'ETTm1' or self.data_type == 'ETTm2':\n            border1s = [0, 12 * 30 * 24 * 4 - self.seq_len, 12 * 30 * 24 * 4 + 4 * 30 * 24 * 4 - self.seq_len]\n            border2s = [12 * 30 * 24 * 4, 12 * 30 * 24 * 4 + 4 * 30 * 24 * 4, 12 * 30 * 24 * 4 + 8 * 30 * 24 * 4]\n        else:\n            data_len = len(df_raw)\n            num_train = int(data_len * 0.7)\n            num_test = int(data_len * 0.2)\n            num_vali = data_len - num_train - num_test\n            border1s = [0, num_train - self.seq_len, data_len - num_test - self.seq_len]\n            border2s = [num_train, num_train + num_vali, data_len]\n\n        border1 = border1s[self.set_type]\n        border2 = border2s[self.set_type]\n\n        if isinstance(df_raw[df_raw.columns[0]][2], str):\n            data = df_raw[df_raw.columns[1:]].values\n        else:\n            data = df_raw.values\n\n        if self.scale:\n            train_data = data[border1s[0]:border2s[0]]\n            self.scaler.fit(train_data)\n            data = self.scaler.transform(data)\n\n        self.data_x = data[border1:border2]\n        self.data_y = data[border1:border2]\n        \n        self.n_var = self.data_x.shape[-1]\n        self.n_timepoint =  len(self.data_x) - self.seq_len - self.output_token_len + 1\n\n    def __getitem__(self, index):\n        feat_id = index // self.n_timepoint\n        s_begin = index % self.n_timepoint\n        s_end = s_begin + self.seq_len\n\n        if not self.nonautoregressive:\n            r_begin = s_begin + self.input_token_len\n            r_end = s_end + self.output_token_len\n\n            seq_x = self.data_x[s_begin:s_end, feat_id:feat_id+1]\n            seq_y = self.data_y[r_begin:r_end, feat_id:feat_id+1]\n            seq_y = torch.tensor(seq_y)\n            seq_y = seq_y.unfold(dimension=0, size=self.output_token_len,\n                                 step=self.input_token_len).permute(0, 2, 1)\n            seq_y = seq_y.reshape(seq_y.shape[0] * seq_y.shape[1], -1)\n        else:\n            r_begin = s_end\n            r_end = r_begin + self.output_token_len\n            seq_x = self.data_x[s_begin:s_end, feat_id:feat_id+1]\n            seq_y = self.data_y[r_begin:r_end, feat_id:feat_id+1]\n        seq_x_mark = torch.zeros((seq_x.shape[0], 1))\n        seq_y_mark = torch.zeros((seq_x.shape[0], 1))\n        return seq_x, seq_y, seq_x_mark, seq_y_mark\n\n    def __len__(self):\n        if self.set_type == 0:\n            return max(int(self.n_var * self.n_timepoint * self.subset_rand_ratio), 1)\n        else:\n            return int(self.n_var * self.n_timepoint)\n\n    def inverse_transform(self, data):\n        return self.scaler.inverse_transform(data)\n\n\nclass MultivariateDatasetBenchmark(Dataset):\n    def __init__(self, root_path, flag='train', size=None, data_path='ETTh1.csv', scale=True, nonautoregressive=False, test_flag='T', subset_rand_ratio=1.0):\n        self.seq_len = size[0]\n        self.input_token_len = size[1]\n        self.output_token_len = size[2]\n        self.flag = flag\n        assert flag in ['train', 'test', 'val']\n        type_map = {'train': 0, 'val': 1, 'test': 2}\n        self.set_type = type_map[flag]\n        self.root_path = root_path\n        self.data_path = data_path\n        self.data_type = data_path.split('.')[0]\n        self.scale = scale\n        self.nonautoregressive = nonautoregressive\n        self.subset_rand_ratio = subset_rand_ratio\n        if self.set_type == 0:\n            self.internal = int(1 // self.subset_rand_ratio)\n        else:\n            self.internal = 1\n        self.__read_data__()\n\n    def __read_data__(self):\n        self.scaler = StandardScaler()\n        dataset_file_path = os.path.join(self.root_path, self.data_path)\n        if dataset_file_path.endswith('.csv'):\n            df_raw = pd.read_csv(dataset_file_path)\n        elif dataset_file_path.endswith('.txt'):\n            df_raw = []\n            with open(dataset_file_path, \"r\", encoding='utf-8') as f:\n                for line in f.readlines():\n                    line = line.strip('\\n').split(',')\n                    data_line = np.stack([float(i) for i in line])\n                    df_raw.append(data_line)\n            df_raw = np.stack(df_raw, 0)\n            df_raw = pd.DataFrame(df_raw)\n        elif dataset_file_path.endswith('.npz'):\n            data = np.load(dataset_file_path, allow_pickle=True)\n            data = data['data'][:, :, 0]\n            df_raw = pd.DataFrame(data)\n        elif dataset_file_path.endswith('.npy'):\n            data = np.load(dataset_file_path)\n            df_raw = pd.DataFrame(data)\n        else:\n            raise ValueError('Unknown data format: {}'.format(dataset_file_path))\n\n        if self.data_type == 'ETTh' or self.data_type == 'ETTh1' or self.data_type == 'ETTh2':\n            border1s = [0, 12 * 30 * 24 - self.seq_len, 12 * 30 * 24 + 4 * 30 * 24 - self.seq_len]\n            border2s = [12 * 30 * 24, 12 * 30 * 24 + 4 * 30 * 24, 12 * 30 * 24 + 8 * 30 * 24]\n        elif self.data_type == 'ETTm' or self.data_type == 'ETTm1' or self.data_type == 'ETTm2':\n            border1s = [0, 12 * 30 * 24 * 4 - self.seq_len, 12 * 30 * 24 * 4 + 4 * 30 * 24 * 4 - self.seq_len]\n            border2s = [12 * 30 * 24 * 4, 12 * 30 * 24 * 4 + 4 * 30 * 24 * 4, 12 * 30 * 24 * 4 + 8 * 30 * 24 * 4]\n        else:\n            data_len = len(df_raw)\n            num_train = int(data_len * 0.7)\n            num_test = int(data_len * 0.2)\n            num_vali = data_len - num_train - num_test\n            border1s = [0, num_train - self.seq_len, data_len - num_test - self.seq_len]\n            border2s = [num_train, num_train + num_vali, data_len]\n\n        border1 = border1s[self.set_type]\n        border2 = border2s[self.set_type]\n\n        if isinstance(df_raw[df_raw.columns[0]][2], str):\n            data = df_raw[df_raw.columns[1:]].values\n        else:\n            data = df_raw.values\n\n        if self.scale:\n            train_data = data[border1s[0]:border2s[0]]\n            self.scaler.fit(train_data)\n            data = self.scaler.transform(data)\n\n        self.data_x = data[border1:border2]\n        self.data_y = data[border1:border2]\n        \n        self.n_var = self.data_x.shape[-1]\n        self.n_timepoint =  len(self.data_x) - self.seq_len - self.output_token_len + 1\n        print(self.n_var, self.n_timepoint)\n\n    def __getitem__(self, index):\n        s_begin = index\n        s_end = s_begin + self.seq_len\n\n        if not self.nonautoregressive:\n            r_begin = s_begin + self.input_token_len\n            r_end = s_end + self.output_token_len\n            seq_x = self.data_x[s_begin:s_end]\n            seq_y = self.data_y[r_begin:r_end]\n            seq_y = torch.tensor(seq_y)\n            seq_y = seq_y.unfold(dimension=0, size=self.output_token_len,\n                                 step=self.input_token_len).permute(0, 2, 1)\n            seq_y = seq_y.reshape(seq_y.shape[0] * seq_y.shape[1], -1)\n        else:\n            r_begin = s_end\n            r_end = r_begin + self.output_token_len\n            seq_x = self.data_x[s_begin:s_end]\n            seq_y = self.data_y[r_begin:r_end]\n        seq_x_mark = torch.zeros((seq_x.shape[0], 1))\n        seq_y_mark = torch.zeros((seq_x.shape[0], 1))\n        return seq_x, seq_y, seq_x_mark, seq_y_mark\n\n    def __len__(self):\n        if self.set_type == 0:\n            return max(int(self.n_timepoint * self.subset_rand_ratio), 1)\n        else:\n            return self.n_timepoint\n\n    def inverse_transform(self, data):\n        return self.scaler.inverse_transform(data)\n\n\nclass Global_Temp(Dataset):\n    def __init__(self, root_path, flag='train', size=None, data_path='ETTh1.csv', scale=True, nonautoregressive=False, test_flag='T'):\n        self.seq_len = size[0]\n        self.input_token_len = size[1]\n        self.output_token_len = size[2]\n        self.flag = flag\n        assert flag in ['train', 'test', 'val']\n        type_map = {'train': 0, 'val': 1, 'test': 2}\n        self.set_type = type_map[flag]\n        self.root_path = root_path\n        self.data_path = data_path\n        self.scale = scale\n        self.nonautoregressive = nonautoregressive\n        self.__read_data__()\n\n    def __read_data__(self):\n        self.raw_data = np.load(os.path.join(self.root_path,\n                                             \"temp_global_hourly_\" + self.flag + \".npy\"),\n                                allow_pickle=True)\n        raw_data = self.raw_data\n        data_len, station, feat = raw_data.shape\n        raw_data = raw_data.reshape(data_len, station * feat)\n        data = raw_data.astype(np.float)\n\n        self.data_x = data\n        self.data_y = data\n\n    def __getitem__(self, index):\n        s_begin = index\n        s_end = s_begin + self.seq_len\n\n        if not self.nonautoregressive:\n            r_begin = s_begin + self.input_token_len\n            r_end = s_end + self.output_token_len\n\n            seq_x = self.data_x[s_begin:s_end]\n            seq_y = self.data_y[r_begin:r_end]\n            seq_y = torch.tensor(seq_y)\n            seq_y = seq_y.unfold(dimension=0, size=self.output_token_len,\n                                 step=self.input_token_len).permute(0, 2, 1)\n            seq_y = seq_y.reshape(seq_y.shape[0] * seq_y.shape[1], -1)\n        else:\n            r_begin = s_end\n            r_end = r_begin + self.output_token_len\n            seq_x = self.data_x[s_begin:s_end]\n            seq_y = self.data_y[r_begin:r_end]\n        seq_x_mark = torch.zeros((seq_x.shape[0], 1))\n        seq_y_mark = torch.zeros((seq_x.shape[0], 1))\n        return seq_x, seq_y, seq_x_mark, seq_y_mark\n\n    def __len__(self):\n        return len(self.data_x) - self.seq_len - self.output_token_len + 1\n\n\nclass Global_Wind(Dataset):\n    def __init__(self, root_path, flag='train', size=None, data_path='ETTh1.csv', scale=True, nonautoregressive=False, test_flag='T'):\n        self.seq_len = size[0]\n        self.input_token_len = size[1]\n        self.output_token_len = size[2]\n        self.flag = flag\n        assert flag in ['train', 'test', 'val']\n        type_map = {'train': 0, 'val': 1, 'test': 2}\n        self.set_type = type_map[flag]\n        self.root_path = root_path\n        self.data_path = data_path\n        self.scale = scale\n        self.nonautoregressive = nonautoregressive\n        self.__read_data__()\n\n    def __read_data__(self):\n        self.raw_data = np.load(os.path.join(self.root_path,\n                                             \"wind_global_hourly_\" + self.flag + \".npy\"),\n                                allow_pickle=True)\n        raw_data = self.raw_data\n        data_len, station, feat = raw_data.shape\n        raw_data = raw_data.reshape(data_len, station * feat)\n        data = raw_data.astype(np.float)\n\n        self.data_x = data\n        self.data_y = data\n\n    def __getitem__(self, index):\n        s_begin = index\n        s_end = s_begin + self.seq_len\n\n        if not self.nonautoregressive:\n            r_begin = s_begin + self.input_token_len\n            r_end = s_end + self.output_token_len\n\n            seq_x = self.data_x[s_begin:s_end]\n            seq_y = self.data_y[r_begin:r_end]\n            seq_y = torch.tensor(seq_y)\n            seq_y = seq_y.unfold(dimension=0, size=self.output_token_len,\n                                 step=self.input_token_len).permute(0, 2, 1)\n            seq_y = seq_y.reshape(seq_y.shape[0] * seq_y.shape[1], -1)\n        else:\n            r_begin = s_end\n            r_end = r_begin + self.output_token_len\n            seq_x = self.data_x[s_begin:s_end]\n            seq_y = self.data_y[r_begin:r_end]\n        seq_x_mark = torch.zeros((seq_x.shape[0], 1))\n        seq_y_mark = torch.zeros((seq_x.shape[0], 1))\n        return seq_x, seq_y, seq_x_mark, seq_y_mark\n\n    def __len__(self):\n        return len(self.data_x) - self.seq_len - self.output_token_len + 1\n\n\nclass Dataset_ERA5_Pretrain(Dataset):\n    def __init__(self, root_path, flag='train', size=None, data_path='ETTh1.csv', scale=True, nonautoregressive=False, test_flag='T'):\n        self.seq_len = size[0]\n        self.input_token_len = size[1]\n        self.output_token_len = size[2]\n        self.flag = flag\n        assert flag in ['train', 'test', 'val']\n        type_map = {'train': 0, 'val': 1, 'test': 2}\n        self.set_type = type_map[flag]\n        self.root_path = root_path\n        self.data_path = data_path\n        self.scale = scale\n        self.nonautoregressive = nonautoregressive\n        self.__read_data__()\n        self.enc_in = self.data_x.shape[-1]\n        self.tot_len = len(self.data_x) - self.seq_len - \\\n            self.output_token_len + 1\n\n    def __read_data__(self):\n        self.scaler = StandardScaler()\n        df_raw = np.load(os.path.join(self.root_path, self.data_path))\n        # split only the train set\n        L, S = df_raw.shape\n        Train_S = int(S * 0.8)\n        df_raw = df_raw[:, :Train_S]\n        num_train = int(len(df_raw) * 0.7)\n        num_test = int(len(df_raw) * 0.2)\n        num_vali = len(df_raw) - num_train - num_test\n        border1s = [0, num_train - self.seq_len,\n                    len(df_raw) - num_test - self.seq_len]\n        border2s = [num_train, num_train + num_vali, len(df_raw)]\n        border1 = border1s[self.set_type]\n        border2 = border2s[self.set_type]\n\n        df_data = df_raw\n        if self.scale:\n            train_data = df_data[border1s[0]:border2s[0]]\n            self.scaler.fit(train_data)\n            data = self.scaler.transform(df_data)\n        else:\n            data = df_data\n        self.data_x = data[border1:border2]\n        self.data_y = data[border1:border2]\n\n    def __getitem__(self, index):\n        feat_id = index // self.tot_len\n        s_begin = index % self.tot_len\n        s_end = s_begin + self.seq_len\n\n        if not self.nonautoregressive:\n            r_begin = s_begin + self.input_token_len\n            r_end = s_end + self.output_token_len\n\n            seq_x = self.data_x[s_begin:s_end, feat_id:feat_id+1]\n            seq_y = self.data_y[r_begin:r_end, feat_id:feat_id+1]\n            seq_y = torch.tensor(seq_y)\n            seq_y = seq_y.unfold(dimension=0, size=self.output_token_len,\n                                 step=self.input_token_len).permute(0, 2, 1)\n            seq_y = seq_y.reshape(seq_y.shape[0] * seq_y.shape[1], -1)\n        else:\n            r_begin = s_end\n            r_end = r_begin + self.output_token_len\n            seq_x = self.data_x[s_begin:s_end, feat_id:feat_id+1]\n            seq_y = self.data_y[r_begin:r_end, feat_id:feat_id+1]\n        seq_x_mark = torch.zeros((seq_x.shape[0], 1))\n        seq_y_mark = torch.zeros((seq_x.shape[0], 1))\n        return seq_x, seq_y, seq_x_mark, seq_y_mark\n\n    def __len__(self):\n        return (len(self.data_x) - self.seq_len - self.output_token_len + 1) * self.enc_in\n\n    def inverse_transform(self, data):\n        return self.scaler.inverse_transform(data)\n\n\nclass Dataset_ERA5_Pretrain_Test(Dataset):\n    def __init__(self, root_path, flag='test', size=None, data_path='ETTh1.csv', scale=True, nonautoregressive=False, test_flag='T'):\n        self.seq_len = size[0]\n        self.input_token_len = size[1]\n        self.output_token_len = size[2]\n        self.test_flag = test_flag\n        assert test_flag in ['T', 'V', 'TandV']\n        type_map = {'T': 0, 'V': 1, 'TandV': 2}\n        self.test_type = type_map[flag]\n        self.root_path = root_path\n        self.data_path = data_path\n        self.scale = scale\n        self.nonautoregressive = nonautoregressive\n        self.__read_data__()\n        self.enc_in = self.data_x.shape[-1]\n        self.tot_len = len(self.data_x) - self.seq_len - \\\n            self.output_token_len + 1\n\n    def __read_data__(self):\n        self.scaler = StandardScaler()\n        df_raw = np.load(os.path.join(self.root_path, self.data_path))\n        # split only the train set\n        L, S = df_raw.shape\n        if self.test_type == 0:\n            Train_S = int(S * 0.8)\n            df_raw = df_raw[:, :Train_S]\n            num_train = int(len(df_raw) * 0.7)\n            num_test = int(len(df_raw) * 0.2)\n            num_vali = len(df_raw) - num_train - num_test\n            border1s = [0, num_train - self.seq_len,\n                        len(df_raw) - num_test - self.seq_len]\n            border2s = [num_train, num_train + num_vali, len(df_raw)]\n            data = df_raw\n            border1 = border1s[-1]\n            border2 = border2s[-1]\n\n            self.data_x = data[border1:border2]\n            self.data_y = data[border1:border2]\n        else:\n            Train_S = int(S * 0.8)\n            df_raw = df_raw[:, Train_S:]\n            num_train = int(len(df_raw) * 0.8)\n            num_test = len(df_raw) - num_train\n            border1s = [0, len(df_raw) - num_test - self.seq_len]\n            border2s = [num_train, len(df_raw)]\n            data = df_raw\n            if self.test_type == 1:\n                border1 = border1s[0]\n                border2 = border2s[0]\n            else:\n                border1 = border1s[1]\n                border2 = border2s[1]\n\n            self.data_x = data[border1:border2]\n            self.data_y = data[border1:border2]\n\n    def __getitem__(self, index):\n        feat_id = index // self.tot_len\n        s_begin = index % self.tot_len\n        s_end = s_begin + self.seq_len\n\n        if not self.nonautoregressive:\n            r_begin = s_begin + self.input_token_len\n            r_end = s_end + self.output_token_len\n\n            seq_x = self.data_x[s_begin:s_end, feat_id:feat_id+1]\n            seq_y = self.data_y[r_begin:r_end, feat_id:feat_id+1]\n            seq_y = torch.tensor(seq_y)\n            seq_y = seq_y.unfold(dimension=0, size=self.output_token_len,\n                                 step=self.input_token_len).permute(0, 2, 1)\n            seq_y = seq_y.reshape(seq_y.shape[0] * seq_y.shape[1], -1)\n        else:\n            r_begin = s_end\n            r_end = r_begin + self.output_token_len\n            seq_x = self.data_x[s_begin:s_end, feat_id:feat_id+1]\n            seq_y = self.data_y[r_begin:r_end, feat_id:feat_id+1]\n        seq_x_mark = torch.zeros((seq_x.shape[0], 1))\n        seq_y_mark = torch.zeros((seq_x.shape[0], 1))\n        return seq_x, seq_y, seq_x_mark, seq_y_mark\n\n    def __len__(self):\n        return (len(self.data_x) - self.seq_len - self.output_token_len + 1) * self.enc_in\n\n    def inverse_transform(self, data):\n        return self.scaler.inverse_transform(data)\n\n\"\"\"\nAll single-variate series in UTSD are divided into (input-output) windows with a uniform length based on S3.\n\"\"\"\nclass UTSDataset(Dataset):\n    def __init__(self, subset_name=r'UTSD-1G', flag='train', split=0.9,\n                 input_len=None, output_len=None, scale=True, stride=1):\n        self.input_len = input_len\n        self.output_len = output_len\n        self.seq_len = input_len + output_len\n        assert flag in ['train', 'val']\n        assert split >= 0 and split <=1.0\n        type_map = {'train': 0, 'val': 1, 'test': 2}\n        self.set_type = type_map[flag]\n        self.flag = flag\n        self.scale = scale\n        self.split = split\n        self.stride = stride\n\n        self.data_list = []\n        self.n_window_list = []\n\n        self.subset_name = subset_name\n        self.__read_data__()\n\n    def __read_data__(self):\n        dataset = datasets.load_dataset(\"thuml/UTSD\", self.subset_name, split='train')\n        # split='train' contains all the time series, which have not been divided into splits, \n        # you can split them by yourself, or use our default split as train:val = 9:1\n        print('Indexing dataset...')\n        for item in tqdm(dataset):\n            self.scaler = StandardScaler()\n            data = item['target']\n            data = np.array(data).reshape(-1, 1)\n            num_train = int(len(data) * self.split)\n            border1s = [0, num_train - self.seq_len]\n            border2s = [num_train, len(data)]\n\n            border1 = border1s[self.set_type]\n            border2 = border2s[self.set_type]\n\n            if self.scale:\n                train_data = data[border1s[0]:border2s[0]]\n                self.scaler.fit(train_data)\n                data = self.scaler.transform(data)\n\n            data = data[border1:border2]\n            n_window = (len(data) - self.seq_len) // self.stride + 1\n            if n_window < 1:\n                continue\n\n            self.data_list.append(data)\n            self.n_window_list.append(n_window if len(self.n_window_list) == 0 else self.n_window_list[-1] + n_window)\n\n\n    def __getitem__(self, index):\n        # you can wirte your own processing code here\n        dataset_index = 0\n        while index >= self.n_window_list[dataset_index]:\n            dataset_index += 1\n\n        index = index - self.n_window_list[dataset_index - 1] if dataset_index > 0 else index\n        n_timepoint = (len(self.data_list[dataset_index]) - self.seq_len) // self.stride + 1\n\n        s_begin = index % n_timepoint\n        s_begin = self.stride * s_begin\n        s_end = s_begin + self.seq_len\n        p_begin = s_end\n        p_end = p_begin + self.output_len\n        seq_x = self.data_list[dataset_index][s_begin:s_end, :]\n        seq_y = self.data_list[dataset_index][p_begin:p_end, :]\n\n        return seq_x, seq_y\n\n    def __len__(self):\n        return self.n_window_list[-1]\n\n# Download link: https://huggingface.co/datasets/thuml/UTSD\nclass UTSD(Dataset):\n    def __init__(self, root_path, flag='train', size=None, data_path='ETTh1.csv', scale=True, nonautoregressive=False, stride=1, split=0.9, test_flag='T'):\n        self.seq_len = size[0]\n        self.input_token_len = size[1]\n        self.output_token_len = size[2]\n        self.context_len = self.seq_len + self.output_token_len\n        self.flag = flag\n        assert flag in ['train', 'test', 'val']\n        type_map = {'train': 0, 'val': 1, 'test': 2}\n        self.set_type = type_map[flag]\n        self.scale = scale\n        self.nonautoregressive = nonautoregressive\n        self.split = split\n        self.stride = stride\n        self.data_list = []\n        self.n_window_list = []\n        self.root_path = root_path\n        self.__confirm_data__()\n\n    def __confirm_data__(self):\n        dataset = datasets.load_dataset(\"thuml/UTSD\", self.subset_name, split='train')\n        # split='train' contains all the time series, which have not been divided into splits, \n        # you can split them by yourself, or use our default split as train:val = 9:1\n        print('Indexing dataset...')\n        for item in tqdm(dataset):\n            self.scaler = StandardScaler()\n            data = item['target']\n            data = np.array(data).reshape(-1, 1)\n            num_train = int(len(data) * self.split)\n            num_test = int(len(data) * (1 - self.split) / 2)\n            num_vali = len(data) - num_train - num_test\n            if num_train < self.context_len:\n                continue\n            border1s = [0, num_train - self.seq_len, len(data) - num_test - self.seq_len]\n            border2s = [num_train, num_train + num_vali, len(data)]\n\n            border1 = border1s[self.set_type]\n            border2 = border2s[self.set_type]\n\n            if self.scale:\n                train_data = data[border1s[0]:border2s[0]]\n                self.scaler.fit(train_data)\n                data = self.scaler.transform(data)\n            else:\n                data = data\n\n            data = data[border1:border2]\n            n_timepoint = (\n                len(data) - self.context_len) // self.stride + 1\n            n_var = data.shape[1]\n            self.data_list.append(data)\n            n_window = n_timepoint * n_var\n            self.n_window_list.append(n_window if len(\n                self.n_window_list) == 0 else self.n_window_list[-1] + n_window)\nprint(\"Total number of windows in merged dataset: \",\n        self.n_window_list[-1])\n\n    def __getitem__(self, index):\n        assert index >= 0\n        # find the location of one dataset by the index\n        dataset_index = 0\n        while index >= self.n_window_list[dataset_index]:\n            dataset_index += 1\n\n        index = index - \\\n            self.n_window_list[dataset_index -\n                               1] if dataset_index > 0 else index\n        n_timepoint = (\n            len(self.data_list[dataset_index]) - self.context_len) // self.stride + 1\n\n        c_begin = index // n_timepoint  # select variable\n        s_begin = index % n_timepoint  # select start timestamp\n        s_begin = self.stride * s_begin\n        s_end = s_begin + self.seq_len\n        r_begin = s_begin + self.input_token_len\n        r_end = s_end + self.output_token_len\n\n        seq_x = self.data_list[dataset_index][s_begin:s_end,\n                                              c_begin:c_begin + 1]\n        seq_y = self.data_list[dataset_index][r_begin:r_end,\n                                              c_begin:c_begin + 1]\n        seq_x_mark = torch.zeros((seq_x.shape[0], 1))\n        seq_y_mark = torch.zeros((seq_x.shape[0], 1))\n        return seq_x, seq_y, seq_x_mark, seq_y_mark\n\n    def __len__(self):\n        return self.n_window_list[-1]\n\n\n# Download link: https://cloud.tsinghua.edu.cn/f/93868e3a9fb144fe9719/\nclass UTSD_Npy(Dataset):\n    def __init__(self, root_path, flag='train', size=None, data_path='ETTh1.csv', scale=True, nonautoregressive=False, stride=1, split=0.9, test_flag='T'):\n        self.seq_len = size[0]\n        self.input_token_len = size[1]\n        self.output_token_len = size[2]\n        self.context_len = self.seq_len + self.output_token_len\n        self.flag = flag\n        assert flag in ['train', 'test', 'val']\n        type_map = {'train': 0, 'val': 1, 'test': 2}\n        self.set_type = type_map[flag]\n        self.scale = scale\n        self.root_path = root_path\n        self.nonautoregressive = nonautoregressive\n        self.split = split\n        self.stride = stride\n        self.data_list = []\n        self.n_window_list = []\n        self.__confirm_data__()\n\n    def __confirm_data__(self):\n        for root, dirs, files in os.walk(self.root_path):\n            for file in files:\n                if file.endswith('.npy'):\n                    dataset_path = os.path.join(root, file)\n\n                    self.scaler = StandardScaler()\n                    data = np.load(dataset_path)\n\n                    num_train = int(len(data) * self.split)\n                    num_test = int(len(data) * (1 - self.split) / 2)\n                    num_vali = len(data) - num_train - num_test\n                    if num_train < self.context_len:\n                        continue\n                    border1s = [0, num_train - self.seq_len, len(data) - num_test - self.seq_len]\n                    border2s = [num_train, num_train + num_vali, len(data)]\n\n                    border1 = border1s[self.set_type]\n                    border2 = border2s[self.set_type]\n\n                    if self.scale:\n                        train_data = data[border1s[0]:border2s[0]]\n                        self.scaler.fit(train_data)\n                        data = self.scaler.transform(data)\n                    else:\n                        data = data\n\n                    data = data[border1:border2]\n                    n_timepoint = (\n                        len(data) - self.context_len) // self.stride + 1\n                    n_var = data.shape[1]\n                    self.data_list.append(data)\n\n                    n_window = n_timepoint * n_var\n                    self.n_window_list.append(n_window if len(\n                        self.n_window_list) == 0 else self.n_window_list[-1] + n_window)\n        print(\"Total number of windows in merged dataset: \",\n              self.n_window_list[-1])\n\n    def __getitem__(self, index):\n        assert index >= 0\n        # find the location of one dataset by the index\n        dataset_index = 0\n        while index >= self.n_window_list[dataset_index]:\n            dataset_index += 1\n\n        index = index - \\\n            self.n_window_list[dataset_index -\n                               1] if dataset_index > 0 else index\n        n_timepoint = (\n            len(self.data_list[dataset_index]) - self.context_len) // self.stride + 1\n\n        c_begin = index // n_timepoint  # select variable\n        s_begin = index % n_timepoint  # select start timestamp\n        s_begin = self.stride * s_begin\n        s_end = s_begin + self.seq_len\n        r_begin = s_begin + self.input_token_len\n        r_end = s_end + self.output_token_len\n\n        seq_x = self.data_list[dataset_index][s_begin:s_end,\n                                              c_begin:c_begin + 1]\n        seq_y = self.data_list[dataset_index][r_begin:r_end,\n                                              c_begin:c_begin + 1]\n        seq_x_mark = torch.zeros((seq_x.shape[0], 1))\n        seq_y_mark = torch.zeros((seq_x.shape[0], 1))\n        return seq_x, seq_y, seq_x_mark, seq_y_mark\n\n    def __len__(self):\n        return self.n_window_list[-1]\n"
        }
    ]
}