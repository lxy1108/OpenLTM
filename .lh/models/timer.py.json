{
    "sourceFile": "models/timer.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 78,
            "patches": [
                {
                    "date": 1730121250887,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1730121258883,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -49,9 +49,9 @@\n         # [B * C, N, P]\n         x_enc = x_enc.reshape(B * C, N, -1)\n         # [B * C, N, D]\n         enc_out = self.embedding(x_enc) + self.position_embedding(x_enc)\n-        enc_out = self.dropout(enc_out).reshape(B , C, N, -1)\n+        enc_out = self.dropout(enc_out).reshape(B, C, N, -1)\n         enc_out, attns = self.encoder(enc_out)\n         # [B * C, N, P]\n         dec_out = self.head(enc_out)\n         # [B, C, L]\n"
                },
                {
                    "date": 1730121628547,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,7 +1,7 @@\n import torch\n from torch import nn\n-from layers.Transformer_EncDec import Encoder, EncoderLayer\n+from layers.Transformer_EncDec import Encoder, EncoderLayer, AlterEncoder\n from layers.SelfAttention_Family import FullAttention, AttentionLayer\n from layers.Embed import PositionalEmbedding\n \n \n@@ -14,9 +14,9 @@\n         self.input_token_len = configs.input_token_len\n         self.embedding = nn.Linear(self.input_token_len, configs.d_model, bias=False)\n         self.position_embedding = PositionalEmbedding(configs.d_model)\n         self.dropout = nn.Dropout(configs.dropout)\n-        self.encoder = Encoder(\n+        self.encoder = AlterEncoder(\n             [\n                 EncoderLayer(\n                     AttentionLayer(\n                         FullAttention(True, attention_dropout=configs.dropout, output_attention=False), configs.d_model, configs.n_heads),\n"
                },
                {
                    "date": 1730384527730,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -50,12 +50,14 @@\n         x_enc = x_enc.reshape(B * C, N, -1)\n         # [B * C, N, D]\n         enc_out = self.embedding(x_enc) + self.position_embedding(x_enc)\n         enc_out = self.dropout(enc_out).reshape(B, C, N, -1)\n+        # enc_out = enc_out.reshape(B, 2, C//2, N, -1).reshape(2*B, C//2, N, -1)\n         enc_out, attns = self.encoder(enc_out)\n         # [B * C, N, P]\n         dec_out = self.head(enc_out)\n         # [B, C, L]\n+        # dec_out = dec_out.reshape(B, 2, C//2, N, -1)\n         dec_out = dec_out.reshape(B, C, -1)\n         # [B, L, C]\n         dec_out = dec_out.permute(0, 2, 1)\n         if self.use_norm:\n"
                },
                {
                    "date": 1730384555080,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -50,14 +50,12 @@\n         x_enc = x_enc.reshape(B * C, N, -1)\n         # [B * C, N, D]\n         enc_out = self.embedding(x_enc) + self.position_embedding(x_enc)\n         enc_out = self.dropout(enc_out).reshape(B, C, N, -1)\n-        # enc_out = enc_out.reshape(B, 2, C//2, N, -1).reshape(2*B, C//2, N, -1)\n         enc_out, attns = self.encoder(enc_out)\n         # [B * C, N, P]\n         dec_out = self.head(enc_out)\n         # [B, C, L]\n-        # dec_out = dec_out.reshape(B, 2, C//2, N, -1)\n         dec_out = dec_out.reshape(B, C, -1)\n         # [B, L, C]\n         dec_out = dec_out.permute(0, 2, 1)\n         if self.use_norm:\n"
                },
                {
                    "date": 1730422350247,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -50,11 +50,13 @@\n         x_enc = x_enc.reshape(B * C, N, -1)\n         # [B * C, N, D]\n         enc_out = self.embedding(x_enc) + self.position_embedding(x_enc)\n         enc_out = self.dropout(enc_out).reshape(B, C, N, -1)\n+        enc_out = enc_out.reshape(B, 2, C//2, N, -1).reshape(2*B, C//2, N, -1)\n         enc_out, attns = self.encoder(enc_out)\n         # [B * C, N, P]\n         dec_out = self.head(enc_out)\n+        dec_out = dec_out.reshape(B, 2, C//2, N, -1)\n         # [B, C, L]\n         dec_out = dec_out.reshape(B, C, -1)\n         # [B, L, C]\n         dec_out = dec_out.permute(0, 2, 1)\n"
                },
                {
                    "date": 1730422976908,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -50,13 +50,13 @@\n         x_enc = x_enc.reshape(B * C, N, -1)\n         # [B * C, N, D]\n         enc_out = self.embedding(x_enc) + self.position_embedding(x_enc)\n         enc_out = self.dropout(enc_out).reshape(B, C, N, -1)\n-        enc_out = enc_out.reshape(B, 2, C//2, N, -1).reshape(2*B, C//2, N, -1)\n+        # enc_out = enc_out.reshape(B, 2, C//2, N, -1).reshape(2*B, C//2, N, -1)\n         enc_out, attns = self.encoder(enc_out)\n         # [B * C, N, P]\n         dec_out = self.head(enc_out)\n-        dec_out = dec_out.reshape(B, 2, C//2, N, -1)\n+        # dec_out = dec_out.reshape(B, 2, C//2, N, -1)\n         # [B, C, L]\n         dec_out = dec_out.reshape(B, C, -1)\n         # [B, L, C]\n         dec_out = dec_out.permute(0, 2, 1)\n"
                },
                {
                    "date": 1730554040938,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -14,9 +14,9 @@\n         self.input_token_len = configs.input_token_len\n         self.embedding = nn.Linear(self.input_token_len, configs.d_model, bias=False)\n         self.position_embedding = PositionalEmbedding(configs.d_model)\n         self.dropout = nn.Dropout(configs.dropout)\n-        self.encoder = AlterEncoder(\n+        self.decoder = AlterEncoder(\n             [\n                 EncoderLayer(\n                     AttentionLayer(\n                         FullAttention(True, attention_dropout=configs.dropout, output_attention=False), configs.d_model, configs.n_heads),\n"
                },
                {
                    "date": 1730554156425,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -27,9 +27,9 @@\n                 ) for l in range(configs.e_layers)\n             ],\n             norm_layer=torch.nn.LayerNorm(configs.d_model)\n         )\n-        self.head = nn.Linear(configs.d_model, configs.output_token_len)\n+        self.proj = nn.Linear(configs.d_model, configs.output_token_len)\n         self.use_norm = configs.use_norm\n \n     def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n         if self.use_norm:\n"
                },
                {
                    "date": 1730555096730,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -51,9 +51,9 @@\n         # [B * C, N, D]\n         enc_out = self.embedding(x_enc) + self.position_embedding(x_enc)\n         enc_out = self.dropout(enc_out).reshape(B, C, N, -1)\n         # enc_out = enc_out.reshape(B, 2, C//2, N, -1).reshape(2*B, C//2, N, -1)\n-        enc_out, attns = self.encoder(enc_out)\n+        enc_out, attns = self.decoder(enc_out)\n         # [B * C, N, P]\n         dec_out = self.head(enc_out)\n         # dec_out = dec_out.reshape(B, 2, C//2, N, -1)\n         # [B, C, L]\n"
                },
                {
                    "date": 1730555127167,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -53,9 +53,9 @@\n         enc_out = self.dropout(enc_out).reshape(B, C, N, -1)\n         # enc_out = enc_out.reshape(B, 2, C//2, N, -1).reshape(2*B, C//2, N, -1)\n         enc_out, attns = self.decoder(enc_out)\n         # [B * C, N, P]\n-        dec_out = self.head(enc_out)\n+        dec_out = self.proj(enc_out)\n         # dec_out = dec_out.reshape(B, 2, C//2, N, -1)\n         # [B, C, L]\n         dec_out = dec_out.reshape(B, C, -1)\n         # [B, L, C]\n"
                },
                {
                    "date": 1730708716014,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -14,9 +14,9 @@\n         self.input_token_len = configs.input_token_len\n         self.embedding = nn.Linear(self.input_token_len, configs.d_model, bias=False)\n         self.position_embedding = PositionalEmbedding(configs.d_model)\n         self.dropout = nn.Dropout(configs.dropout)\n-        self.decoder = AlterEncoder(\n+        self.encoder = AlterEncoder(\n             [\n                 EncoderLayer(\n                     AttentionLayer(\n                         FullAttention(True, attention_dropout=configs.dropout, output_attention=False), configs.d_model, configs.n_heads),\n@@ -27,9 +27,9 @@\n                 ) for l in range(configs.e_layers)\n             ],\n             norm_layer=torch.nn.LayerNorm(configs.d_model)\n         )\n-        self.proj = nn.Linear(configs.d_model, configs.output_token_len)\n+        self.head = nn.Linear(configs.d_model, configs.output_token_len)\n         self.use_norm = configs.use_norm\n \n     def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n         if self.use_norm:\n@@ -51,11 +51,11 @@\n         # [B * C, N, D]\n         enc_out = self.embedding(x_enc) + self.position_embedding(x_enc)\n         enc_out = self.dropout(enc_out).reshape(B, C, N, -1)\n         # enc_out = enc_out.reshape(B, 2, C//2, N, -1).reshape(2*B, C//2, N, -1)\n-        enc_out, attns = self.decoder(enc_out)\n+        enc_out, attns = self.encoder(enc_out)\n         # [B * C, N, P]\n-        dec_out = self.proj(enc_out)\n+        dec_out = self.head(enc_out)\n         # dec_out = dec_out.reshape(B, 2, C//2, N, -1)\n         # [B, C, L]\n         dec_out = dec_out.reshape(B, C, -1)\n         # [B, L, C]\n"
                },
                {
                    "date": 1730721568228,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -14,9 +14,9 @@\n         self.input_token_len = configs.input_token_len\n         self.embedding = nn.Linear(self.input_token_len, configs.d_model, bias=False)\n         self.position_embedding = PositionalEmbedding(configs.d_model)\n         self.dropout = nn.Dropout(configs.dropout)\n-        self.encoder = AlterEncoder(\n+        self.decoder = AlterEncoder(\n             [\n                 EncoderLayer(\n                     AttentionLayer(\n                         FullAttention(True, attention_dropout=configs.dropout, output_attention=False), configs.d_model, configs.n_heads),\n@@ -27,9 +27,9 @@\n                 ) for l in range(configs.e_layers)\n             ],\n             norm_layer=torch.nn.LayerNorm(configs.d_model)\n         )\n-        self.head = nn.Linear(configs.d_model, configs.output_token_len)\n+        self.proj = nn.Linear(configs.d_model, configs.output_token_len)\n         self.use_norm = configs.use_norm\n \n     def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n         if self.use_norm:\n@@ -51,11 +51,11 @@\n         # [B * C, N, D]\n         enc_out = self.embedding(x_enc) + self.position_embedding(x_enc)\n         enc_out = self.dropout(enc_out).reshape(B, C, N, -1)\n         # enc_out = enc_out.reshape(B, 2, C//2, N, -1).reshape(2*B, C//2, N, -1)\n-        enc_out, attns = self.encoder(enc_out)\n+        enc_out, attns = self.decoder(enc_out)\n         # [B * C, N, P]\n-        dec_out = self.head(enc_out)\n+        dec_out = self.proj(enc_out)\n         # dec_out = dec_out.reshape(B, 2, C//2, N, -1)\n         # [B, C, L]\n         dec_out = dec_out.reshape(B, C, -1)\n         # [B, L, C]\n"
                },
                {
                    "date": 1730817662742,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -23,9 +23,19 @@\n                     configs.d_model,\n                     configs.d_ff,\n                     dropout=configs.dropout,\n                     activation=configs.activation\n-                ) for l in range(configs.e_layers)\n+                ) \n+                if l%2==1 else\n+                EncoderLayer(\n+                    AttentionLayer(\n+                        FullAttention(True, attention_dropout=configs.dropout, output_attention=False), configs.d_model, configs.n_heads),\n+                    configs.d_model,\n+                    configs.d_ff,\n+                    dropout=configs.dropout,\n+                    activation=configs.activation\n+                )\n+                for l in range(configs.e_layers)\n             ],\n             norm_layer=torch.nn.LayerNorm(configs.d_model)\n         )\n         self.proj = nn.Linear(configs.d_model, configs.output_token_len)\n"
                },
                {
                    "date": 1730817677836,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,8 +1,8 @@\n import torch\n from torch import nn\n from layers.Transformer_EncDec import Encoder, EncoderLayer, AlterEncoder\n-from layers.SelfAttention_Family import FullAttention, AttentionLayer\n+from layers.SelfAttention_Family import FullAttention, AttentionLayer, TimeAttention\n from layers.Embed import PositionalEmbedding\n \n \n class Model(nn.Module):\n@@ -27,9 +27,9 @@\n                 ) \n                 if l%2==1 else\n                 EncoderLayer(\n                     AttentionLayer(\n-                        FullAttention(True, attention_dropout=configs.dropout, output_attention=False), configs.d_model, configs.n_heads),\n+                        TimeAttention(True, attention_dropout=configs.dropout, output_attention=False), configs.d_model, configs.n_heads),\n                     configs.d_model,\n                     configs.d_ff,\n                     dropout=configs.dropout,\n                     activation=configs.activation\n"
                },
                {
                    "date": 1730818014734,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -25,9 +25,9 @@\n                     dropout=configs.dropout,\n                     activation=configs.activation\n                 ) \n                 if l%2==1 else\n-                EncoderLayer(\n+                TimerLayer(\n                     AttentionLayer(\n                         TimeAttention(True, attention_dropout=configs.dropout, output_attention=False), configs.d_model, configs.n_heads),\n                     configs.d_model,\n                     configs.d_ff,\n"
                },
                {
                    "date": 1730818025788,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,7 +1,7 @@\n import torch\n from torch import nn\n-from layers.Transformer_EncDec import Encoder, EncoderLayer, AlterEncoder\n+from layers.Transformer_EncDec import Encoder, EncoderLayer, AlterEncoder, TimerLayer\n from layers.SelfAttention_Family import FullAttention, AttentionLayer, TimeAttention\n from layers.Embed import PositionalEmbedding\n \n \n"
                },
                {
                    "date": 1730818307815,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -24,9 +24,9 @@\n                     configs.d_ff,\n                     dropout=configs.dropout,\n                     activation=configs.activation\n                 ) \n-                if l%2==1 else\n+                if l%2==0 else\n                 TimerLayer(\n                     AttentionLayer(\n                         TimeAttention(True, attention_dropout=configs.dropout, output_attention=False), configs.d_model, configs.n_heads),\n                     configs.d_model,\n"
                },
                {
                    "date": 1730818347284,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -24,9 +24,9 @@\n                     configs.d_ff,\n                     dropout=configs.dropout,\n                     activation=configs.activation\n                 ) \n-                if l%2==0 else\n+                if l%2==1 else\n                 TimerLayer(\n                     AttentionLayer(\n                         TimeAttention(True, attention_dropout=configs.dropout, output_attention=False), configs.d_model, configs.n_heads),\n                     configs.d_model,\n"
                },
                {
                    "date": 1730853529580,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -24,9 +24,9 @@\n                     configs.d_ff,\n                     dropout=configs.dropout,\n                     activation=configs.activation\n                 ) \n-                if l%2==1 else\n+                if l<3 else\n                 TimerLayer(\n                     AttentionLayer(\n                         TimeAttention(True, attention_dropout=configs.dropout, output_attention=False), configs.d_model, configs.n_heads),\n                     configs.d_model,\n"
                },
                {
                    "date": 1730879491841,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -24,17 +24,17 @@\n                     configs.d_ff,\n                     dropout=configs.dropout,\n                     activation=configs.activation\n                 ) \n-                if l<3 else\n-                TimerLayer(\n-                    AttentionLayer(\n-                        TimeAttention(True, attention_dropout=configs.dropout, output_attention=False), configs.d_model, configs.n_heads),\n-                    configs.d_model,\n-                    configs.d_ff,\n-                    dropout=configs.dropout,\n-                    activation=configs.activation\n-                )\n+                # if l<3 else\n+                # TimerLayer(\n+                #     AttentionLayer(\n+                #         TimeAttention(True, attention_dropout=configs.dropout, output_attention=False), configs.d_model, configs.n_heads),\n+                #     configs.d_model,\n+                #     configs.d_ff,\n+                #     dropout=configs.dropout,\n+                #     activation=configs.activation\n+                # )\n                 for l in range(configs.e_layers)\n             ],\n             norm_layer=torch.nn.LayerNorm(configs.d_model)\n         )\n"
                },
                {
                    "date": 1730883057828,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -12,9 +12,9 @@\n     def __init__(self, configs):\n         super().__init__()\n         self.input_token_len = configs.input_token_len\n         self.embedding = nn.Linear(self.input_token_len, configs.d_model, bias=False)\n-        self.position_embedding = PositionalEmbedding(configs.d_model)\n+        # self.position_embedding = PositionalEmbedding(configs.d_model)\n         self.dropout = nn.Dropout(configs.dropout)\n         self.decoder = AlterEncoder(\n             [\n                 EncoderLayer(\n@@ -58,9 +58,9 @@\n         N = x_enc.shape[2]\n         # [B * C, N, P]\n         x_enc = x_enc.reshape(B * C, N, -1)\n         # [B * C, N, D]\n-        enc_out = self.embedding(x_enc) + self.position_embedding(x_enc)\n+        enc_out = self.embedding(x_enc)# + self.position_embedding(x_enc)\n         enc_out = self.dropout(enc_out).reshape(B, C, N, -1)\n         # enc_out = enc_out.reshape(B, 2, C//2, N, -1).reshape(2*B, C//2, N, -1)\n         enc_out, attns = self.decoder(enc_out)\n         # [B * C, N, P]\n"
                },
                {
                    "date": 1730883068025,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -56,9 +56,9 @@\n         x_enc = x_enc.unfold(\n             dimension=-1, size=self.input_token_len, step=self.input_token_len)\n         N = x_enc.shape[2]\n         # [B * C, N, P]\n-        x_enc = x_enc.reshape(B * C, N, -1)\n+        # x_enc = x_enc.reshape(B * C, N, -1)\n         # [B * C, N, D]\n         enc_out = self.embedding(x_enc)# + self.position_embedding(x_enc)\n         enc_out = self.dropout(enc_out).reshape(B, C, N, -1)\n         # enc_out = enc_out.reshape(B, 2, C//2, N, -1).reshape(2*B, C//2, N, -1)\n"
                },
                {
                    "date": 1730883270449,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -18,9 +18,9 @@\n         self.decoder = AlterEncoder(\n             [\n                 EncoderLayer(\n                     AttentionLayer(\n-                        FullAttention(True, attention_dropout=configs.dropout, output_attention=False), configs.d_model, configs.n_heads),\n+                        FullAttention(True, attention_dropout=configs.dropout, d_model=configs.d_model, num_heads=configs.n_heads, output_attention=False), configs.d_model, configs.n_heads),\n                     configs.d_model,\n                     configs.d_ff,\n                     dropout=configs.dropout,\n                     activation=configs.activation\n"
                },
                {
                    "date": 1731073107588,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -37,9 +37,9 @@\n                 for l in range(configs.e_layers)\n             ],\n             norm_layer=torch.nn.LayerNorm(configs.d_model)\n         )\n-        self.proj = nn.Linear(configs.d_model, configs.output_token_len)\n+        self.projs = nn.ModuleList([nn.Linear(configs.d_model, configs.output_token_len) for i in range(4)])\n         self.use_norm = configs.use_norm\n \n     def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n         if self.use_norm:\n"
                },
                {
                    "date": 1731073156439,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -63,12 +63,12 @@\n         enc_out = self.dropout(enc_out).reshape(B, C, N, -1)\n         # enc_out = enc_out.reshape(B, 2, C//2, N, -1).reshape(2*B, C//2, N, -1)\n         enc_out, attns = self.decoder(enc_out)\n         # [B * C, N, P]\n-        dec_out = self.proj(enc_out)\n+        enc_out = self.proj(enc_out)\n         # dec_out = dec_out.reshape(B, 2, C//2, N, -1)\n         # [B, C, L]\n-        dec_out = dec_out.reshape(B, C, -1)\n+        dec_out = enc_out.reshape(B, C, -1)\n         # [B, L, C]\n         dec_out = dec_out.permute(0, 2, 1)\n         if self.use_norm:\n             dec_out = dec_out * stdev + means\n"
                },
                {
                    "date": 1731073424475,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -63,9 +63,9 @@\n         enc_out = self.dropout(enc_out).reshape(B, C, N, -1)\n         # enc_out = enc_out.reshape(B, 2, C//2, N, -1).reshape(2*B, C//2, N, -1)\n         enc_out, attns = self.decoder(enc_out)\n         # [B * C, N, P]\n-        enc_out = self.proj(enc_out)\n+        dec_out = self.proj(enc_out)\n         # dec_out = dec_out.reshape(B, 2, C//2, N, -1)\n         # [B, C, L]\n         dec_out = enc_out.reshape(B, C, -1)\n         # [B, L, C]\n"
                },
                {
                    "date": 1731073509085,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -63,9 +63,12 @@\n         enc_out = self.dropout(enc_out).reshape(B, C, N, -1)\n         # enc_out = enc_out.reshape(B, 2, C//2, N, -1).reshape(2*B, C//2, N, -1)\n         enc_out, attns = self.decoder(enc_out)\n         # [B * C, N, P]\n-        dec_out = self.proj(enc_out)\n+        dec_out = enc_out.detach()\n+        dec_out = dec_out.requires_grad = True\n+        for i in range(4):\n+            pred = self.projs[i](enc_out).reshape(B, C, -1)\n         # dec_out = dec_out.reshape(B, 2, C//2, N, -1)\n         # [B, C, L]\n         dec_out = enc_out.reshape(B, C, -1)\n         # [B, L, C]\n"
                },
                {
                    "date": 1731073575107,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -39,8 +39,9 @@\n             norm_layer=torch.nn.LayerNorm(configs.d_model)\n         )\n         self.projs = nn.ModuleList([nn.Linear(configs.d_model, configs.output_token_len) for i in range(4)])\n         self.use_norm = configs.use_norm\n+        self.criterion = nn.MSELoss()\n \n     def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n         if self.use_norm:\n             means = x_enc.mean(1, keepdim=True).detach()\n@@ -66,9 +67,12 @@\n         # [B * C, N, P]\n         dec_out = enc_out.detach()\n         dec_out = dec_out.requires_grad = True\n         for i in range(4):\n-            pred = self.projs[i](enc_out).reshape(B, C, -1)\n+            pred = self.projs[i](enc_out).reshape(B, C, -1).permute(0, 2, 1)\n+            if self.use_norm:\n+                pred = pred * stdev + means\n+\n         # dec_out = dec_out.reshape(B, 2, C//2, N, -1)\n         # [B, C, L]\n         dec_out = enc_out.reshape(B, C, -1)\n         # [B, L, C]\n"
                },
                {
                    "date": 1731073690955,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -70,16 +70,17 @@\n         for i in range(4):\n             pred = self.projs[i](enc_out).reshape(B, C, -1).permute(0, 2, 1)\n             if self.use_norm:\n                 pred = pred * stdev + means\n-\n+            self.criterion(pred, x_dec[]).backward()\n+        enc_out.backward(gradient=dec_out.grad)\n         # dec_out = dec_out.reshape(B, 2, C//2, N, -1)\n         # [B, C, L]\n-        dec_out = enc_out.reshape(B, C, -1)\n-        # [B, L, C]\n-        dec_out = dec_out.permute(0, 2, 1)\n-        if self.use_norm:\n-            dec_out = dec_out * stdev + means\n+        # dec_out = enc_out.reshape(B, C, -1)\n+        # # [B, L, C]\n+        # dec_out = dec_out.permute(0, 2, 1)\n+        # if self.use_norm:\n+        #     dec_out = dec_out * stdev + means\n         return dec_out\n \n     def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n         return self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n"
                },
                {
                    "date": 1731073709923,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -49,9 +49,9 @@\n             stdev = torch.sqrt(\n                 torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)\n             x_enc /= stdev\n         # [B, L, C]\n-        B, _, C = x_enc.shape\n+        B, L, C = x_enc.shape\n         # [B, C, L]\n         x_enc = x_enc.permute(0, 2, 1)\n         # [B, C, N, P]\n         x_enc = x_enc.unfold(\n"
                },
                {
                    "date": 1731073744825,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -70,9 +70,9 @@\n         for i in range(4):\n             pred = self.projs[i](enc_out).reshape(B, C, -1).permute(0, 2, 1)\n             if self.use_norm:\n                 pred = pred * stdev + means\n-            self.criterion(pred, x_dec[]).backward()\n+            self.criterion(pred, x_dec[i*self.input_token_len:i*self.input_token_len+L]).backward()\n         enc_out.backward(gradient=dec_out.grad)\n         # dec_out = dec_out.reshape(B, 2, C//2, N, -1)\n         # [B, C, L]\n         # dec_out = enc_out.reshape(B, C, -1)\n"
                },
                {
                    "date": 1731073807062,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -66,21 +66,24 @@\n         enc_out, attns = self.decoder(enc_out)\n         # [B * C, N, P]\n         dec_out = enc_out.detach()\n         dec_out = dec_out.requires_grad = True\n+        loss = 0\n         for i in range(4):\n             pred = self.projs[i](enc_out).reshape(B, C, -1).permute(0, 2, 1)\n             if self.use_norm:\n                 pred = pred * stdev + means\n-            self.criterion(pred, x_dec[i*self.input_token_len:i*self.input_token_len+L]).backward()\n+            loss = self.criterion(pred, x_dec[i*self.input_token_len:i*self.input_token_len+L])\n+            loss.backward()\n         enc_out.backward(gradient=dec_out.grad)\n         # dec_out = dec_out.reshape(B, 2, C//2, N, -1)\n         # [B, C, L]\n         # dec_out = enc_out.reshape(B, C, -1)\n         # # [B, L, C]\n         # dec_out = dec_out.permute(0, 2, 1)\n         # if self.use_norm:\n         #     dec_out = dec_out * stdev + means\n-        return dec_out\n+        return\n \n     def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n+        \n         return self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n"
                },
                {
                    "date": 1731073827312,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -71,19 +71,20 @@\n         for i in range(4):\n             pred = self.projs[i](enc_out).reshape(B, C, -1).permute(0, 2, 1)\n             if self.use_norm:\n                 pred = pred * stdev + means\n-            loss = self.criterion(pred, x_dec[i*self.input_token_len:i*self.input_token_len+L])\n-            loss.backward()\n+            cur_loss = self.criterion(pred, x_dec[i*self.input_token_len:i*self.input_token_len+L])\n+            cur_loss.backward()\n+            loss += cur_loss.item()\n         enc_out.backward(gradient=dec_out.grad)\n         # dec_out = dec_out.reshape(B, 2, C//2, N, -1)\n         # [B, C, L]\n         # dec_out = enc_out.reshape(B, C, -1)\n         # # [B, L, C]\n         # dec_out = dec_out.permute(0, 2, 1)\n         # if self.use_norm:\n         #     dec_out = dec_out * stdev + means\n-        return\n+        return loss/4\n \n     def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n         \n         return self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n"
                },
                {
                    "date": 1731073846872,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -85,6 +85,8 @@\n         #     dec_out = dec_out * stdev + means\n         return loss/4\n \n     def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n-        \n-        return self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n+        if self.training:\n+            return self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n+        else:\n+            return self.inference(x_enc)\n"
                },
                {
                    "date": 1731073856766,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -83,8 +83,51 @@\n         # dec_out = dec_out.permute(0, 2, 1)\n         # if self.use_norm:\n         #     dec_out = dec_out * stdev + means\n         return loss/4\n+    \n+    def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n+        if self.use_norm:\n+            means = x_enc.mean(1, keepdim=True).detach()\n+            x_enc = x_enc - means\n+            stdev = torch.sqrt(\n+                torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)\n+            x_enc /= stdev\n+        # [B, L, C]\n+        B, L, C = x_enc.shape\n+        # [B, C, L]\n+        x_enc = x_enc.permute(0, 2, 1)\n+        # [B, C, N, P]\n+        x_enc = x_enc.unfold(\n+            dimension=-1, size=self.input_token_len, step=self.input_token_len)\n+        N = x_enc.shape[2]\n+        # [B * C, N, P]\n+        # x_enc = x_enc.reshape(B * C, N, -1)\n+        # [B * C, N, D]\n+        enc_out = self.embedding(x_enc)# + self.position_embedding(x_enc)\n+        enc_out = self.dropout(enc_out).reshape(B, C, N, -1)\n+        # enc_out = enc_out.reshape(B, 2, C//2, N, -1).reshape(2*B, C//2, N, -1)\n+        enc_out, attns = self.decoder(enc_out)\n+        # [B * C, N, P]\n+        dec_out = enc_out.detach()\n+        dec_out = dec_out.requires_grad = True\n+        loss = 0\n+        for i in range(4):\n+            pred = self.projs[i](enc_out).reshape(B, C, -1).permute(0, 2, 1)\n+            if self.use_norm:\n+                pred = pred * stdev + means\n+            cur_loss = self.criterion(pred, x_dec[i*self.input_token_len:i*self.input_token_len+L])\n+            cur_loss.backward()\n+            loss += cur_loss.item()\n+        enc_out.backward(gradient=dec_out.grad)\n+        # dec_out = dec_out.reshape(B, 2, C//2, N, -1)\n+        # [B, C, L]\n+        # dec_out = enc_out.reshape(B, C, -1)\n+        # # [B, L, C]\n+        # dec_out = dec_out.permute(0, 2, 1)\n+        # if self.use_norm:\n+        #     dec_out = dec_out * stdev + means\n+        return loss/4\n \n     def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n         if self.training:\n             return self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n"
                },
                {
                    "date": 1731073875154,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -120,14 +120,14 @@\n             loss += cur_loss.item()\n         enc_out.backward(gradient=dec_out.grad)\n         # dec_out = dec_out.reshape(B, 2, C//2, N, -1)\n         # [B, C, L]\n-        # dec_out = enc_out.reshape(B, C, -1)\n-        # # [B, L, C]\n-        # dec_out = dec_out.permute(0, 2, 1)\n-        # if self.use_norm:\n-        #     dec_out = dec_out * stdev + means\n-        return loss/4\n+        dec_out = enc_out.reshape(B, C, -1)\n+        # [B, L, C]\n+        dec_out = dec_out.permute(0, 2, 1)\n+        if self.use_norm:\n+            dec_out = dec_out * stdev + means\n+        return dec_out\n \n     def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n         if self.training:\n             return self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n"
                },
                {
                    "date": 1731073931869,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -107,20 +107,9 @@\n         enc_out = self.dropout(enc_out).reshape(B, C, N, -1)\n         # enc_out = enc_out.reshape(B, 2, C//2, N, -1).reshape(2*B, C//2, N, -1)\n         enc_out, attns = self.decoder(enc_out)\n         # [B * C, N, P]\n-        dec_out = enc_out.detach()\n-        dec_out = dec_out.requires_grad = True\n-        loss = 0\n-        for i in range(4):\n-            pred = self.projs[i](enc_out).reshape(B, C, -1).permute(0, 2, 1)\n-            if self.use_norm:\n-                pred = pred * stdev + means\n-            cur_loss = self.criterion(pred, x_dec[i*self.input_token_len:i*self.input_token_len+L])\n-            cur_loss.backward()\n-            loss += cur_loss.item()\n-        enc_out.backward(gradient=dec_out.grad)\n-        # dec_out = dec_out.reshape(B, 2, C//2, N, -1)\n+        pred = self.projs[i](enc_out).reshape(B, C, -1).permute(0, 2, 1)\n         # [B, C, L]\n         dec_out = enc_out.reshape(B, C, -1)\n         # [B, L, C]\n         dec_out = dec_out.permute(0, 2, 1)\n"
                },
                {
                    "date": 1731073989769,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -84,9 +84,9 @@\n         # if self.use_norm:\n         #     dec_out = dec_out * stdev + means\n         return loss/4\n     \n-    def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n+    def inference(self, x_enc, steps):\n         if self.use_norm:\n             means = x_enc.mean(1, keepdim=True).detach()\n             x_enc = x_enc - means\n             stdev = torch.sqrt(\n"
                },
                {
                    "date": 1731074025750,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -107,9 +107,10 @@\n         enc_out = self.dropout(enc_out).reshape(B, C, N, -1)\n         # enc_out = enc_out.reshape(B, 2, C//2, N, -1).reshape(2*B, C//2, N, -1)\n         enc_out, attns = self.decoder(enc_out)\n         # [B * C, N, P]\n-        pred = self.projs[i](enc_out).reshape(B, C, -1).permute(0, 2, 1)\n+        \n+        pred = [self.projs[i](enc_out).reshape(B, C, -1).permute(0, 2, 1) for i in range(steps)]\n         # [B, C, L]\n         dec_out = enc_out.reshape(B, C, -1)\n         # [B, L, C]\n         dec_out = dec_out.permute(0, 2, 1)\n"
                },
                {
                    "date": 1731074047221,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -109,14 +109,11 @@\n         enc_out, attns = self.decoder(enc_out)\n         # [B * C, N, P]\n         \n         pred = [self.projs[i](enc_out).reshape(B, C, -1).permute(0, 2, 1) for i in range(steps)]\n-        # [B, C, L]\n-        dec_out = enc_out.reshape(B, C, -1)\n-        # [B, L, C]\n-        dec_out = dec_out.permute(0, 2, 1)\n+        \n         if self.use_norm:\n-            dec_out = dec_out * stdev + means\n+            pred = pred * stdev + means\n         return dec_out\n \n     def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n         if self.training:\n"
                },
                {
                    "date": 1731074173331,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -108,9 +108,9 @@\n         # enc_out = enc_out.reshape(B, 2, C//2, N, -1).reshape(2*B, C//2, N, -1)\n         enc_out, attns = self.decoder(enc_out)\n         # [B * C, N, P]\n         \n-        pred = [self.projs[i](enc_out).reshape(B, C, -1).permute(0, 2, 1) for i in range(steps)]\n+        pred = torch.cat([self.projs[i](enc_out[:,:,-1,:]) for i in range(steps)], dim=-1)\n         \n         if self.use_norm:\n             pred = pred * stdev + means\n         return dec_out\n"
                },
                {
                    "date": 1731074181158,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -108,9 +108,9 @@\n         # enc_out = enc_out.reshape(B, 2, C//2, N, -1).reshape(2*B, C//2, N, -1)\n         enc_out, attns = self.decoder(enc_out)\n         # [B * C, N, P]\n         \n-        pred = torch.cat([self.projs[i](enc_out[:,:,-1,:]) for i in range(steps)], dim=-1)\n+        pred = torch.cat([self.projs[i](enc_out[:,:,-1,:]) for i in range(steps)], dim=-1).permute(0,2,1)\n         \n         if self.use_norm:\n             pred = pred * stdev + means\n         return dec_out\n"
                },
                {
                    "date": 1731074246334,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -112,9 +112,9 @@\n         pred = torch.cat([self.projs[i](enc_out[:,:,-1,:]) for i in range(steps)], dim=-1).permute(0,2,1)\n         \n         if self.use_norm:\n             pred = pred * stdev + means\n-        return dec_out\n+        return pred\n \n     def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n         if self.training:\n             return self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n"
                },
                {
                    "date": 1731074275199,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -118,5 +118,5 @@\n     def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n         if self.training:\n             return self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n         else:\n-            return self.inference(x_enc)\n+            return self.inference(x_enc, steps)\n"
                },
                {
                    "date": 1731074280974,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -114,9 +114,9 @@\n         if self.use_norm:\n             pred = pred * stdev + means\n         return pred\n \n-    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n+    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None, steps=1):\n         if self.training:\n             return self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n         else:\n             return self.inference(x_enc, steps)\n"
                },
                {
                    "date": 1731075586183,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -65,9 +65,9 @@\n         # enc_out = enc_out.reshape(B, 2, C//2, N, -1).reshape(2*B, C//2, N, -1)\n         enc_out, attns = self.decoder(enc_out)\n         # [B * C, N, P]\n         dec_out = enc_out.detach()\n-        dec_out = dec_out.requires_grad = True\n+        dec_out.requires_grad = True\n         loss = 0\n         for i in range(4):\n             pred = self.projs[i](enc_out).reshape(B, C, -1).permute(0, 2, 1)\n             if self.use_norm:\n"
                },
                {
                    "date": 1731075616893,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -71,9 +71,9 @@\n         for i in range(4):\n             pred = self.projs[i](enc_out).reshape(B, C, -1).permute(0, 2, 1)\n             if self.use_norm:\n                 pred = pred * stdev + means\n-            cur_loss = self.criterion(pred, x_dec[i*self.input_token_len:i*self.input_token_len+L])\n+            cur_loss = self.criterion(pred, x_dec[:,i*self.input_token_len:i*self.input_token_len+L,:])\n             cur_loss.backward()\n             loss += cur_loss.item()\n         enc_out.backward(gradient=dec_out.grad)\n         # dec_out = dec_out.reshape(B, 2, C//2, N, -1)\n"
                },
                {
                    "date": 1731075667632,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -71,8 +71,9 @@\n         for i in range(4):\n             pred = self.projs[i](enc_out).reshape(B, C, -1).permute(0, 2, 1)\n             if self.use_norm:\n                 pred = pred * stdev + means\n+            print(pred.shape, x_dec[:,i*self.input_token_len:i*self.input_token_len+L,:].shape)\n             cur_loss = self.criterion(pred, x_dec[:,i*self.input_token_len:i*self.input_token_len+L,:])\n             cur_loss.backward()\n             loss += cur_loss.item()\n         enc_out.backward(gradient=dec_out.grad)\n"
                },
                {
                    "date": 1731075859904,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -37,9 +37,9 @@\n                 for l in range(configs.e_layers)\n             ],\n             norm_layer=torch.nn.LayerNorm(configs.d_model)\n         )\n-        self.projs = nn.ModuleList([nn.Linear(configs.d_model, configs.output_token_len) for i in range(4)])\n+        self.projs = nn.ModuleList([nn.Linear(configs.d_model, configs.input_token_len) for i in range(4)])\n         self.use_norm = configs.use_norm\n         self.criterion = nn.MSELoss()\n \n     def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n@@ -71,9 +71,9 @@\n         for i in range(4):\n             pred = self.projs[i](enc_out).reshape(B, C, -1).permute(0, 2, 1)\n             if self.use_norm:\n                 pred = pred * stdev + means\n-            print(pred.shape, x_dec[:,i*self.input_token_len:i*self.input_token_len+L,:].shape)\n+            print(pred.shape, x_dec[:,i*self.input_token_len:i*self.input_token_len+L,:].shape, x_dec.shape)\n             cur_loss = self.criterion(pred, x_dec[:,i*self.input_token_len:i*self.input_token_len+L,:])\n             cur_loss.backward()\n             loss += cur_loss.item()\n         enc_out.backward(gradient=dec_out.grad)\n"
                },
                {
                    "date": 1731075962614,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -73,9 +73,9 @@\n             if self.use_norm:\n                 pred = pred * stdev + means\n             print(pred.shape, x_dec[:,i*self.input_token_len:i*self.input_token_len+L,:].shape, x_dec.shape)\n             cur_loss = self.criterion(pred, x_dec[:,i*self.input_token_len:i*self.input_token_len+L,:])\n-            cur_loss.backward()\n+            cur_loss.backward(retain_graph=True)\n             loss += cur_loss.item()\n         enc_out.backward(gradient=dec_out.grad)\n         # dec_out = dec_out.reshape(B, 2, C//2, N, -1)\n         # [B, C, L]\n"
                },
                {
                    "date": 1731076181193,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -75,8 +75,9 @@\n             print(pred.shape, x_dec[:,i*self.input_token_len:i*self.input_token_len+L,:].shape, x_dec.shape)\n             cur_loss = self.criterion(pred, x_dec[:,i*self.input_token_len:i*self.input_token_len+L,:])\n             cur_loss.backward(retain_graph=True)\n             loss += cur_loss.item()\n+        print(dec_out.grad.shape, enc_out.shape)\n         enc_out.backward(gradient=dec_out.grad)\n         # dec_out = dec_out.reshape(B, 2, C//2, N, -1)\n         # [B, C, L]\n         # dec_out = enc_out.reshape(B, C, -1)\n"
                },
                {
                    "date": 1731076264302,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -68,9 +68,9 @@\n         dec_out = enc_out.detach()\n         dec_out.requires_grad = True\n         loss = 0\n         for i in range(4):\n-            pred = self.projs[i](enc_out).reshape(B, C, -1).permute(0, 2, 1)\n+            pred = self.projs[i](dec_out).reshape(B, C, -1).permute(0, 2, 1)\n             if self.use_norm:\n                 pred = pred * stdev + means\n             print(pred.shape, x_dec[:,i*self.input_token_len:i*self.input_token_len+L,:].shape, x_dec.shape)\n             cur_loss = self.criterion(pred, x_dec[:,i*self.input_token_len:i*self.input_token_len+L,:])\n"
                },
                {
                    "date": 1731076289323,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -71,13 +71,11 @@\n         for i in range(4):\n             pred = self.projs[i](dec_out).reshape(B, C, -1).permute(0, 2, 1)\n             if self.use_norm:\n                 pred = pred * stdev + means\n-            print(pred.shape, x_dec[:,i*self.input_token_len:i*self.input_token_len+L,:].shape, x_dec.shape)\n             cur_loss = self.criterion(pred, x_dec[:,i*self.input_token_len:i*self.input_token_len+L,:])\n             cur_loss.backward(retain_graph=True)\n             loss += cur_loss.item()\n-        print(dec_out.grad.shape, enc_out.shape)\n         enc_out.backward(gradient=dec_out.grad)\n         # dec_out = dec_out.reshape(B, 2, C//2, N, -1)\n         # [B, C, L]\n         # dec_out = enc_out.reshape(B, C, -1)\n"
                },
                {
                    "date": 1731076482069,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -73,9 +73,9 @@\n             if self.use_norm:\n                 pred = pred * stdev + means\n             cur_loss = self.criterion(pred, x_dec[:,i*self.input_token_len:i*self.input_token_len+L,:])\n             cur_loss.backward(retain_graph=True)\n-            loss += cur_loss.item()\n+            loss += cur_loss\n         enc_out.backward(gradient=dec_out.grad)\n         # dec_out = dec_out.reshape(B, 2, C//2, N, -1)\n         # [B, C, L]\n         # dec_out = enc_out.reshape(B, C, -1)\n"
                },
                {
                    "date": 1731077152583,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -64,27 +64,27 @@\n         enc_out = self.dropout(enc_out).reshape(B, C, N, -1)\n         # enc_out = enc_out.reshape(B, 2, C//2, N, -1).reshape(2*B, C//2, N, -1)\n         enc_out, attns = self.decoder(enc_out)\n         # [B * C, N, P]\n-        dec_out = enc_out.detach()\n-        dec_out.requires_grad = True\n-        loss = 0\n-        for i in range(4):\n-            pred = self.projs[i](dec_out).reshape(B, C, -1).permute(0, 2, 1)\n-            if self.use_norm:\n-                pred = pred * stdev + means\n-            cur_loss = self.criterion(pred, x_dec[:,i*self.input_token_len:i*self.input_token_len+L,:])\n-            cur_loss.backward(retain_graph=True)\n-            loss += cur_loss\n-        enc_out.backward(gradient=dec_out.grad)\n+        # dec_out = enc_out.detach()\n+        # dec_out.requires_grad = True\n+        # loss = 0\n+        # for i in range(4):\n+        #     pred = self.projs[i](dec_out).reshape(B, C, -1).permute(0, 2, 1)\n+        #     if self.use_norm:\n+        #         pred = pred * stdev + means\n+        #     cur_loss = self.criterion(pred, x_dec[:,i*self.input_token_len:i*self.input_token_len+L,:])\n+        #     cur_loss.backward(retain_graph=True)\n+        #     loss += cur_loss\n+        # enc_out.backward(gradient=dec_out.grad)\n         # dec_out = dec_out.reshape(B, 2, C//2, N, -1)\n         # [B, C, L]\n-        # dec_out = enc_out.reshape(B, C, -1)\n-        # # [B, L, C]\n-        # dec_out = dec_out.permute(0, 2, 1)\n-        # if self.use_norm:\n-        #     dec_out = dec_out * stdev + means\n-        return loss/4\n+        dec_out = enc_out.reshape(B, C, -1)\n+        # [B, L, C]\n+        dec_out = dec_out.permute(0, 2, 1)\n+        if self.use_norm:\n+            dec_out = dec_out * stdev + means\n+        return dec_out\n     \n     def inference(self, x_enc, steps):\n         if self.use_norm:\n             means = x_enc.mean(1, keepdim=True).detach()\n"
                },
                {
                    "date": 1731077160718,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -77,8 +77,9 @@\n         #     loss += cur_loss\n         # enc_out.backward(gradient=dec_out.grad)\n         # dec_out = dec_out.reshape(B, 2, C//2, N, -1)\n         # [B, C, L]\n+        dec_out = self.proj(dec_out)\n         dec_out = enc_out.reshape(B, C, -1)\n         # [B, L, C]\n         dec_out = dec_out.permute(0, 2, 1)\n         if self.use_norm:\n"
                },
                {
                    "date": 1731077185615,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -37,9 +37,10 @@\n                 for l in range(configs.e_layers)\n             ],\n             norm_layer=torch.nn.LayerNorm(configs.d_model)\n         )\n-        self.projs = nn.ModuleList([nn.Linear(configs.d_model, configs.input_token_len) for i in range(4)])\n+        # self.projs = nn.ModuleList([nn.Linear(configs.d_model, configs.input_token_len) for i in range(4)])\n+        self.proj = nn.Linear(configs.d_model, configs.input_token_len)\n         self.use_norm = configs.use_norm\n         self.criterion = nn.MSELoss()\n \n     def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n"
                },
                {
                    "date": 1731077197697,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -38,9 +38,9 @@\n             ],\n             norm_layer=torch.nn.LayerNorm(configs.d_model)\n         )\n         # self.projs = nn.ModuleList([nn.Linear(configs.d_model, configs.input_token_len) for i in range(4)])\n-        self.proj = nn.Linear(configs.d_model, configs.input_token_len)\n+        self.proj = nn.Linear(configs.d_model, configs.output_token_len)\n         self.use_norm = configs.use_norm\n         self.criterion = nn.MSELoss()\n \n     def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n"
                },
                {
                    "date": 1731077203546,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -40,9 +40,9 @@\n         )\n         # self.projs = nn.ModuleList([nn.Linear(configs.d_model, configs.input_token_len) for i in range(4)])\n         self.proj = nn.Linear(configs.d_model, configs.output_token_len)\n         self.use_norm = configs.use_norm\n-        self.criterion = nn.MSELoss()\n+        # self.criterion = nn.MSELoss()\n \n     def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n         if self.use_norm:\n             means = x_enc.mean(1, keepdim=True).detach()\n"
                },
                {
                    "date": 1731077224039,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -78,9 +78,9 @@\n         #     loss += cur_loss\n         # enc_out.backward(gradient=dec_out.grad)\n         # dec_out = dec_out.reshape(B, 2, C//2, N, -1)\n         # [B, C, L]\n-        dec_out = self.proj(dec_out)\n+        dec_out = self.proj(enc_out)\n         dec_out = enc_out.reshape(B, C, -1)\n         # [B, L, C]\n         dec_out = dec_out.permute(0, 2, 1)\n         if self.use_norm:\n"
                },
                {
                    "date": 1731077490431,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -78,9 +78,9 @@\n         #     loss += cur_loss\n         # enc_out.backward(gradient=dec_out.grad)\n         # dec_out = dec_out.reshape(B, 2, C//2, N, -1)\n         # [B, C, L]\n-        dec_out = self.proj(enc_out)\n+        dec_out = self.proj(_out)\n         dec_out = enc_out.reshape(B, C, -1)\n         # [B, L, C]\n         dec_out = dec_out.permute(0, 2, 1)\n         if self.use_norm:\n@@ -116,9 +116,6 @@\n         if self.use_norm:\n             pred = pred * stdev + means\n         return pred\n \n-    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None, steps=1):\n-        if self.training:\n-            return self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n-        else:\n-            return self.inference(x_enc, steps)\n+    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n+        return self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n"
                },
                {
                    "date": 1731077511328,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -78,9 +78,9 @@\n         #     loss += cur_loss\n         # enc_out.backward(gradient=dec_out.grad)\n         # dec_out = dec_out.reshape(B, 2, C//2, N, -1)\n         # [B, C, L]\n-        dec_out = self.proj(_out)\n+        dec_out = self.proj(enc_out)\n         dec_out = enc_out.reshape(B, C, -1)\n         # [B, L, C]\n         dec_out = dec_out.permute(0, 2, 1)\n         if self.use_norm:\n@@ -116,6 +116,6 @@\n         if self.use_norm:\n             pred = pred * stdev + means\n         return pred\n \n-    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n+    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None, steps=1):\n         return self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n"
                },
                {
                    "date": 1731077573359,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -79,9 +79,9 @@\n         # enc_out.backward(gradient=dec_out.grad)\n         # dec_out = dec_out.reshape(B, 2, C//2, N, -1)\n         # [B, C, L]\n         dec_out = self.proj(enc_out)\n-        dec_out = enc_out.reshape(B, C, -1)\n+        dec_out = dec_out.reshape(B, C, -1)\n         # [B, L, C]\n         dec_out = dec_out.permute(0, 2, 1)\n         if self.use_norm:\n             dec_out = dec_out * stdev + means\n"
                },
                {
                    "date": 1731149440519,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -60,9 +60,9 @@\n         N = x_enc.shape[2]\n         # [B * C, N, P]\n         # x_enc = x_enc.reshape(B * C, N, -1)\n         # [B * C, N, D]\n-        enc_out = self.embedding(x_enc)# + self.position_embedding(x_enc)\n+        enc_out = self.embedding(x_enc) + self.position_embedding(x_enc)\n         enc_out = self.dropout(enc_out).reshape(B, C, N, -1)\n         # enc_out = enc_out.reshape(B, 2, C//2, N, -1).reshape(2*B, C//2, N, -1)\n         enc_out, attns = self.decoder(enc_out)\n         # [B * C, N, P]\n"
                },
                {
                    "date": 1731149445920,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -12,9 +12,9 @@\n     def __init__(self, configs):\n         super().__init__()\n         self.input_token_len = configs.input_token_len\n         self.embedding = nn.Linear(self.input_token_len, configs.d_model, bias=False)\n-        # self.position_embedding = PositionalEmbedding(configs.d_model)\n+        self.position_embedding = PositionalEmbedding(configs.d_model)\n         self.dropout = nn.Dropout(configs.dropout)\n         self.decoder = AlterEncoder(\n             [\n                 EncoderLayer(\n"
                },
                {
                    "date": 1731159598218,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -60,9 +60,9 @@\n         N = x_enc.shape[2]\n         # [B * C, N, P]\n         # x_enc = x_enc.reshape(B * C, N, -1)\n         # [B * C, N, D]\n-        enc_out = self.embedding(x_enc) + self.position_embedding(x_enc)\n+        enc_out = self.embedding(x_enc)# + self.position_embedding(x_enc)\n         enc_out = self.dropout(enc_out).reshape(B, C, N, -1)\n         # enc_out = enc_out.reshape(B, 2, C//2, N, -1).reshape(2*B, C//2, N, -1)\n         enc_out, attns = self.decoder(enc_out)\n         # [B * C, N, P]\n"
                },
                {
                    "date": 1731200383228,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -12,9 +12,9 @@\n     def __init__(self, configs):\n         super().__init__()\n         self.input_token_len = configs.input_token_len\n         self.embedding = nn.Linear(self.input_token_len, configs.d_model, bias=False)\n-        self.position_embedding = PositionalEmbedding(configs.d_model)\n+        # self.position_embedding = PositionalEmbedding(configs.d_model)\n         self.dropout = nn.Dropout(configs.dropout)\n         self.decoder = AlterEncoder(\n             [\n                 EncoderLayer(\n@@ -38,9 +38,9 @@\n             ],\n             norm_layer=torch.nn.LayerNorm(configs.d_model)\n         )\n         # self.projs = nn.ModuleList([nn.Linear(configs.d_model, configs.input_token_len) for i in range(4)])\n-        self.proj = nn.Linear(configs.d_model, configs.output_token_len)\n+        self.projs = nn.Linear(configs.d_model, configs.output_token_len)\n         self.use_norm = configs.use_norm\n         # self.criterion = nn.MSELoss()\n \n     def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n"
                },
                {
                    "date": 1731200414145,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -37,10 +37,10 @@\n                 for l in range(configs.e_layers)\n             ],\n             norm_layer=torch.nn.LayerNorm(configs.d_model)\n         )\n-        # self.projs = nn.ModuleList([nn.Linear(configs.d_model, configs.input_token_len) for i in range(4)])\n-        self.projs = nn.Linear(configs.d_model, configs.output_token_len)\n+        self.projs = nn.ModuleList([nn.Linear(configs.d_model, out) for out in [96 192 336 720]])\n+        # self.proj = nn.Linear(configs.d_model, configs.output_token_len)\n         self.use_norm = configs.use_norm\n         # self.criterion = nn.MSELoss()\n \n     def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n"
                },
                {
                    "date": 1731200493586,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -37,9 +37,9 @@\n                 for l in range(configs.e_layers)\n             ],\n             norm_layer=torch.nn.LayerNorm(configs.d_model)\n         )\n-        self.projs = nn.ModuleList([nn.Linear(configs.d_model, out) for out in [96 192 336 720]])\n+        self.projs = nn.ModuleList([nn.Linear(configs.d_model, out) for out in [96, 192, 336, 720]])\n         # self.proj = nn.Linear(configs.d_model, configs.output_token_len)\n         self.use_norm = configs.use_norm\n         # self.criterion = nn.MSELoss()\n \n"
                },
                {
                    "date": 1731200572051,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -78,14 +78,17 @@\n         #     loss += cur_loss\n         # enc_out.backward(gradient=dec_out.grad)\n         # dec_out = dec_out.reshape(B, 2, C//2, N, -1)\n         # [B, C, L]\n-        dec_out = self.proj(enc_out)\n-        dec_out = dec_out.reshape(B, C, -1)\n-        # [B, L, C]\n-        dec_out = dec_out.permute(0, 2, 1)\n-        if self.use_norm:\n-            dec_out = dec_out * stdev + means\n+        dec_outs = []\n+        for self.proj in self.projs:\n+            dec_out = self.proj(enc_out)\n+            dec_out = dec_out.reshape(B, C, -1)\n+            # [B, L, C]\n+            dec_out = dec_out.permute(0, 2, 1)\n+            if self.use_norm:\n+                dec_out = dec_out * stdev + means\n+            dec_outs.append(dec_out)\n         return dec_out\n     \n     def inference(self, x_enc, steps):\n         if self.use_norm:\n"
                },
                {
                    "date": 1731813677391,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -38,13 +38,13 @@\n             ],\n             norm_layer=torch.nn.LayerNorm(configs.d_model)\n         )\n         self.projs = nn.ModuleList([nn.Linear(configs.d_model, out) for out in [96, 192, 336, 720]])\n-        # self.proj = nn.Linear(configs.d_model, configs.output_token_len)\n+        self.proj = nn.Linear(configs.d_model, configs.output_token_len)\n         self.use_norm = configs.use_norm\n         # self.criterion = nn.MSELoss()\n \n-    def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n+    def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n         if self.use_norm:\n             means = x_enc.mean(1, keepdim=True).detach()\n             x_enc = x_enc - means\n             stdev = torch.sqrt(\n@@ -63,9 +63,9 @@\n         # [B * C, N, D]\n         enc_out = self.embedding(x_enc)# + self.position_embedding(x_enc)\n         enc_out = self.dropout(enc_out).reshape(B, C, N, -1)\n         # enc_out = enc_out.reshape(B, 2, C//2, N, -1).reshape(2*B, C//2, N, -1)\n-        enc_out, attns = self.decoder(enc_out)\n+        enc_out, attns = self.decoder(enc_out,mask)\n         # [B * C, N, P]\n         # dec_out = enc_out.detach()\n         # dec_out.requires_grad = True\n         # loss = 0\n@@ -87,9 +87,9 @@\n             dec_out = dec_out.permute(0, 2, 1)\n             if self.use_norm:\n                 dec_out = dec_out * stdev + means\n             dec_outs.append(dec_out)\n-        return dec_out\n+        return dec_outs\n     \n     def inference(self, x_enc, steps):\n         if self.use_norm:\n             means = x_enc.mean(1, keepdim=True).detach()\n@@ -120,5 +120,5 @@\n             pred = pred * stdev + means\n         return pred\n \n     def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None, steps=1):\n-        return self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n+        return self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec, mask)\n"
                },
                {
                    "date": 1731910159874,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -37,9 +37,9 @@\n                 for l in range(configs.e_layers)\n             ],\n             norm_layer=torch.nn.LayerNorm(configs.d_model)\n         )\n-        self.projs = nn.ModuleList([nn.Linear(configs.d_model, out) for out in [96, 192, 336, 720]])\n+        self.projs = nn.ModuleList([nn.Linear(configs.d_model, out) for out in [96]])\n         self.proj = nn.Linear(configs.d_model, configs.output_token_len)\n         self.use_norm = configs.use_norm\n         # self.criterion = nn.MSELoss()\n \n"
                },
                {
                    "date": 1732889911763,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -37,9 +37,9 @@\n                 for l in range(configs.e_layers)\n             ],\n             norm_layer=torch.nn.LayerNorm(configs.d_model)\n         )\n-        self.projs = nn.ModuleList([nn.Linear(configs.d_model, out) for out in [96]])\n+        self.projs = nn.ModuleList([nn.Linear(configs.d_model, out) for out in [96192336720]])\n         self.proj = nn.Linear(configs.d_model, configs.output_token_len)\n         self.use_norm = configs.use_norm\n         # self.criterion = nn.MSELoss()\n \n"
                },
                {
                    "date": 1732889917667,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -37,9 +37,9 @@\n                 for l in range(configs.e_layers)\n             ],\n             norm_layer=torch.nn.LayerNorm(configs.d_model)\n         )\n-        self.projs = nn.ModuleList([nn.Linear(configs.d_model, out) for out in [96192336720]])\n+        self.projs = nn.ModuleList([nn.Linear(configs.d_model, out) for out in [96,192,336,720]])\n         self.proj = nn.Linear(configs.d_model, configs.output_token_len)\n         self.use_norm = configs.use_norm\n         # self.criterion = nn.MSELoss()\n \n"
                },
                {
                    "date": 1733234576154,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -37,9 +37,9 @@\n                 for l in range(configs.e_layers)\n             ],\n             norm_layer=torch.nn.LayerNorm(configs.d_model)\n         )\n-        self.projs = nn.ModuleList([nn.Linear(configs.d_model, out) for out in [96,192,336,720]])\n+        self.projs = nn.ModuleList([nn.Linear(configs.d_model, out) for out in [96]])\n         self.proj = nn.Linear(configs.d_model, configs.output_token_len)\n         self.use_norm = configs.use_norm\n         # self.criterion = nn.MSELoss()\n \n"
                },
                {
                    "date": 1733372626570,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -37,9 +37,9 @@\n                 for l in range(configs.e_layers)\n             ],\n             norm_layer=torch.nn.LayerNorm(configs.d_model)\n         )\n-        self.projs = nn.ModuleList([nn.Linear(configs.d_model, out) for out in [96]])\n+        self.projs = nn.ModuleList([nn.Linear(configs.d_model, out) for out in [96,192,336,720]])\n         self.proj = nn.Linear(configs.d_model, configs.output_token_len)\n         self.use_norm = configs.use_norm\n         # self.criterion = nn.MSELoss()\n \n"
                },
                {
                    "date": 1733388967226,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -37,9 +37,9 @@\n                 for l in range(configs.e_layers)\n             ],\n             norm_layer=torch.nn.LayerNorm(configs.d_model)\n         )\n-        self.projs = nn.ModuleList([nn.Linear(configs.d_model, out) for out in [96,192,336,720]])\n+        self.projs = nn.ModuleList([nn.Linear(configs.d_model, out) for out in [96]])\n         self.proj = nn.Linear(configs.d_model, configs.output_token_len)\n         self.use_norm = configs.use_norm\n         # self.criterion = nn.MSELoss()\n \n"
                },
                {
                    "date": 1733454369387,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -37,9 +37,9 @@\n                 for l in range(configs.e_layers)\n             ],\n             norm_layer=torch.nn.LayerNorm(configs.d_model)\n         )\n-        self.projs = nn.ModuleList([nn.Linear(configs.d_model, out) for out in [96]])\n+        self.projs = nn.ModuleList([nn.Linear(configs.d_model, out) for out in [96,192,336,720]])\n         self.proj = nn.Linear(configs.d_model, configs.output_token_len)\n         self.use_norm = configs.use_norm\n         # self.criterion = nn.MSELoss()\n \n"
                }
            ],
            "date": 1730121250887,
            "name": "Commit-0",
            "content": "import torch\nfrom torch import nn\nfrom layers.Transformer_EncDec import Encoder, EncoderLayer\nfrom layers.SelfAttention_Family import FullAttention, AttentionLayer\nfrom layers.Embed import PositionalEmbedding\n\n\nclass Model(nn.Module):\n    \"\"\"\n    Paper link: https://arxiv.org/abs/2402.02368\n    \"\"\"\n    def __init__(self, configs):\n        super().__init__()\n        self.input_token_len = configs.input_token_len\n        self.embedding = nn.Linear(self.input_token_len, configs.d_model, bias=False)\n        self.position_embedding = PositionalEmbedding(configs.d_model)\n        self.dropout = nn.Dropout(configs.dropout)\n        self.encoder = Encoder(\n            [\n                EncoderLayer(\n                    AttentionLayer(\n                        FullAttention(True, attention_dropout=configs.dropout, output_attention=False), configs.d_model, configs.n_heads),\n                    configs.d_model,\n                    configs.d_ff,\n                    dropout=configs.dropout,\n                    activation=configs.activation\n                ) for l in range(configs.e_layers)\n            ],\n            norm_layer=torch.nn.LayerNorm(configs.d_model)\n        )\n        self.head = nn.Linear(configs.d_model, configs.output_token_len)\n        self.use_norm = configs.use_norm\n\n    def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n        if self.use_norm:\n            means = x_enc.mean(1, keepdim=True).detach()\n            x_enc = x_enc - means\n            stdev = torch.sqrt(\n                torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)\n            x_enc /= stdev\n        # [B, L, C]\n        B, _, C = x_enc.shape\n        # [B, C, L]\n        x_enc = x_enc.permute(0, 2, 1)\n        # [B, C, N, P]\n        x_enc = x_enc.unfold(\n            dimension=-1, size=self.input_token_len, step=self.input_token_len)\n        N = x_enc.shape[2]\n        # [B * C, N, P]\n        x_enc = x_enc.reshape(B * C, N, -1)\n        # [B * C, N, D]\n        enc_out = self.embedding(x_enc) + self.position_embedding(x_enc)\n        enc_out = self.dropout(enc_out).reshape(B , C, N, -1)\n        enc_out, attns = self.encoder(enc_out)\n        # [B * C, N, P]\n        dec_out = self.head(enc_out)\n        # [B, C, L]\n        dec_out = dec_out.reshape(B, C, -1)\n        # [B, L, C]\n        dec_out = dec_out.permute(0, 2, 1)\n        if self.use_norm:\n            dec_out = dec_out * stdev + means\n        return dec_out\n\n    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n        return self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n"
        }
    ]
}