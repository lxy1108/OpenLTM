{
    "sourceFile": "exp/exp_forecast.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 251,
            "patches": [
                {
                    "date": 1730422452540,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1730422954616,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -137,11 +137,11 @@\n                 batch_y = batch_y.float().to(self.device)\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n                 batch_y_mark = batch_y_mark.float().to(self.device)\n                 \n-                indices = torch.randperm(batch_x.size(-1)).to(batch_x.device)\n-                batch_x = batch_x.index_select(-1, indices)\n-                batch_y = batch_y.index_select(-1, indices)\n+                # indices = torch.randperm(batch_x.size(-1)).to(batch_x.device)\n+                # batch_x = batch_x.index_select(-1, indices)\n+                # batch_y = batch_y.index_select(-1, indices)\n \n                 outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)\n                 if self.args.dp:\n                     torch.cuda.synchronize()\n"
                },
                {
                    "date": 1730440061337,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -31,9 +31,11 @@\n             self.device = self.args.gpu\n             model = model.to(self.device)\n             \n         if self.args.adaptation:\n-            model.load_state_dict(torch.load(self.args.pretrain_model_path))\n+            state_dict = torch.load(self.args.pretrain_model_path)\n+            state_dict = dict([('module.'+k,v) for k,v in state_dict.items()])\n+            model.load_state_dict()\n         return model\n \n     def _get_data(self, flag):\n         data_set, data_loader = data_provider(self.args, flag)\n"
                },
                {
                    "date": 1730553895062,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -31,11 +31,12 @@\n             self.device = self.args.gpu\n             model = model.to(self.device)\n             \n         if self.args.adaptation:\n-            state_dict = torch.load(self.args.pretrain_model_path)\n+            state_dict = torch.load(self.args.pretrain_model_path)['state_dict']\n+            print(state_dict.keys())\n             state_dict = dict([('module.'+k,v) for k,v in state_dict.items()])\n-            model.load_state_dict()\n+            model.load_state_dict(state_dict)\n         return model\n \n     def _get_data(self, flag):\n         data_set, data_loader = data_provider(self.args, flag)\n"
                },
                {
                    "date": 1730554033397,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -33,9 +33,9 @@\n             \n         if self.args.adaptation:\n             state_dict = torch.load(self.args.pretrain_model_path)['state_dict']\n             print(state_dict.keys())\n-            state_dict = dict([('module.'+k,v) for k,v in state_dict.items()])\n+            state_dict = dict([(k.replace('module.model', 'module'),v) for k,v in state_dict.items()])\n             model.load_state_dict(state_dict)\n         return model\n \n     def _get_data(self, flag):\n"
                },
                {
                    "date": 1730554306970,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -32,9 +32,8 @@\n             model = model.to(self.device)\n             \n         if self.args.adaptation:\n             state_dict = torch.load(self.args.pretrain_model_path)['state_dict']\n-            print(state_dict.keys())\n             state_dict = dict([(k.replace('module.model', 'module'),v) for k,v in state_dict.items()])\n             model.load_state_dict(state_dict)\n         return model\n \n"
                },
                {
                    "date": 1730554369463,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -33,8 +33,9 @@\n             \n         if self.args.adaptation:\n             state_dict = torch.load(self.args.pretrain_model_path)['state_dict']\n             state_dict = dict([(k.replace('module.model', 'module'),v) for k,v in state_dict.items()])\n+            print(state_dict)\n             model.load_state_dict(state_dict)\n         return model\n \n     def _get_data(self, flag):\n"
                },
                {
                    "date": 1730554542016,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -32,10 +32,11 @@\n             model = model.to(self.device)\n             \n         if self.args.adaptation:\n             state_dict = torch.load(self.args.pretrain_model_path)['state_dict']\n+            print(state_dict.keys())\n             state_dict = dict([(k.replace('module.model', 'module'),v) for k,v in state_dict.items()])\n-            print(state_dict)\n+            \n             model.load_state_dict(state_dict)\n         return model\n \n     def _get_data(self, flag):\n"
                },
                {
                    "date": 1730554568047,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -33,9 +33,9 @@\n             \n         if self.args.adaptation:\n             state_dict = torch.load(self.args.pretrain_model_path)['state_dict']\n             print(state_dict.keys())\n-            state_dict = dict([(k.replace('module.model', 'module'),v) for k,v in state_dict.items()])\n+            state_dict = dict([(k.replace('model.', 'module.'),v) for k,v in state_dict.items()])\n             \n             model.load_state_dict(state_dict)\n         return model\n \n"
                },
                {
                    "date": 1730554635460,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -32,11 +32,9 @@\n             model = model.to(self.device)\n             \n         if self.args.adaptation:\n             state_dict = torch.load(self.args.pretrain_model_path)['state_dict']\n-            print(state_dict.keys())\n             state_dict = dict([(k.replace('model.', 'module.'),v) for k,v in state_dict.items()])\n-            \n             model.load_state_dict(state_dict)\n         return model\n \n     def _get_data(self, flag):\n"
                },
                {
                    "date": 1730554905082,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -32,9 +32,16 @@\n             model = model.to(self.device)\n             \n         if self.args.adaptation:\n             state_dict = torch.load(self.args.pretrain_model_path)['state_dict']\n-            state_dict = dict([(k.replace('model.', 'module.'),v) for k,v in state_dict.items()])\n+            # state_dict = dict([(k.replace('model.', 'module.'),v) for k,v in state_dict.items()])\n+            for k,v in state_dict.items():\n+                if k.startswith('model.patch_embedding.value_embedding'):\n+                    k = k.replace('model.patch_embedding.value_embedding', 'module.embedding')\n+                elif k.startswith('model.patch_embedding.position_embedding'):\n+                    k = k.replace('model.patch_embedding.position_embedding', 'module.position_embedding')\n+                else:\n+                    k = k.replace('model.', 'module.')\n             model.load_state_dict(state_dict)\n         return model\n \n     def _get_data(self, flag):\n"
                },
                {
                    "date": 1730554928721,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -33,16 +33,18 @@\n             \n         if self.args.adaptation:\n             state_dict = torch.load(self.args.pretrain_model_path)['state_dict']\n             # state_dict = dict([(k.replace('model.', 'module.'),v) for k,v in state_dict.items()])\n+            state_dict_new = {}\n             for k,v in state_dict.items():\n                 if k.startswith('model.patch_embedding.value_embedding'):\n                     k = k.replace('model.patch_embedding.value_embedding', 'module.embedding')\n                 elif k.startswith('model.patch_embedding.position_embedding'):\n                     k = k.replace('model.patch_embedding.position_embedding', 'module.position_embedding')\n                 else:\n                     k = k.replace('model.', 'module.')\n-            model.load_state_dict(state_dict)\n+                state_dict_new[k] = v\n+            model.load_state_dict(state_dict_new)\n         return model\n \n     def _get_data(self, flag):\n         data_set, data_loader = data_provider(self.args, flag)\n"
                },
                {
                    "date": 1730636452694,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -80,8 +80,10 @@\n                 batch_y = batch_y.float()\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n                 batch_y_mark = batch_y_mark.float().to(self.device)\n                 \n+                batch_x = batch_x[:,-self.args.input_token_len:,:]\n+                \n                 outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)\n                 if is_test or self.args.nonautoregressive:\n                         outputs = outputs[:, -self.args.output_token_len:, :]\n                         batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n"
                },
                {
                    "date": 1730636512816,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,8 +1,9 @@\n from data_provider.data_factory import data_provider\n from exp.exp_basic import Exp_Basic\n from utils.tools import EarlyStopping, adjust_learning_rate, visual\n from utils.metrics import metric\n+import random\n import torch\n import torch.nn as nn\n from torch import optim\n import os\n@@ -80,9 +81,9 @@\n                 batch_y = batch_y.float()\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n                 batch_y_mark = batch_y_mark.float().to(self.device)\n                 \n-                batch_x = batch_x[:,-self.args.input_token_len:,:]\n+                batch_x = batch_x[:,-self.args.input_token_len*random.randint():,:]\n                 \n                 outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)\n                 if is_test or self.args.nonautoregressive:\n                         outputs = outputs[:, -self.args.output_token_len:, :]\n"
                },
                {
                    "date": 1730636539750,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -81,9 +81,8 @@\n                 batch_y = batch_y.float()\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n                 batch_y_mark = batch_y_mark.float().to(self.device)\n                 \n-                batch_x = batch_x[:,-self.args.input_token_len*random.randint():,:]\n                 \n                 outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)\n                 if is_test or self.args.nonautoregressive:\n                         outputs = outputs[:, -self.args.output_token_len:, :]\n@@ -151,8 +150,10 @@\n                 batch_y = batch_y.float().to(self.device)\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n                 batch_y_mark = batch_y_mark.float().to(self.device)\n                 \n+                batch_x = batch_x[:,-self.args.input_token_len*random.randint():,:]\n+                \n                 # indices = torch.randperm(batch_x.size(-1)).to(batch_x.device)\n                 # batch_x = batch_x.index_select(-1, indices)\n                 # batch_y = batch_y.index_select(-1, indices)\n \n"
                },
                {
                    "date": 1730636547326,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -150,9 +150,9 @@\n                 batch_y = batch_y.float().to(self.device)\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n                 batch_y_mark = batch_y_mark.float().to(self.device)\n                 \n-                batch_x = batch_x[:,-self.args.input_token_len*random.randint():,:]\n+                batch_x = batch_x[:,-self.args.input_token_len*random.randint(1,7):,:]\n                 \n                 # indices = torch.randperm(batch_x.size(-1)).to(batch_x.device)\n                 # batch_x = batch_x.index_select(-1, indices)\n                 # batch_y = batch_y.index_select(-1, indices)\n"
                },
                {
                    "date": 1730636664805,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -151,8 +151,9 @@\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n                 batch_y_mark = batch_y_mark.float().to(self.device)\n                 \n                 batch_x = batch_x[:,-self.args.input_token_len*random.randint(1,7):,:]\n+                batch_x = batch_y[:,-self.args.input_token_len*random.randint(1,7):,:]\n                 \n                 # indices = torch.randperm(batch_x.size(-1)).to(batch_x.device)\n                 # batch_x = batch_x.index_select(-1, indices)\n                 # batch_y = batch_y.index_select(-1, indices)\n"
                },
                {
                    "date": 1730636728051,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -150,10 +150,11 @@\n                 batch_y = batch_y.float().to(self.device)\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n                 batch_y_mark = batch_y_mark.float().to(self.device)\n                 \n-                batch_x = batch_x[:,-self.args.input_token_len*random.randint(1,7):,:]\n-                batch_x = batch_y[:,-self.args.input_token_len*random.randint(1,7):,:]\n+                patch_num = random.randint(1,7)\n+                batch_x = batch_x[:,-self.args.input_token_len*patch_num:,:]\n+                batch_y = batch_y[:,-self.args.input_token_len*patch_num:,:]\n                 \n                 # indices = torch.randperm(batch_x.size(-1)).to(batch_x.device)\n                 # batch_x = batch_x.index_select(-1, indices)\n                 # batch_y = batch_y.index_select(-1, indices)\n"
                },
                {
                    "date": 1730636763412,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -150,11 +150,11 @@\n                 batch_y = batch_y.float().to(self.device)\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n                 batch_y_mark = batch_y_mark.float().to(self.device)\n                 \n-                patch_num = random.randint(1,7)\n-                batch_x = batch_x[:,-self.args.input_token_len*patch_num:,:]\n-                batch_y = batch_y[:,-self.args.input_token_len*patch_num:,:]\n+                lookback = random.randint(1,7)*self.args.input_token_len\n+                batch_x = batch_x[:,-lookback:,:]\n+                batch_y = batch_y[:,-lookback:,:]\n                 \n                 # indices = torch.randperm(batch_x.size(-1)).to(batch_x.device)\n                 # batch_x = batch_x.index_select(-1, indices)\n                 # batch_y = batch_y.index_select(-1, indices)\n"
                },
                {
                    "date": 1730641007791,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -150,11 +150,11 @@\n                 batch_y = batch_y.float().to(self.device)\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n                 batch_y_mark = batch_y_mark.float().to(self.device)\n                 \n-                lookback = random.randint(1,7)*self.args.input_token_len\n-                batch_x = batch_x[:,-lookback:,:]\n-                batch_y = batch_y[:,-lookback:,:]\n+                # lookback = random.randint(1,7)*self.args.input_token_len\n+                # batch_x = batch_x[:,-lookback:,:]\n+                # batch_y = batch_y[:,-lookback:,:]\n                 \n                 # indices = torch.randperm(batch_x.size(-1)).to(batch_x.device)\n                 # batch_x = batch_x.index_select(-1, indices)\n                 # batch_y = batch_y.index_select(-1, indices)\n"
                },
                {
                    "date": 1730679237956,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -150,11 +150,11 @@\n                 batch_y = batch_y.float().to(self.device)\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n                 batch_y_mark = batch_y_mark.float().to(self.device)\n                 \n-                # lookback = random.randint(1,7)*self.args.input_token_len\n-                # batch_x = batch_x[:,-lookback:,:]\n-                # batch_y = batch_y[:,-lookback:,:]\n+                lookback = random.randint(1,7)*self.args.input_token_len\n+                batch_x = batch_x[:,-lookback:,:]\n+                batch_y = batch_y[:,-lookback:,:]\n                 \n                 # indices = torch.randperm(batch_x.size(-1)).to(batch_x.device)\n                 # batch_x = batch_x.index_select(-1, indices)\n                 # batch_y = batch_y.index_select(-1, indices)\n"
                },
                {
                    "date": 1730679303505,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -150,9 +150,9 @@\n                 batch_y = batch_y.float().to(self.device)\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n                 batch_y_mark = batch_y_mark.float().to(self.device)\n                 \n-                lookback = random.randint(1,7)*self.args.input_token_len\n+                lookback = random.randint(1,max(7,epoch+1))*self.args.input_token_len\n                 batch_x = batch_x[:,-lookback:,:]\n                 batch_y = batch_y[:,-lookback:,:]\n                 \n                 # indices = torch.randperm(batch_x.size(-1)).to(batch_x.device)\n"
                },
                {
                    "date": 1730679581788,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -37,13 +37,13 @@\n             # state_dict = dict([(k.replace('model.', 'module.'),v) for k,v in state_dict.items()])\n             state_dict_new = {}\n             for k,v in state_dict.items():\n                 if k.startswith('model.patch_embedding.value_embedding'):\n-                    k = k.replace('model.patch_embedding.value_embedding', 'module.embedding')\n+                    k = k.replace('model.patch_embedding.value_embedding', 'embedding')\n                 elif k.startswith('model.patch_embedding.position_embedding'):\n-                    k = k.replace('model.patch_embedding.position_embedding', 'module.position_embedding')\n+                    k = k.replace('model.patch_embedding.position_embedding', 'position_embedding')\n                 else:\n-                    k = k.replace('model.', 'module.')\n+                    k = k.replace('model.', '')\n                 state_dict_new[k] = v\n             model.load_state_dict(state_dict_new)\n         return model\n \n"
                },
                {
                    "date": 1730679635423,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -37,13 +37,13 @@\n             # state_dict = dict([(k.replace('model.', 'module.'),v) for k,v in state_dict.items()])\n             state_dict_new = {}\n             for k,v in state_dict.items():\n                 if k.startswith('model.patch_embedding.value_embedding'):\n-                    k = k.replace('model.patch_embedding.value_embedding', 'embedding')\n+                    k = k.replace('model.patch_embedding.value_embedding', 'module.embedding')\n                 elif k.startswith('model.patch_embedding.position_embedding'):\n-                    k = k.replace('model.patch_embedding.position_embedding', 'position_embedding')\n+                    k = k.replace('model.patch_embedding.position_embedding', 'module.position_embedding')\n                 else:\n-                    k = k.replace('model.', '')\n+                    k = k.replace('model.', 'module.')\n                 state_dict_new[k] = v\n             model.load_state_dict(state_dict_new)\n         return model\n \n@@ -150,9 +150,9 @@\n                 batch_y = batch_y.float().to(self.device)\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n                 batch_y_mark = batch_y_mark.float().to(self.device)\n                 \n-                lookback = random.randint(1,max(7,epoch+1))*self.args.input_token_len\n+                lookback = random.randint(1,min(7,epoch+1))*self.args.input_token_len\n                 batch_x = batch_x[:,-lookback:,:]\n                 batch_y = batch_y[:,-lookback:,:]\n                 \n                 # indices = torch.randperm(batch_x.size(-1)).to(batch_x.device)\n"
                },
                {
                    "date": 1730689899787,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -150,9 +150,9 @@\n                 batch_y = batch_y.float().to(self.device)\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n                 batch_y_mark = batch_y_mark.float().to(self.device)\n                 \n-                lookback = random.randint(1,min(7,epoch+1))*self.args.input_token_len\n+                lookback = random.randint(1,min(7,epoch+4))*self.args.input_token_len\n                 batch_x = batch_x[:,-lookback:,:]\n                 batch_y = batch_y[:,-lookback:,:]\n                 \n                 # indices = torch.randperm(batch_x.size(-1)).to(batch_x.device)\n"
                },
                {
                    "date": 1730697217836,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -150,11 +150,11 @@\n                 batch_y = batch_y.float().to(self.device)\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n                 batch_y_mark = batch_y_mark.float().to(self.device)\n                 \n-                lookback = random.randint(1,min(7,epoch+4))*self.args.input_token_len\n-                batch_x = batch_x[:,-lookback:,:]\n-                batch_y = batch_y[:,-lookback:,:]\n+                # lookback = random.randint(1,min(7,epoch+4))*self.args.input_token_len\n+                # batch_x = batch_x[:,-lookback:,:]\n+                # batch_y = batch_y[:,-lookback:,:]\n                 \n                 # indices = torch.randperm(batch_x.size(-1)).to(batch_x.device)\n                 # batch_x = batch_x.index_select(-1, indices)\n                 # batch_y = batch_y.index_select(-1, indices)\n"
                },
                {
                    "date": 1730721590380,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -37,13 +37,13 @@\n             # state_dict = dict([(k.replace('model.', 'module.'),v) for k,v in state_dict.items()])\n             state_dict_new = {}\n             for k,v in state_dict.items():\n                 if k.startswith('model.patch_embedding.value_embedding'):\n-                    k = k.replace('model.patch_embedding.value_embedding', 'module.embedding')\n+                    k = k.replace('model.patch_embedding.value_embedding', 'embedding')\n                 elif k.startswith('model.patch_embedding.position_embedding'):\n-                    k = k.replace('model.patch_embedding.position_embedding', 'module.position_embedding')\n+                    k = k.replace('model.patch_embedding.position_embedding', 'position_embedding')\n                 else:\n-                    k = k.replace('model.', 'module.')\n+                    k = k.replace('model.', '')\n                 state_dict_new[k] = v\n             model.load_state_dict(state_dict_new)\n         return model\n \n"
                },
                {
                    "date": 1730730574499,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -42,8 +42,10 @@\n                 elif k.startswith('model.patch_embedding.position_embedding'):\n                     k = k.replace('model.patch_embedding.position_embedding', 'position_embedding')\n                 else:\n                     k = k.replace('model.', '')\n+                if self.args.dp:\n+                    k = 'module' + k\n                 state_dict_new[k] = v\n             model.load_state_dict(state_dict_new)\n         return model\n \n"
                },
                {
                    "date": 1730730598441,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -43,9 +43,9 @@\n                     k = k.replace('model.patch_embedding.position_embedding', 'position_embedding')\n                 else:\n                     k = k.replace('model.', '')\n                 if self.args.dp:\n-                    k = 'module' + k\n+                    k = 'module.' + k\n                 state_dict_new[k] = v\n             model.load_state_dict(state_dict_new)\n         return model\n \n"
                },
                {
                    "date": 1730988366718,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -45,9 +45,9 @@\n                     k = k.replace('model.', '')\n                 if self.args.dp:\n                     k = 'module.' + k\n                 state_dict_new[k] = v\n-            model.load_state_dict(state_dict_new)\n+            model.load_state_dict(state_dict_new, strict=False)\n         return model\n \n     def _get_data(self, flag):\n         data_set, data_loader = data_provider(self.args, flag)\n"
                },
                {
                    "date": 1731074533462,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -160,9 +160,9 @@\n                 # indices = torch.randperm(batch_x.size(-1)).to(batch_x.device)\n                 # batch_x = batch_x.index_select(-1, indices)\n                 # batch_y = batch_y.index_select(-1, indices)\n \n-                outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)\n+                loss = self.model(batch_x, batch_x_mark, None, batch_y_mark)\n                 if self.args.dp:\n                     torch.cuda.synchronize()\n                 if self.args.nonautoregressive:\n                     batch_y = batch_y[:, -self.args.output_token_len:, :]\n"
                },
                {
                    "date": 1731074547027,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -172,9 +172,9 @@\n                         batch_y = batch_y[:, -self.args.output_token_len:, -1]\n                     else:\n                         outputs = outputs[:, :, -1]\n                         batch_y = batch_y[:, :, -1]\n-                loss = criterion(outputs, batch_y)\n+                # loss = criterion(outputs, batch_y)\n                 if (i + 1) % 100 == 0:\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                         print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n                         speed = (time.time() - time_now) / iter_count\n@@ -182,9 +182,9 @@\n                         print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n                         iter_count = 0\n                         time_now = time.time()\n \n-                loss.backward()\n+                # loss.backward()\n                 model_optim.step()\n \n             if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                 print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n"
                },
                {
                    "date": 1731074566822,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -160,9 +160,9 @@\n                 # indices = torch.randperm(batch_x.size(-1)).to(batch_x.device)\n                 # batch_x = batch_x.index_select(-1, indices)\n                 # batch_y = batch_y.index_select(-1, indices)\n \n-                loss = self.model(batch_x, batch_x_mark, None, batch_y_mark)\n+                loss = self.model(batch_x, batch_x_mark, batch_y, batch_y_mark)\n                 if self.args.dp:\n                     torch.cuda.synchronize()\n                 if self.args.nonautoregressive:\n                     batch_y = batch_y[:, -self.args.output_token_len:, :]\n"
                },
                {
                    "date": 1731074692209,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -75,8 +75,12 @@\n         time_now = time.time()\n         test_steps = len(vali_loader)\n         iter_count = 0\n         self.model.eval()\n+        inference_steps = self.args.test_pred_len // self.args.input_token_len\n+                dis = self.args.test_pred_len - inference_steps * self.args.input_token_len\n+                if dis != 0:\n+                    inference_steps += 1\n         with torch.no_grad():\n             for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(vali_loader):\n                 iter_count += 1\n                 batch_x = batch_x.float().to(self.device)\n"
                },
                {
                    "date": 1731074875500,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -76,11 +76,11 @@\n         test_steps = len(vali_loader)\n         iter_count = 0\n         self.model.eval()\n         inference_steps = self.args.test_pred_len // self.args.input_token_len\n-                dis = self.args.test_pred_len - inference_steps * self.args.input_token_len\n-                if dis != 0:\n-                    inference_steps += 1\n+        dis = self.args.test_pred_len - inference_steps * self.args.input_token_len\n+        if dis != 0:\n+            inference_steps += 1\n         with torch.no_grad():\n             for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(vali_loader):\n                 iter_count += 1\n                 batch_x = batch_x.float().to(self.device)\n@@ -88,9 +88,9 @@\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n                 batch_y_mark = batch_y_mark.float().to(self.device)\n                 \n                 \n-                outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)\n+                outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark, inference_steps)[:,:self.test_pred_len,:]\n                 if is_test or self.args.nonautoregressive:\n                         outputs = outputs[:, -self.args.output_token_len:, :]\n                         batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n                 else:\n"
                },
                {
                    "date": 1731074891241,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -89,14 +89,15 @@\n                 batch_y_mark = batch_y_mark.float().to(self.device)\n                 \n                 \n                 outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark, inference_steps)[:,:self.test_pred_len,:]\n-                if is_test or self.args.nonautoregressive:\n-                        outputs = outputs[:, -self.args.output_token_len:, :]\n-                        batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n-                else:\n-                    outputs = outputs[:, :, :]\n-                    batch_y = batch_y[:, :, :].to(self.device)\n+                batch_y = batch_y[:, :, :].to(self.device)\n+                # if is_test or self.args.nonautoregressive:\n+                #         outputs = outputs[:, -self.args.output_token_len:, :]\n+                #         batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n+                # else:\n+                #     outputs = outputs[:, :, :]\n+                #     batch_y = batch_y[:, :, :].to(self.device)\n \n                 if self.args.covariate:\n                     if self.args.last_token:\n                         outputs = outputs[:, -self.args.output_token_len:, -1]\n"
                },
                {
                    "date": 1731074934702,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -239,20 +239,21 @@\n         time_now = time.time()\n         test_steps = len(test_loader)\n         iter_count = 0\n         self.model.eval()\n+        inference_steps = self.args.test_pred_len // self.args.input_token_len\n+                dis = self.args.test_pred_len - inference_steps * self.args.input_token_len\n+                if dis != 0:\n+                    inference_steps += 1\n         with torch.no_grad():\n             for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n                 iter_count += 1\n                 batch_x = batch_x.float().to(self.device)\n                 batch_y = batch_y.float().to(self.device)\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n                 batch_y_mark = batch_y_mark.float().to(self.device)\n                 \n-                inference_steps = self.args.test_pred_len // self.args.input_token_len\n-                dis = self.args.test_pred_len - inference_steps * self.args.input_token_len\n-                if dis != 0:\n-                    inference_steps += 1\n+                \n                 pred_y = []\n                 for j in range(inference_steps):  \n                     if len(pred_y) != 0:\n                         batch_x = torch.cat([batch_x[:, self.args.input_token_len:, :], pred_y[-1]], dim=1)\n"
                },
                {
                    "date": 1731074988793,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -240,27 +240,27 @@\n         test_steps = len(test_loader)\n         iter_count = 0\n         self.model.eval()\n         inference_steps = self.args.test_pred_len // self.args.input_token_len\n-                dis = self.args.test_pred_len - inference_steps * self.args.input_token_len\n-                if dis != 0:\n-                    inference_steps += 1\n+        dis = self.args.test_pred_len - inference_steps * self.args.input_token_len\n+        if dis != 0:\n+            inference_steps += 1\n         with torch.no_grad():\n             for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n                 iter_count += 1\n                 batch_x = batch_x.float().to(self.device)\n                 batch_y = batch_y.float().to(self.device)\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n                 batch_y_mark = batch_y_mark.float().to(self.device)\n                 \n-                \n-                pred_y = []\n-                for j in range(inference_steps):  \n-                    if len(pred_y) != 0:\n-                        batch_x = torch.cat([batch_x[:, self.args.input_token_len:, :], pred_y[-1]], dim=1)\n-                    outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)\n-                    pred_y.append(outputs[:, -self.args.output_token_len:, :])\n-                pred_y = torch.cat(pred_y, dim=1)\n+                # pred_y = []\n+                # for j in range(inference_steps):  \n+                #     if len(pred_y) != 0:\n+                #         batch_x = torch.cat([batch_x[:, self.args.input_token_len:, :], pred_y[-1]], dim=1)\n+                #     outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)\n+                #     pred_y.append(outputs[:, -self.args.output_token_len:, :])\n+                # pred_y = torch.cat(pred_y, dim=1)\n+                pred_y = self.model(batch_x, batch_x_mark, None, batch_y_mark, inference_steps)\n                 if dis != 0:\n                     pred_y = pred_y[:, :-dis, :]\n                 batch_y = batch_y[:, -self.args.test_pred_len:, :].to(self.device)\n                 \n"
                },
                {
                    "date": 1731076334339,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -186,8 +186,10 @@\n                         left_time = speed * ((self.args.train_epochs - epoch) * train_steps - i)\n                         print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n                         iter_count = 0\n                         time_now = time.time()\n+                    vali_loss = self.vali(vali_data, vali_loader, criterion, is_test=self.args.valid_last)\n+            test_loss = self.vali(test_data, test_loader, criterion, is_test=True)\n \n                 # loss.backward()\n                 model_optim.step()\n \n"
                },
                {
                    "date": 1731076352532,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -187,9 +187,12 @@\n                         print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n                         iter_count = 0\n                         time_now = time.time()\n                     vali_loss = self.vali(vali_data, vali_loader, criterion, is_test=self.args.valid_last)\n-            test_loss = self.vali(test_data, test_loader, criterion, is_test=True)\n+                    test_loss = self.vali(test_data, test_loader, criterion, is_test=True)\n+                    if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n+                print(\"Epoch: {}, Steps: {} | Vali Loss: {:.7f} Test Loss: {:.7f}\".format(\n+                    epoch + 1, train_steps, vali_loss, test_loss))\n \n                 # loss.backward()\n                 model_optim.step()\n \n"
                },
                {
                    "date": 1731076490507,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -178,9 +178,9 @@\n                     else:\n                         outputs = outputs[:, :, -1]\n                         batch_y = batch_y[:, :, -1]\n                 # loss = criterion(outputs, batch_y)\n-                if (i + 1) % 100 == 0:\n+                if (i + 1) % 10 == 0:\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                         print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n                         speed = (time.time() - time_now) / iter_count\n                         left_time = speed * ((self.args.train_epochs - epoch) * train_steps - i)\n@@ -189,10 +189,10 @@\n                         time_now = time.time()\n                     vali_loss = self.vali(vali_data, vali_loader, criterion, is_test=self.args.valid_last)\n                     test_loss = self.vali(test_data, test_loader, criterion, is_test=True)\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n-                print(\"Epoch: {}, Steps: {} | Vali Loss: {:.7f} Test Loss: {:.7f}\".format(\n-                    epoch + 1, train_steps, vali_loss, test_loss))\n+                        print(\"Epoch: {}, Steps: {} | Vali Loss: {:.7f} Test Loss: {:.7f}\".format(\n+                            epoch + 1, train_steps, vali_loss, test_loss))\n \n                 # loss.backward()\n                 model_optim.step()\n \n"
                },
                {
                    "date": 1731076534670,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -88,9 +88,9 @@\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n                 batch_y_mark = batch_y_mark.float().to(self.device)\n                 \n                 \n-                outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark, inference_steps)[:,:self.test_pred_len,:]\n+                outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark, inference_steps)[:,:self.args.output_token_len,:]\n                 batch_y = batch_y[:, :, :].to(self.device)\n                 # if is_test or self.args.nonautoregressive:\n                 #         outputs = outputs[:, -self.args.output_token_len:, :]\n                 #         batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n"
                },
                {
                    "date": 1731076816429,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -90,8 +90,9 @@\n                 \n                 \n                 outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark, inference_steps)[:,:self.args.output_token_len,:]\n                 batch_y = batch_y[:, :, :].to(self.device)\n+                print(outputs.shape, batch_y.shape)\n                 # if is_test or self.args.nonautoregressive:\n                 #         outputs = outputs[:, -self.args.output_token_len:, :]\n                 #         batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n                 # else:\n"
                },
                {
                    "date": 1731077010537,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -79,8 +79,9 @@\n         inference_steps = self.args.test_pred_len // self.args.input_token_len\n         dis = self.args.test_pred_len - inference_steps * self.args.input_token_len\n         if dis != 0:\n             inference_steps += 1\n+        print(inference_steps)\n         with torch.no_grad():\n             for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(vali_loader):\n                 iter_count += 1\n                 batch_x = batch_x.float().to(self.device)\n"
                },
                {
                    "date": 1731077083798,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -89,17 +89,16 @@\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n                 batch_y_mark = batch_y_mark.float().to(self.device)\n                 \n                 \n-                outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark, inference_steps)[:,:self.args.output_token_len,:]\n-                batch_y = batch_y[:, :, :].to(self.device)\n-                print(outputs.shape, batch_y.shape)\n-                # if is_test or self.args.nonautoregressive:\n-                #         outputs = outputs[:, -self.args.output_token_len:, :]\n-                #         batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n-                # else:\n-                #     outputs = outputs[:, :, :]\n-                #     batch_y = batch_y[:, :, :].to(self.device)\n+                # outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark, inference_steps)[:,:self.args.output_token_len,:]\n+                # batch_y = batch_y[:, :, :].to(self.device)\n+                if is_test or self.args.nonautoregressive:\n+                        outputs = outputs[:, -self.args.output_token_len:, :]\n+                        batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n+                else:\n+                    outputs = outputs[:, :, :]\n+                    batch_y = batch_y[:, :, :].to(self.device)\n \n                 if self.args.covariate:\n                     if self.args.last_token:\n                         outputs = outputs[:, -self.args.output_token_len:, -1]\n"
                },
                {
                    "date": 1731077098898,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -178,9 +178,9 @@\n                         batch_y = batch_y[:, -self.args.output_token_len:, -1]\n                     else:\n                         outputs = outputs[:, :, -1]\n                         batch_y = batch_y[:, :, -1]\n-                # loss = criterion(outputs, batch_y)\n+                loss = criterion(outputs, batch_y)\n                 if (i + 1) % 10 == 0:\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                         print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n                         speed = (time.time() - time_now) / iter_count\n@@ -193,9 +193,9 @@\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                         print(\"Epoch: {}, Steps: {} | Vali Loss: {:.7f} Test Loss: {:.7f}\".format(\n                             epoch + 1, train_steps, vali_loss, test_loss))\n \n-                # loss.backward()\n+                loss.backward()\n                 model_optim.step()\n \n             if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                 print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n"
                },
                {
                    "date": 1731077104744,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -166,9 +166,9 @@\n                 # indices = torch.randperm(batch_x.size(-1)).to(batch_x.device)\n                 # batch_x = batch_x.index_select(-1, indices)\n                 # batch_y = batch_y.index_select(-1, indices)\n \n-                loss = self.model(batch_x, batch_x_mark, batch_y, batch_y_mark)\n+                outputs = self.model(batch_x, batch_x_mark, batch_y, batch_y_mark)\n                 if self.args.dp:\n                     torch.cuda.synchronize()\n                 if self.args.nonautoregressive:\n                     batch_y = batch_y[:, -self.args.output_token_len:, :]\n"
                },
                {
                    "date": 1731077239663,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -257,16 +257,16 @@\n                 batch_y = batch_y.float().to(self.device)\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n                 batch_y_mark = batch_y_mark.float().to(self.device)\n                 \n-                # pred_y = []\n-                # for j in range(inference_steps):  \n-                #     if len(pred_y) != 0:\n-                #         batch_x = torch.cat([batch_x[:, self.args.input_token_len:, :], pred_y[-1]], dim=1)\n-                #     outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)\n-                #     pred_y.append(outputs[:, -self.args.output_token_len:, :])\n-                # pred_y = torch.cat(pred_y, dim=1)\n-                pred_y = self.model(batch_x, batch_x_mark, None, batch_y_mark, inference_steps)\n+                pred_y = []\n+                for j in range(inference_steps):  \n+                    if len(pred_y) != 0:\n+                        batch_x = torch.cat([batch_x[:, self.args.input_token_len:, :], pred_y[-1]], dim=1)\n+                    outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)\n+                    pred_y.append(outputs[:, -self.args.output_token_len:, :])\n+                pred_y = torch.cat(pred_y, dim=1)\n+                # pred_y = self.model(batch_x, batch_x_mark, None, batch_y_mark, inference_steps)\n                 if dis != 0:\n                     pred_y = pred_y[:, :-dis, :]\n                 batch_y = batch_y[:, -self.args.test_pred_len:, :].to(self.device)\n                 \n"
                },
                {
                    "date": 1731077644952,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -89,10 +89,10 @@\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n                 batch_y_mark = batch_y_mark.float().to(self.device)\n                 \n                 \n-                # outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark, inference_steps)[:,:self.args.output_token_len,:]\n-                # batch_y = batch_y[:, :, :].to(self.device)\n+                outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark, inference_steps)\n+                batch_y = batch_y[:, :, :].to(self.device)\n                 if is_test or self.args.nonautoregressive:\n                         outputs = outputs[:, -self.args.output_token_len:, :]\n                         batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n                 else:\n"
                },
                {
                    "date": 1731077912311,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -79,9 +79,8 @@\n         inference_steps = self.args.test_pred_len // self.args.input_token_len\n         dis = self.args.test_pred_len - inference_steps * self.args.input_token_len\n         if dis != 0:\n             inference_steps += 1\n-        print(inference_steps)\n         with torch.no_grad():\n             for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(vali_loader):\n                 iter_count += 1\n                 batch_x = batch_x.float().to(self.device)\n@@ -179,21 +178,16 @@\n                     else:\n                         outputs = outputs[:, :, -1]\n                         batch_y = batch_y[:, :, -1]\n                 loss = criterion(outputs, batch_y)\n-                if (i + 1) % 10 == 0:\n+                if (i + 1) % 100 == 0:\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                         print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n                         speed = (time.time() - time_now) / iter_count\n                         left_time = speed * ((self.args.train_epochs - epoch) * train_steps - i)\n                         print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n                         iter_count = 0\n                         time_now = time.time()\n-                    vali_loss = self.vali(vali_data, vali_loader, criterion, is_test=self.args.valid_last)\n-                    test_loss = self.vali(test_data, test_loader, criterion, is_test=True)\n-                    if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n-                        print(\"Epoch: {}, Steps: {} | Vali Loss: {:.7f} Test Loss: {:.7f}\".format(\n-                            epoch + 1, train_steps, vali_loss, test_loss))\n \n                 loss.backward()\n                 model_optim.step()\n \n"
                },
                {
                    "date": 1731107176732,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -239,10 +239,10 @@\n         time_now = time.time()\n         test_steps = len(test_loader)\n         iter_count = 0\n         self.model.eval()\n-        inference_steps = self.args.test_pred_len // self.args.input_token_len\n-        dis = self.args.test_pred_len - inference_steps * self.args.input_token_len\n+        inference_steps = self.args.test_pred_len // self.args.output_token_len\n+        dis = self.args.test_pred_len - inference_steps * self.args.output_token_len\n         if dis != 0:\n             inference_steps += 1\n         with torch.no_grad():\n             for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n"
                },
                {
                    "date": 1731107272266,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -259,11 +259,12 @@\n                     outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)\n                     pred_y.append(outputs[:, -self.args.output_token_len:, :])\n                 pred_y = torch.cat(pred_y, dim=1)\n                 # pred_y = self.model(batch_x, batch_x_mark, None, batch_y_mark, inference_steps)\n-                if dis != 0:\n-                    pred_y = pred_y[:, :-dis, :]\n-                batch_y = batch_y[:, -self.args.test_pred_len:, :].to(self.device)\n+                # if dis != 0:\n+                #     pred_y = pred_y[:, :-dis, :]\n+                pred_y = pred_y[:, :self.args.test_pred_len, :]\n+                batch_y = batch_y[:, :self.args.test_pred_len, :].to(self.device)\n                 \n                 outputs = pred_y.detach().cpu()\n                 batch_y = batch_y.detach().cpu()\n                 pred = outputs\n"
                },
                {
                    "date": 1731107392913,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -259,12 +259,11 @@\n                     outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)\n                     pred_y.append(outputs[:, -self.args.output_token_len:, :])\n                 pred_y = torch.cat(pred_y, dim=1)\n                 # pred_y = self.model(batch_x, batch_x_mark, None, batch_y_mark, inference_steps)\n-                # if dis != 0:\n-                #     pred_y = pred_y[:, :-dis, :]\n-                pred_y = pred_y[:, :self.args.test_pred_len, :]\n-                batch_y = batch_y[:, :self.args.test_pred_len, :].to(self.device)\n+                if dis != 0:\n+                    pred_y = pred_y[:, :-dis, :]\n+                batch_y = batch_y[:, -self.args.test_pred_len:, :].to(self.device)\n                 \n                 outputs = pred_y.detach().cpu()\n                 batch_y = batch_y.detach().cpu()\n                 pred = outputs\n"
                },
                {
                    "date": 1731107925071,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -259,10 +259,11 @@\n                     outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)\n                     pred_y.append(outputs[:, -self.args.output_token_len:, :])\n                 pred_y = torch.cat(pred_y, dim=1)\n                 # pred_y = self.model(batch_x, batch_x_mark, None, batch_y_mark, inference_steps)\n-                if dis != 0:\n-                    pred_y = pred_y[:, :-dis, :]\n+                # if dis != 0:\n+                #     pred_y = pred_y[:, :-dis, :]\n+                pred_y = pred_y[:, :self.args.test_pred_len, :]\n                 batch_y = batch_y[:, -self.args.test_pred_len:, :].to(self.device)\n                 \n                 outputs = pred_y.detach().cpu()\n                 batch_y = batch_y.detach().cpu()\n"
                },
                {
                    "date": 1731110961284,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -145,8 +145,10 @@\n         model_optim = self._select_optimizer()\n         scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(model_optim, T_max=self.args.tmax, eta_min=1e-8)\n         criterion = self._select_criterion()\n         \n+        weights = torch.linspace(1, 0.5, steps=720)\n+        \n         for epoch in range(self.args.train_epochs):\n             iter_count = 0\n             self.model.train()\n             epoch_time = time.time()\n"
                },
                {
                    "date": 1731110973382,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -145,9 +145,9 @@\n         model_optim = self._select_optimizer()\n         scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(model_optim, T_max=self.args.tmax, eta_min=1e-8)\n         criterion = self._select_criterion()\n         \n-        weights = torch.linspace(1, 0.5, steps=720)\n+        weights = torch.linspace(1, 0.5, steps=720)[None,:,None]\n         \n         for epoch in range(self.args.train_epochs):\n             iter_count = 0\n             self.model.train()\n"
                },
                {
                    "date": 1731110988462,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -179,9 +179,9 @@\n                         batch_y = batch_y[:, -self.args.output_token_len:, -1]\n                     else:\n                         outputs = outputs[:, :, -1]\n                         batch_y = batch_y[:, :, -1]\n-                loss = criterion(outputs, batch_y)\n+                loss = (criterion(outputs, batch_y) * weights).mean()\n                 if (i + 1) % 100 == 0:\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                         print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n                         speed = (time.time() - time_now) / iter_count\n"
                },
                {
                    "date": 1731111028989,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -65,9 +65,9 @@\n             print('next learning rate is {}'.format(self.args.learning_rate))\n         return model_optim\n \n     def _select_criterion(self):\n-        criterion = nn.MSELoss()\n+        criterion = nn.MSELoss(reduction='none')\n         return criterion\n \n     def vali(self, vali_data, vali_loader, criterion, is_test=False):\n         total_loss = []\n"
                },
                {
                    "date": 1731111286209,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -145,9 +145,9 @@\n         model_optim = self._select_optimizer()\n         scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(model_optim, T_max=self.args.tmax, eta_min=1e-8)\n         criterion = self._select_criterion()\n         \n-        weights = torch.linspace(1, 0.5, steps=720)[None,:,None]\n+        weights = torch.linspace(1, 0.5, steps=720).repeat(7)[None,:,None]\n         \n         for epoch in range(self.args.train_epochs):\n             iter_count = 0\n             self.model.train()\n"
                },
                {
                    "date": 1731111332456,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -179,9 +179,9 @@\n                         batch_y = batch_y[:, -self.args.output_token_len:, -1]\n                     else:\n                         outputs = outputs[:, :, -1]\n                         batch_y = batch_y[:, :, -1]\n-                loss = (criterion(outputs, batch_y) * weights).mean()\n+                loss = (criterion(outputs, batch_y) * weights.to(self.device)).mean()\n                 if (i + 1) % 100 == 0:\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                         print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n                         speed = (time.time() - time_now) / iter_count\n"
                },
                {
                    "date": 1731111344789,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -145,9 +145,9 @@\n         model_optim = self._select_optimizer()\n         scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(model_optim, T_max=self.args.tmax, eta_min=1e-8)\n         criterion = self._select_criterion()\n         \n-        weights = torch.linspace(1, 0.5, steps=720).repeat(7)[None,:,None]\n+        weights = torch.linspace(1, 0.5, steps=720).repeat(7)[None,:,None].to(self.device)\n         \n         for epoch in range(self.args.train_epochs):\n             iter_count = 0\n             self.model.train()\n@@ -179,9 +179,9 @@\n                         batch_y = batch_y[:, -self.args.output_token_len:, -1]\n                     else:\n                         outputs = outputs[:, :, -1]\n                         batch_y = batch_y[:, :, -1]\n-                loss = (criterion(outputs, batch_y) * weights.to(self.device)).mean()\n+                loss = (criterion(outputs, batch_y) * weights).mean()\n                 if (i + 1) % 100 == 0:\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                         print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n                         speed = (time.time() - time_now) / iter_count\n"
                },
                {
                    "date": 1731111356755,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -65,9 +65,9 @@\n             print('next learning rate is {}'.format(self.args.learning_rate))\n         return model_optim\n \n     def _select_criterion(self):\n-        criterion = nn.MSELoss(reduction='none')\n+        criterion = nn.MSELoss()\n         return criterion\n \n     def vali(self, vali_data, vali_loader, criterion, is_test=False):\n         total_loss = []\n"
                },
                {
                    "date": 1731111368230,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -143,9 +143,9 @@\n         early_stopping = EarlyStopping(self.args, verbose=True)\n         \n         model_optim = self._select_optimizer()\n         scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(model_optim, T_max=self.args.tmax, eta_min=1e-8)\n-        criterion = self._select_criterion()\n+        criterion = nn.MSELoss(reduction='none')\n         \n         weights = torch.linspace(1, 0.5, steps=720).repeat(7)[None,:,None].to(self.device)\n         \n         for epoch in range(self.args.train_epochs):\n"
                },
                {
                    "date": 1731114079670,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -143,9 +143,10 @@\n         early_stopping = EarlyStopping(self.args, verbose=True)\n         \n         model_optim = self._select_optimizer()\n         scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(model_optim, T_max=self.args.tmax, eta_min=1e-8)\n-        criterion = nn.MSELoss(reduction='none')\n+        train_criterion = nn.MSELoss(reduction='none')\n+        criterion = self._select_criterion\n         \n         weights = torch.linspace(1, 0.5, steps=720).repeat(7)[None,:,None].to(self.device)\n         \n         for epoch in range(self.args.train_epochs):\n"
                },
                {
                    "date": 1731114090104,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -144,9 +144,9 @@\n         \n         model_optim = self._select_optimizer()\n         scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(model_optim, T_max=self.args.tmax, eta_min=1e-8)\n         train_criterion = nn.MSELoss(reduction='none')\n-        criterion = self._select_criterion\n+        criterion = self._select_criterion()\n         \n         weights = torch.linspace(1, 0.5, steps=720).repeat(7)[None,:,None].to(self.device)\n         \n         for epoch in range(self.args.train_epochs):\n@@ -180,9 +180,9 @@\n                         batch_y = batch_y[:, -self.args.output_token_len:, -1]\n                     else:\n                         outputs = outputs[:, :, -1]\n                         batch_y = batch_y[:, :, -1]\n-                loss = (criterion(outputs, batch_y) * weights).mean()\n+                loss = (train_criterion(outputs, batch_y) * weights).mean()\n                 if (i + 1) % 100 == 0:\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                         print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n                         speed = (time.time() - time_now) / iter_count\n"
                },
                {
                    "date": 1731200601209,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -143,12 +143,12 @@\n         early_stopping = EarlyStopping(self.args, verbose=True)\n         \n         model_optim = self._select_optimizer()\n         scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(model_optim, T_max=self.args.tmax, eta_min=1e-8)\n-        train_criterion = nn.MSELoss(reduction='none')\n+        # train_criterion = nn.MSELoss(reduction='none')\n         criterion = self._select_criterion()\n         \n-        weights = torch.linspace(1, 0.5, steps=720).repeat(7)[None,:,None].to(self.device)\n+        # weights = torch.linspace(1, 0.5, steps=720).repeat(7)[None,:,None].to(self.device)\n         \n         for epoch in range(self.args.train_epochs):\n             iter_count = 0\n             self.model.train()\n@@ -180,9 +180,9 @@\n                         batch_y = batch_y[:, -self.args.output_token_len:, -1]\n                     else:\n                         outputs = outputs[:, :, -1]\n                         batch_y = batch_y[:, :, -1]\n-                loss = (train_criterion(outputs, batch_y) * weights).mean()\n+                # loss = (train_criterion(outputs, batch_y) * weights).mean()\n                 if (i + 1) % 100 == 0:\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                         print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n                         speed = (time.time() - time_now) / iter_count\n"
                },
                {
                    "date": 1731200707071,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -180,8 +180,13 @@\n                         batch_y = batch_y[:, -self.args.output_token_len:, -1]\n                     else:\n                         outputs = outputs[:, :, -1]\n                         batch_y = batch_y[:, :, -1]\n+                loss = 0\n+                B,O,C = batch_y.shape\n+                batch_y.reshape()\n+                for output in outputs:\n+                    loss = criterion(output, batch_y)\n                 # loss = (train_criterion(outputs, batch_y) * weights).mean()\n                 if (i + 1) % 100 == 0:\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                         print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n"
                },
                {
                    "date": 1731200768874,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -182,11 +182,11 @@\n                         outputs = outputs[:, :, -1]\n                         batch_y = batch_y[:, :, -1]\n                 loss = 0\n                 B,O,C = batch_y.shape\n-                batch_y.reshape()\n+                batch_y = batch_y.reshape(B,7,O//7,C)\n                 for output in outputs:\n-                    loss = criterion(output, batch_y)\n+                    loss = criterion(output, batch_y[:,:,:output.shape[1],:].reshape(B,-1,C))\n                 # loss = (train_criterion(outputs, batch_y) * weights).mean()\n                 if (i + 1) % 100 == 0:\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                         print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n"
                },
                {
                    "date": 1731200967898,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -69,18 +69,14 @@\n         criterion = nn.MSELoss()\n         return criterion\n \n     def vali(self, vali_data, vali_loader, criterion, is_test=False):\n-        total_loss = []\n+        total_loss = [[]]\n         total_count = []\n         time_now = time.time()\n         test_steps = len(vali_loader)\n         iter_count = 0\n         self.model.eval()\n-        inference_steps = self.args.test_pred_len // self.args.input_token_len\n-        dis = self.args.test_pred_len - inference_steps * self.args.input_token_len\n-        if dis != 0:\n-            inference_steps += 1\n         with torch.no_grad():\n             for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(vali_loader):\n                 iter_count += 1\n                 batch_x = batch_x.float().to(self.device)\n@@ -88,9 +84,9 @@\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n                 batch_y_mark = batch_y_mark.float().to(self.device)\n                 \n                 \n-                outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark, inference_steps)\n+                outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)\n                 batch_y = batch_y[:, :, :].to(self.device)\n                 if is_test or self.args.nonautoregressive:\n                         outputs = outputs[:, -self.args.output_token_len:, :]\n                         batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n@@ -104,9 +100,14 @@\n                         batch_y = batch_y[:, -self.args.output_token_len:, -1]\n                     else:\n                         outputs = outputs[:, :, -1]\n                         batch_y = batch_y[:, :, -1]\n-                loss = criterion(outputs, batch_y)\n+                B,O,C = batch_y.shape\n+                batch_y = batch_y.reshape(B,7,O//7,C)\n+                for output,tl in zip(outputs,total_loss):\n+                    loss = criterion(output, batch_y[:,:,:output.shape[1],:].reshape(B,-1,C))\n+                    tl.append(loss)\n+                # loss = criterion(outputs, batch_y)\n \n                 loss = loss.detach().cpu()\n                 total_loss.append(loss)\n                 total_count.append(batch_x.shape[0])\n@@ -184,9 +185,9 @@\n                 loss = 0\n                 B,O,C = batch_y.shape\n                 batch_y = batch_y.reshape(B,7,O//7,C)\n                 for output in outputs:\n-                    loss = criterion(output, batch_y[:,:,:output.shape[1],:].reshape(B,-1,C))\n+                    loss += criterion(output, batch_y[:,:,:output.shape[1],:].reshape(B,-1,C))\n                 # loss = (train_criterion(outputs, batch_y) * weights).mean()\n                 if (i + 1) % 100 == 0:\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                         print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n"
                },
                {
                    "date": 1731200977635,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -103,9 +103,9 @@\n                         batch_y = batch_y[:, :, -1]\n                 B,O,C = batch_y.shape\n                 batch_y = batch_y.reshape(B,7,O//7,C)\n                 for output,tl in zip(outputs,total_loss):\n-                    loss = criterion(output, batch_y[:,:,:output.shape[1],:].reshape(B,-1,C))\n+                    loss = criterion(output, batch_y[:,:,:output.shape[1],:].reshape(B,-1,C)).detach().cpu()\n                     tl.append(loss)\n                 # loss = criterion(outputs, batch_y)\n \n                 loss = loss.detach().cpu()\n"
                },
                {
                    "date": 1731201029019,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -107,10 +107,10 @@\n                     loss = criterion(output, batch_y[:,:,:output.shape[1],:].reshape(B,-1,C)).detach().cpu()\n                     tl.append(loss)\n                 # loss = criterion(outputs, batch_y)\n \n-                loss = loss.detach().cpu()\n-                total_loss.append(loss)\n+                # loss = loss.detach().cpu()\n+                # total_loss.append(loss)\n                 total_count.append(batch_x.shape[0])\n                 if (i + 1) % 100 == 0:\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                         speed = (time.time() - time_now) / iter_count\n@@ -123,9 +123,10 @@\n             dist.barrier()\n             dist.reduce(total_loss, dst=0, op=dist.ReduceOp.SUM)\n             total_loss = total_loss.item() / dist.get_world_size()\n         else:\n-            total_loss = np.average(total_loss, weights=total_count)\n+            # total_loss = np.average(total_loss, weights=total_count)\n+            total_loss = [np.average(tl, weights=total_count) for tl in total_loss]\n         self.model.train()\n         return total_loss\n \n     def train(self, setting):\n"
                },
                {
                    "date": 1731201152572,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -186,9 +186,9 @@\n                 loss = 0\n                 B,O,C = batch_y.shape\n                 batch_y = batch_y.reshape(B,7,O//7,C)\n                 for output in outputs:\n-                    loss += criterion(output, batch_y[:,:,:output.shape[1],:].reshape(B,-1,C))\n+                    loss += criterion(output, batch_y[:,:,:output.shape[1]//7,:].reshape(B,-1,C))\n                 # loss = (train_criterion(outputs, batch_y) * weights).mean()\n                 if (i + 1) % 100 == 0:\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                         print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n"
                },
                {
                    "date": 1731201288791,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -206,9 +206,9 @@\n \n             vali_loss = self.vali(vali_data, vali_loader, criterion, is_test=self.args.valid_last)\n             test_loss = self.vali(test_data, test_loader, criterion, is_test=True)\n             if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n-                print(\"Epoch: {}, Steps: {} | Vali Loss: {:.7f} Test Loss: {:.7f}\".format(\n+                print(\"Epoch: {}, Steps: {} | Vali Loss: {} Test Loss: {}\".format(\n                     epoch + 1, train_steps, vali_loss, test_loss))\n             early_stopping(vali_loss, self.model, path)\n             if early_stopping.early_stop:\n                 if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n"
                },
                {
                    "date": 1731203643504,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -101,9 +101,9 @@\n                     else:\n                         outputs = outputs[:, :, -1]\n                         batch_y = batch_y[:, :, -1]\n                 B,O,C = batch_y.shape\n-                batch_y = batch_y.reshape(B,7,O//7,C)\n+                batch_y = batch_y.reshape(B,7,O//7,C).to(self.device)\n                 for output,tl in zip(outputs,total_loss):\n                     loss = criterion(output, batch_y[:,:,:output.shape[1],:].reshape(B,-1,C)).detach().cpu()\n                     tl.append(loss)\n                 # loss = criterion(outputs, batch_y)\n"
                },
                {
                    "date": 1731203690035,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -101,9 +101,9 @@\n                     else:\n                         outputs = outputs[:, :, -1]\n                         batch_y = batch_y[:, :, -1]\n                 B,O,C = batch_y.shape\n-                batch_y = batch_y.reshape(B,7,O//7,C).to(self.device)\n+                batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n                 for output,tl in zip(outputs,total_loss):\n                     loss = criterion(output, batch_y[:,:,:output.shape[1],:].reshape(B,-1,C)).detach().cpu()\n                     tl.append(loss)\n                 # loss = criterion(outputs, batch_y)\n"
                },
                {
                    "date": 1731203759316,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -86,26 +86,25 @@\n                 \n                 \n                 outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)\n                 batch_y = batch_y[:, :, :].to(self.device)\n-                if is_test or self.args.nonautoregressive:\n-                        outputs = outputs[:, -self.args.output_token_len:, :]\n-                        batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n-                else:\n-                    outputs = outputs[:, :, :]\n-                    batch_y = batch_y[:, :, :].to(self.device)\n+                # if is_test or self.args.nonautoregressive:\n+                #         outputs = outputs[:, -self.args.output_token_len:, :]\n+                #         batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n+                # else:\n+                #     outputs = outputs[:, :, :]\n+                #     batch_y = batch_y[:, :, :].to(self.device)\n \n                 if self.args.covariate:\n                     if self.args.last_token:\n                         outputs = outputs[:, -self.args.output_token_len:, -1]\n                         batch_y = batch_y[:, -self.args.output_token_len:, -1]\n                     else:\n                         outputs = outputs[:, :, -1]\n                         batch_y = batch_y[:, :, -1]\n-                B,O,C = batch_y.shape\n                 batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n-                for output,tl in zip(outputs,total_loss):\n-                    loss = criterion(output, batch_y[:,:,:output.shape[1],:].reshape(B,-1,C)).detach().cpu()\n+                for output,tl in zip([96,192,336,720]outputs,total_loss):\n+                    loss = criterion(outputs[:, -self.args.output_token_len:, :], batch_y[:,:,:output.shape[1],:]).detach().cpu()\n                     tl.append(loss)\n                 # loss = criterion(outputs, batch_y)\n \n                 # loss = loss.detach().cpu()\n"
                },
                {
                    "date": 1731203766535,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -101,9 +101,9 @@\n                     else:\n                         outputs = outputs[:, :, -1]\n                         batch_y = batch_y[:, :, -1]\n                 batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n-                for output,tl in zip([96,192,336,720]outputs,total_loss):\n+                for length,output,tl in zip([96,192,336,720],outputs,total_loss):\n                     loss = criterion(outputs[:, -self.args.output_token_len:, :], batch_y[:,:,:output.shape[1],:]).detach().cpu()\n                     tl.append(loss)\n                 # loss = criterion(outputs, batch_y)\n \n"
                },
                {
                    "date": 1731203779533,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -102,9 +102,9 @@\n                         outputs = outputs[:, :, -1]\n                         batch_y = batch_y[:, :, -1]\n                 batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n                 for length,output,tl in zip([96,192,336,720],outputs,total_loss):\n-                    loss = criterion(outputs[:, -self.args.output_token_len:, :], batch_y[:,:,:output.shape[1],:]).detach().cpu()\n+                    loss = criterion(outputs[:, -length:, :], batch_y[:,:length,:]).detach().cpu()\n                     tl.append(loss)\n                 # loss = criterion(outputs, batch_y)\n \n                 # loss = loss.detach().cpu()\n"
                },
                {
                    "date": 1731203790119,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -102,9 +102,9 @@\n                         outputs = outputs[:, :, -1]\n                         batch_y = batch_y[:, :, -1]\n                 batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n                 for length,output,tl in zip([96,192,336,720],outputs,total_loss):\n-                    loss = criterion(outputs[:, -length:, :], batch_y[:,:length,:]).detach().cpu()\n+                    loss = criterion(output[:, -length:, :], batch_y[:,:length,:]).detach().cpu()\n                     tl.append(loss)\n                 # loss = criterion(outputs, batch_y)\n \n                 # loss = loss.detach().cpu()\n"
                },
                {
                    "date": 1731203839983,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -188,8 +188,9 @@\n                 for output in outputs:\n                     loss += criterion(output, batch_y[:,:,:output.shape[1]//7,:].reshape(B,-1,C))\n                 # loss = (train_criterion(outputs, batch_y) * weights).mean()\n                 if (i + 1) % 100 == 0:\n+                    vali_loss = self.vali(vali_data, vali_loader, criterion, is_test=self.args.valid_last)\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                         print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n                         speed = (time.time() - time_now) / iter_count\n                         left_time = speed * ((self.args.train_epochs - epoch) * train_steps - i)\n"
                },
                {
                    "date": 1731204039193,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -187,9 +187,9 @@\n                 batch_y = batch_y.reshape(B,7,O//7,C)\n                 for output in outputs:\n                     loss += criterion(output, batch_y[:,:,:output.shape[1]//7,:].reshape(B,-1,C))\n                 # loss = (train_criterion(outputs, batch_y) * weights).mean()\n-                if (i + 1) % 100 == 0:\n+                if (i + 1) % 10 == 0:\n                     vali_loss = self.vali(vali_data, vali_loader, criterion, is_test=self.args.valid_last)\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                         print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n                         speed = (time.time() - time_now) / iter_count\n@@ -208,9 +208,9 @@\n             test_loss = self.vali(test_data, test_loader, criterion, is_test=True)\n             if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                 print(\"Epoch: {}, Steps: {} | Vali Loss: {} Test Loss: {}\".format(\n                     epoch + 1, train_steps, vali_loss, test_loss))\n-            early_stopping(vali_loss, self.model, path)\n+            early_stopping(vali_loss.sum(), self.model, path)\n             if early_stopping.early_stop:\n                 if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                     print(\"Early stopping\")\n                 break\n"
                },
                {
                    "date": 1731204075839,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -188,9 +188,8 @@\n                 for output in outputs:\n                     loss += criterion(output, batch_y[:,:,:output.shape[1]//7,:].reshape(B,-1,C))\n                 # loss = (train_criterion(outputs, batch_y) * weights).mean()\n                 if (i + 1) % 10 == 0:\n-                    vali_loss = self.vali(vali_data, vali_loader, criterion, is_test=self.args.valid_last)\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                         print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n                         speed = (time.time() - time_now) / iter_count\n                         left_time = speed * ((self.args.train_epochs - epoch) * train_steps - i)\n"
                },
                {
                    "date": 1731207470577,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -69,9 +69,9 @@\n         criterion = nn.MSELoss()\n         return criterion\n \n     def vali(self, vali_data, vali_loader, criterion, is_test=False):\n-        total_loss = [[]]\n+        total_loss = [[] for _ in range(4)]\n         total_count = []\n         time_now = time.time()\n         test_steps = len(vali_loader)\n         iter_count = 0\n@@ -187,9 +187,9 @@\n                 batch_y = batch_y.reshape(B,7,O//7,C)\n                 for output in outputs:\n                     loss += criterion(output, batch_y[:,:,:output.shape[1]//7,:].reshape(B,-1,C))\n                 # loss = (train_criterion(outputs, batch_y) * weights).mean()\n-                if (i + 1) % 10 == 0:\n+                if (i + 1) % 100 == 0:\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                         print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n                         speed = (time.time() - time_now) / iter_count\n                         left_time = speed * ((self.args.train_epochs - epoch) * train_steps - i)\n"
                },
                {
                    "date": 1731207485280,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -207,9 +207,9 @@\n             test_loss = self.vali(test_data, test_loader, criterion, is_test=True)\n             if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                 print(\"Epoch: {}, Steps: {} | Vali Loss: {} Test Loss: {}\".format(\n                     epoch + 1, train_steps, vali_loss, test_loss))\n-            early_stopping(vali_loss.sum(), self.model, path)\n+            early_stopping(sum(vali_loss), self.model, path)\n             if early_stopping.early_stop:\n                 if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                     print(\"Early stopping\")\n                 break\n"
                },
                {
                    "date": 1731211844878,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -102,8 +102,9 @@\n                         outputs = outputs[:, :, -1]\n                         batch_y = batch_y[:, :, -1]\n                 batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n                 for length,output,tl in zip([96,192,336,720],outputs,total_loss):\n+                    print(output.shape, batch_y.shape)\n                     loss = criterion(output[:, -length:, :], batch_y[:,:length,:]).detach().cpu()\n                     tl.append(loss)\n                 # loss = criterion(outputs, batch_y)\n \n"
                },
                {
                    "date": 1731211863022,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -188,9 +188,9 @@\n                 batch_y = batch_y.reshape(B,7,O//7,C)\n                 for output in outputs:\n                     loss += criterion(output, batch_y[:,:,:output.shape[1]//7,:].reshape(B,-1,C))\n                 # loss = (train_criterion(outputs, batch_y) * weights).mean()\n-                if (i + 1) % 100 == 0:\n+                if (i + 1) % 10 == 0:\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                         print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n                         speed = (time.time() - time_now) / iter_count\n                         left_time = speed * ((self.args.train_epochs - epoch) * train_steps - i)\n"
                },
                {
                    "date": 1731211872597,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -196,8 +196,9 @@\n                         left_time = speed * ((self.args.train_epochs - epoch) * train_steps - i)\n                         print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n                         iter_count = 0\n                         time_now = time.time()\n+                    vali_loss = self.vali(vali_data, vali_loader, criterion, is_test=self.args.valid_last)\n \n                 loss.backward()\n                 model_optim.step()\n \n"
                },
                {
                    "date": 1731212039911,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -102,9 +102,8 @@\n                         outputs = outputs[:, :, -1]\n                         batch_y = batch_y[:, :, -1]\n                 batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n                 for length,output,tl in zip([96,192,336,720],outputs,total_loss):\n-                    print(output.shape, batch_y.shape)\n                     loss = criterion(output[:, -length:, :], batch_y[:,:length,:]).detach().cpu()\n                     tl.append(loss)\n                 # loss = criterion(outputs, batch_y)\n \n@@ -188,17 +187,16 @@\n                 batch_y = batch_y.reshape(B,7,O//7,C)\n                 for output in outputs:\n                     loss += criterion(output, batch_y[:,:,:output.shape[1]//7,:].reshape(B,-1,C))\n                 # loss = (train_criterion(outputs, batch_y) * weights).mean()\n-                if (i + 1) % 10 == 0:\n+                if (i + 1) % 100 == 0:\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                         print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n                         speed = (time.time() - time_now) / iter_count\n                         left_time = speed * ((self.args.train_epochs - epoch) * train_steps - i)\n                         print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n                         iter_count = 0\n                         time_now = time.time()\n-                    vali_loss = self.vali(vali_data, vali_loader, criterion, is_test=self.args.valid_last)\n \n                 loss.backward()\n                 model_optim.step()\n \n"
                },
                {
                    "date": 1731590675582,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -148,9 +148,8 @@\n         # train_criterion = nn.MSELoss(reduction='none')\n         criterion = self._select_criterion()\n         \n         # weights = torch.linspace(1, 0.5, steps=720).repeat(7)[None,:,None].to(self.device)\n-        \n         for epoch in range(self.args.train_epochs):\n             iter_count = 0\n             self.model.train()\n             epoch_time = time.time()\n@@ -168,8 +167,12 @@\n                 \n                 # indices = torch.randperm(batch_x.size(-1)).to(batch_x.device)\n                 # batch_x = batch_x.index_select(-1, indices)\n                 # batch_y = batch_y.index_select(-1, indices)\n+                # gnum = 2\n+                # B,S,M = batch_x.shape\n+                # batch_x = batch_x[...,:M//gnum*gnum].reshape(B,-1,gnum,M//gnum).permute(0,2,1,3).reshape(B*gnum,-1,M//gnum)\n+                # batch_y = batch_y[...,:M//gnum*gnum].reshape(B,-1,gnum,M//gnum).permute(0,2,1,3).reshape(B*gnum,-1,M//gnum)\n \n                 outputs = self.model(batch_x, batch_x_mark, batch_y, batch_y_mark)\n                 if self.args.dp:\n                     torch.cuda.synchronize()\n@@ -239,9 +242,9 @@\n             setting = self.args.test_dir\n             best_model_path = self.args.test_file_name\n             print(\"loading model from {}\".format(os.path.join(self.args.checkpoints, setting, best_model_path)))\n             self.model.load_state_dict(torch.load(os.path.join(self.args.checkpoints, setting, best_model_path)))\n-        preds = []\n+        preds = [[] for _ in range(4)]\n         trues = []\n         folder_path = './test_results/' + setting + '/'\n         if not os.path.exists(folder_path):\n             os.makedirs(folder_path)\n@@ -260,27 +263,31 @@\n                 batch_y = batch_y.float().to(self.device)\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n                 batch_y_mark = batch_y_mark.float().to(self.device)\n                 \n-                pred_y = []\n-                for j in range(inference_steps):  \n-                    if len(pred_y) != 0:\n-                        batch_x = torch.cat([batch_x[:, self.args.input_token_len:, :], pred_y[-1]], dim=1)\n-                    outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)\n-                    pred_y.append(outputs[:, -self.args.output_token_len:, :])\n-                pred_y = torch.cat(pred_y, dim=1)\n+                # pred_y = []\n+                # for j in range(inference_steps):  \n+                #     if len(pred_y) != 0:\n+                #         batch_x = torch.cat([batch_x[:, self.args.input_token_len:, :], pred_y[-1]], dim=1)\n+                #     outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)\n+                #     pred_y.append(outputs[:, -self.args.output_token_len:, :])\n+                # pred_y = torch.cat(pred_y, dim=1)\n                 # pred_y = self.model(batch_x, batch_x_mark, None, batch_y_mark, inference_steps)\n                 # if dis != 0:\n                 #     pred_y = pred_y[:, :-dis, :]\n-                pred_y = pred_y[:, :self.args.test_pred_len, :]\n+                outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)\n+                batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n+                for length,output,pred in zip([96,192,336,720],outputs,preds):\n+                    pred.append(output[:, -length:, :].detach().cpu())\n+                # pred_y = pred_y[:, :self.args.test_pred_len, :]\n                 batch_y = batch_y[:, -self.args.test_pred_len:, :].to(self.device)\n                 \n-                outputs = pred_y.detach().cpu()\n+                # outputs = pred_y.detach().cpu()\n                 batch_y = batch_y.detach().cpu()\n-                pred = outputs\n+                # pred = outputs\n                 true = batch_y\n \n-                preds.append(pred)\n+                # preds.append(pred)\n                 trues.append(true)\n                 if (i + 1) % 100 == 0:\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                         speed = (time.time() - time_now) / iter_count\n@@ -295,20 +302,24 @@\n                     gt = np.array(true[0, :, -1])\n                     pd = np.array(pred[0, :, -1])\n                     visual(gt, pd, os.path.join(dir_path, f'{i}.pdf'))\n \n-        preds = torch.cat(preds, dim=0).numpy()\n+        preds = [torch.cat(pred, dim=0).numpy() for pred in preds]\n         trues = torch.cat(trues, dim=0).numpy()\n-        print('preds shape:', preds.shape)\n+        # print('preds shape:', preds.shape)\n         print('trues shape:', trues.shape)\n         if self.args.covariate:\n             preds = preds[:, :, -1]\n             trues = trues[:, :, -1]\n-        mae, mse, rmse, mape, mspe = metric(preds, trues)\n-        print('mse:{}, mae:{}'.format(mse, mae))\n+        maes, mses = [], []\n+        for pred in preds:\n+            mae, mse, rmse, mape, mspe = metric(pred, trues[:,:pred.shape[1],:])\n+            maes.append(mae)\n+            mses.append(mse)\n+        print('mse:{}, mae:{}'.format(mses, maes))\n         f = open(\"result_long_term_forecast.txt\", 'a')\n         f.write(setting + \"  \\n\")\n-        f.write('mse:{}, mae:{}'.format(mse, mae))\n+        f.write('mse:{}, mae:{}'.format(mses, maes))\n         f.write('\\n')\n         f.write('\\n')\n         f.close()\n         return\n"
                },
                {
                    "date": 1731590693169,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -164,15 +164,15 @@\n                 # lookback = random.randint(1,min(7,epoch+4))*self.args.input_token_len\n                 # batch_x = batch_x[:,-lookback:,:]\n                 # batch_y = batch_y[:,-lookback:,:]\n                 \n-                # indices = torch.randperm(batch_x.size(-1)).to(batch_x.device)\n-                # batch_x = batch_x.index_select(-1, indices)\n-                # batch_y = batch_y.index_select(-1, indices)\n-                # gnum = 2\n-                # B,S,M = batch_x.shape\n-                # batch_x = batch_x[...,:M//gnum*gnum].reshape(B,-1,gnum,M//gnum).permute(0,2,1,3).reshape(B*gnum,-1,M//gnum)\n-                # batch_y = batch_y[...,:M//gnum*gnum].reshape(B,-1,gnum,M//gnum).permute(0,2,1,3).reshape(B*gnum,-1,M//gnum)\n+                indices = torch.randperm(batch_x.size(-1)).to(batch_x.device)\n+                batch_x = batch_x.index_select(-1, indices)\n+                batch_y = batch_y.index_select(-1, indices)\n+                gnum = 2\n+                B,S,M = batch_x.shape\n+                batch_x = batch_x[...,:M//gnum*gnum].reshape(B,-1,gnum,M//gnum).permute(0,2,1,3).reshape(B*gnum,-1,M//gnum)\n+                batch_y = batch_y[...,:M//gnum*gnum].reshape(B,-1,gnum,M//gnum).permute(0,2,1,3).reshape(B*gnum,-1,M//gnum)\n \n                 outputs = self.model(batch_x, batch_x_mark, batch_y, batch_y_mark)\n                 if self.args.dp:\n                     torch.cuda.synchronize()\n"
                },
                {
                    "date": 1731590707700,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -164,13 +164,13 @@\n                 # lookback = random.randint(1,min(7,epoch+4))*self.args.input_token_len\n                 # batch_x = batch_x[:,-lookback:,:]\n                 # batch_y = batch_y[:,-lookback:,:]\n                 \n-                indices = torch.randperm(batch_x.size(-1)).to(batch_x.device)\n+                B,S,M = batch_x.shape\n+                indices = torch.randperm(M).to(batch_x.device)\n                 batch_x = batch_x.index_select(-1, indices)\n                 batch_y = batch_y.index_select(-1, indices)\n                 gnum = 2\n-                B,S,M = batch_x.shape\n                 batch_x = batch_x[...,:M//gnum*gnum].reshape(B,-1,gnum,M//gnum).permute(0,2,1,3).reshape(B*gnum,-1,M//gnum)\n                 batch_y = batch_y[...,:M//gnum*gnum].reshape(B,-1,gnum,M//gnum).permute(0,2,1,3).reshape(B*gnum,-1,M//gnum)\n \n                 outputs = self.model(batch_x, batch_x_mark, batch_y, batch_y_mark)\n"
                },
                {
                    "date": 1731590738887,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -169,10 +169,11 @@\n                 indices = torch.randperm(M).to(batch_x.device)\n                 batch_x = batch_x.index_select(-1, indices)\n                 batch_y = batch_y.index_select(-1, indices)\n                 gnum = 2\n-                batch_x = batch_x[...,:M//gnum*gnum].reshape(B,-1,gnum,M//gnum).permute(0,2,1,3).reshape(B*gnum,-1,M//gnum)\n-                batch_y = batch_y[...,:M//gnum*gnum].reshape(B,-1,gnum,M//gnum).permute(0,2,1,3).reshape(B*gnum,-1,M//gnum)\n+                gsize = M//gnum\n+                batch_x = batch_x[...,:gsize*gnum].reshape(B,-1,gnum,gsize).permute(0,2,1,3).reshape(B*gnum,-1,gsize)\n+                batch_y = batch_y[...,:gsize*gnum].reshape(B,-1,gnum,gsize).permute(0,2,1,3).reshape(B*gnum,-1,gsize)\n \n                 outputs = self.model(batch_x, batch_x_mark, batch_y, batch_y_mark)\n                 if self.args.dp:\n                     torch.cuda.synchronize()\n"
                },
                {
                    "date": 1731639411527,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -164,16 +164,16 @@\n                 # lookback = random.randint(1,min(7,epoch+4))*self.args.input_token_len\n                 # batch_x = batch_x[:,-lookback:,:]\n                 # batch_y = batch_y[:,-lookback:,:]\n                 \n-                B,S,M = batch_x.shape\n-                indices = torch.randperm(M).to(batch_x.device)\n-                batch_x = batch_x.index_select(-1, indices)\n-                batch_y = batch_y.index_select(-1, indices)\n-                gnum = 2\n-                gsize = M//gnum\n-                batch_x = batch_x[...,:gsize*gnum].reshape(B,-1,gnum,gsize).permute(0,2,1,3).reshape(B*gnum,-1,gsize)\n-                batch_y = batch_y[...,:gsize*gnum].reshape(B,-1,gnum,gsize).permute(0,2,1,3).reshape(B*gnum,-1,gsize)\n+                # B,S,M = batch_x.shape\n+                # indices = torch.randperm(M).to(batch_x.device)\n+                # batch_x = batch_x.index_select(-1, indices)\n+                # batch_y = batch_y.index_select(-1, indices)\n+                # gnum = 2\n+                # gsize = M//gnum\n+                # batch_x = batch_x[...,:gsize*gnum].reshape(B,-1,gnum,gsize).permute(0,2,1,3).reshape(B*gnum,-1,gsize)\n+                # batch_y = batch_y[...,:gsize*gnum].reshape(B,-1,gnum,gsize).permute(0,2,1,3).reshape(B*gnum,-1,gsize)\n \n                 outputs = self.model(batch_x, batch_x_mark, batch_y, batch_y_mark)\n                 if self.args.dp:\n                     torch.cuda.synchronize()\n"
                },
                {
                    "date": 1731678987808,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -152,15 +152,16 @@\n         for epoch in range(self.args.train_epochs):\n             iter_count = 0\n             self.model.train()\n             epoch_time = time.time()\n-            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n+            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark, lengths) in enumerate(train_loader):\n                 iter_count += 1\n                 model_optim.zero_grad()\n                 batch_x = batch_x.float().to(self.device)\n                 batch_y = batch_y.float().to(self.device)\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n                 batch_y_mark = batch_y_mark.float().to(self.device)\n+                lengths = lengths.long().to(self.device)\n                 \n                 # lookback = random.randint(1,min(7,epoch+4))*self.args.input_token_len\n                 # batch_x = batch_x[:,-lookback:,:]\n                 # batch_y = batch_y[:,-lookback:,:]\n@@ -173,9 +174,9 @@\n                 # gsize = M//gnum\n                 # batch_x = batch_x[...,:gsize*gnum].reshape(B,-1,gnum,gsize).permute(0,2,1,3).reshape(B*gnum,-1,gsize)\n                 # batch_y = batch_y[...,:gsize*gnum].reshape(B,-1,gnum,gsize).permute(0,2,1,3).reshape(B*gnum,-1,gsize)\n \n-                outputs = self.model(batch_x, batch_x_mark, batch_y, batch_y_mark)\n+                outputs = self.model(batch_x, batch_x_mark, batch_y, batch_y_mark, lengths)\n                 if self.args.dp:\n                     torch.cuda.synchronize()\n                 if self.args.nonautoregressive:\n                     batch_y = batch_y[:, -self.args.output_token_len:, :]\n"
                },
                {
                    "date": 1731727843615,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -144,10 +144,10 @@\n         early_stopping = EarlyStopping(self.args, verbose=True)\n         \n         model_optim = self._select_optimizer()\n         scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(model_optim, T_max=self.args.tmax, eta_min=1e-8)\n-        # train_criterion = nn.MSELoss(reduction='none')\n-        criterion = self._select_criterion()\n+        train_criterion = nn.MSELoss(reduction='none')\n+        # criterion = self._select_criterion()\n         \n         # weights = torch.linspace(1, 0.5, steps=720).repeat(7)[None,:,None].to(self.device)\n         for epoch in range(self.args.train_epochs):\n             iter_count = 0\n@@ -190,9 +190,9 @@\n                 loss = 0\n                 B,O,C = batch_y.shape\n                 batch_y = batch_y.reshape(B,7,O//7,C)\n                 for output in outputs:\n-                    loss += criterion(output, batch_y[:,:,:output.shape[1]//7,:].reshape(B,-1,C))\n+                    loss += train_criterion(output, batch_y[:,:,:output.shape[1]//7,:].reshape(B,-1,C))\n                 # loss = (train_criterion(outputs, batch_y) * weights).mean()\n                 if (i + 1) % 100 == 0:\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                         print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n"
                },
                {
                    "date": 1731727886035,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -173,9 +173,9 @@\n                 # gnum = 2\n                 # gsize = M//gnum\n                 # batch_x = batch_x[...,:gsize*gnum].reshape(B,-1,gnum,gsize).permute(0,2,1,3).reshape(B*gnum,-1,gsize)\n                 # batch_y = batch_y[...,:gsize*gnum].reshape(B,-1,gnum,gsize).permute(0,2,1,3).reshape(B*gnum,-1,gsize)\n-\n+                B,O,C = batch_y.shape\n                 outputs = self.model(batch_x, batch_x_mark, batch_y, batch_y_mark, lengths)\n                 if self.args.dp:\n                     torch.cuda.synchronize()\n                 if self.args.nonautoregressive:\n@@ -187,9 +187,9 @@\n                     else:\n                         outputs = outputs[:, :, -1]\n                         batch_y = batch_y[:, :, -1]\n                 loss = 0\n-                B,O,C = batch_y.shape\n+                \n                 batch_y = batch_y.reshape(B,7,O//7,C)\n                 for output in outputs:\n                     loss += train_criterion(output, batch_y[:,:,:output.shape[1]//7,:].reshape(B,-1,C))\n                 # loss = (train_criterion(outputs, batch_y) * weights).mean()\n"
                },
                {
                    "date": 1731728193262,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -174,8 +174,9 @@\n                 # gsize = M//gnum\n                 # batch_x = batch_x[...,:gsize*gnum].reshape(B,-1,gnum,gsize).permute(0,2,1,3).reshape(B*gnum,-1,gsize)\n                 # batch_y = batch_y[...,:gsize*gnum].reshape(B,-1,gnum,gsize).permute(0,2,1,3).reshape(B*gnum,-1,gsize)\n                 B,O,C = batch_y.shape\n+                mask = torch.arange(C).expand(B, C) < lengths.unsqueeze(1)\n                 outputs = self.model(batch_x, batch_x_mark, batch_y, batch_y_mark, lengths)\n                 if self.args.dp:\n                     torch.cuda.synchronize()\n                 if self.args.nonautoregressive:\n"
                },
                {
                    "date": 1731728213890,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -191,9 +191,9 @@\n                 loss = 0\n                 \n                 batch_y = batch_y.reshape(B,7,O//7,C)\n                 for output in outputs:\n-                    loss += train_criterion(output, batch_y[:,:,:output.shape[1]//7,:].reshape(B,-1,C))\n+                    loss += (train_criterion(output, batch_y[:,:,:output.shape[1]//7,:].reshape(B,-1,C))*mask).mean()\n                 # loss = (train_criterion(outputs, batch_y) * weights).mean()\n                 if (i + 1) % 100 == 0:\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                         print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n"
                },
                {
                    "date": 1731728241928,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -174,9 +174,9 @@\n                 # gsize = M//gnum\n                 # batch_x = batch_x[...,:gsize*gnum].reshape(B,-1,gnum,gsize).permute(0,2,1,3).reshape(B*gnum,-1,gsize)\n                 # batch_y = batch_y[...,:gsize*gnum].reshape(B,-1,gnum,gsize).permute(0,2,1,3).reshape(B*gnum,-1,gsize)\n                 B,O,C = batch_y.shape\n-                mask = torch.arange(C).expand(B, C) < lengths.unsqueeze(1)\n+                mask = torch.arange(C).expand(B, C) < lengths.unsqueeze(1).to(self.device)\n                 outputs = self.model(batch_x, batch_x_mark, batch_y, batch_y_mark, lengths)\n                 if self.args.dp:\n                     torch.cuda.synchronize()\n                 if self.args.nonautoregressive:\n"
                },
                {
                    "date": 1731728455602,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -174,9 +174,9 @@\n                 # gsize = M//gnum\n                 # batch_x = batch_x[...,:gsize*gnum].reshape(B,-1,gnum,gsize).permute(0,2,1,3).reshape(B*gnum,-1,gsize)\n                 # batch_y = batch_y[...,:gsize*gnum].reshape(B,-1,gnum,gsize).permute(0,2,1,3).reshape(B*gnum,-1,gsize)\n                 B,O,C = batch_y.shape\n-                mask = torch.arange(C).expand(B, C) < lengths.unsqueeze(1).to(self.device)\n+                mask = torch.arange(C).expand(B, C).to(self.device) < lengths.unsqueeze(1)\n                 outputs = self.model(batch_x, batch_x_mark, batch_y, batch_y_mark, lengths)\n                 if self.args.dp:\n                     torch.cuda.synchronize()\n                 if self.args.nonautoregressive:\n"
                },
                {
                    "date": 1731728710995,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -191,9 +191,9 @@\n                 loss = 0\n                 \n                 batch_y = batch_y.reshape(B,7,O//7,C)\n                 for output in outputs:\n-                    loss += (train_criterion(output, batch_y[:,:,:output.shape[1]//7,:].reshape(B,-1,C))*mask).mean()\n+                    loss += (train_criterion(output, batch_y[:,:,:output.shape[1]//7,:].reshape(B,-1,C))*mask.unsqueeze(1)).mean()\n                 # loss = (train_criterion(outputs, batch_y) * weights).mean()\n                 if (i + 1) % 100 == 0:\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                         print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n"
                },
                {
                    "date": 1731749446115,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -145,9 +145,9 @@\n         \n         model_optim = self._select_optimizer()\n         scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(model_optim, T_max=self.args.tmax, eta_min=1e-8)\n         train_criterion = nn.MSELoss(reduction='none')\n-        # criterion = self._select_criterion()\n+        criterion = self._select_criterion()\n         \n         # weights = torch.linspace(1, 0.5, steps=720).repeat(7)[None,:,None].to(self.device)\n         for epoch in range(self.args.train_epochs):\n             iter_count = 0\n"
                },
                {
                    "date": 1731749599656,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -76,15 +76,17 @@\n         test_steps = len(vali_loader)\n         iter_count = 0\n         self.model.eval()\n         with torch.no_grad():\n-            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(vali_loader):\n+            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark, mask) in enumerate(vali_loader):\n                 iter_count += 1\n                 batch_x = batch_x.float().to(self.device)\n                 batch_y = batch_y.float()\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n                 batch_y_mark = batch_y_mark.float().to(self.device)\n                 \n+                B,O,C = batch_y.shape\n+                mask = torch.arange(C).expand(B, C).to(self.device) < lengths.unsqueeze(1)\n                 \n                 outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)\n                 batch_y = batch_y[:, :, :].to(self.device)\n                 # if is_test or self.args.nonautoregressive:\n@@ -208,10 +210,10 @@\n \n             if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                 print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n \n-            vali_loss = self.vali(vali_data, vali_loader, criterion, is_test=self.args.valid_last)\n-            test_loss = self.vali(test_data, test_loader, criterion, is_test=True)\n+            vali_loss = self.vali(vali_data, vali_loader, train_criterion, is_test=self.args.valid_last)\n+            test_loss = self.vali(test_data, test_loader, train_criterion, is_test=True)\n             if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                 print(\"Epoch: {}, Steps: {} | Vali Loss: {} Test Loss: {}\".format(\n                     epoch + 1, train_steps, vali_loss, test_loss))\n             early_stopping(sum(vali_loss), self.model, path)\n"
                },
                {
                    "date": 1731749627492,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -76,9 +76,9 @@\n         test_steps = len(vali_loader)\n         iter_count = 0\n         self.model.eval()\n         with torch.no_grad():\n-            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark, mask) in enumerate(vali_loader):\n+            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark, lengths) in enumerate(vali_loader):\n                 iter_count += 1\n                 batch_x = batch_x.float().to(self.device)\n                 batch_y = batch_y.float()\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n@@ -86,9 +86,9 @@\n                 \n                 B,O,C = batch_y.shape\n                 mask = torch.arange(C).expand(B, C).to(self.device) < lengths.unsqueeze(1)\n                 \n-                outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)\n+                outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark, mask)\n                 batch_y = batch_y[:, :, :].to(self.device)\n                 # if is_test or self.args.nonautoregressive:\n                 #         outputs = outputs[:, -self.args.output_token_len:, :]\n                 #         batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n"
                },
                {
                    "date": 1731749668995,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -104,9 +104,9 @@\n                         outputs = outputs[:, :, -1]\n                         batch_y = batch_y[:, :, -1]\n                 batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n                 for length,output,tl in zip([96,192,336,720],outputs,total_loss):\n-                    loss = criterion(output[:, -length:, :], batch_y[:,:length,:]).detach().cpu()\n+                    loss = (criterion(output[:, -length:, :], batch_y[:,:length,:])*mask.unsqueeze(1)).mean().detach().cpu()\n                     tl.append(loss)\n                 # loss = criterion(outputs, batch_y)\n \n                 # loss = loss.detach().cpu()\n"
                },
                {
                    "date": 1731749711153,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -203,8 +203,9 @@\n                         left_time = speed * ((self.args.train_epochs - epoch) * train_steps - i)\n                         print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n                         iter_count = 0\n                         time_now = time.time()\n+                vali_loss = self.vali(vali_data, vali_loader, train_criterion, is_test=self.args.valid_last)\n \n                 loss.backward()\n                 model_optim.step()\n \n"
                },
                {
                    "date": 1731749999404,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -84,9 +84,9 @@\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n                 batch_y_mark = batch_y_mark.float().to(self.device)\n                 \n                 B,O,C = batch_y.shape\n-                mask = torch.arange(C).expand(B, C).to(self.device) < lengths.unsqueeze(1)\n+                mask = torch.arange(C).expand(B, C).to(self.device) < lengths.unsqueeze(1).to(self.device)\n                 \n                 outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark, mask)\n                 batch_y = batch_y[:, :, :].to(self.device)\n                 # if is_test or self.args.nonautoregressive:\n"
                },
                {
                    "date": 1731751034476,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -203,9 +203,8 @@\n                         left_time = speed * ((self.args.train_epochs - epoch) * train_steps - i)\n                         print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n                         iter_count = 0\n                         time_now = time.time()\n-                vali_loss = self.vali(vali_data, vali_loader, train_criterion, is_test=self.args.valid_last)\n \n                 loss.backward()\n                 model_optim.step()\n \n"
                },
                {
                    "date": 1731816689008,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -76,19 +76,19 @@\n         test_steps = len(vali_loader)\n         iter_count = 0\n         self.model.eval()\n         with torch.no_grad():\n-            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark, lengths) in enumerate(vali_loader):\n+            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(vali_loader):\n                 iter_count += 1\n                 batch_x = batch_x.float().to(self.device)\n                 batch_y = batch_y.float()\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n                 batch_y_mark = batch_y_mark.float().to(self.device)\n                 \n                 B,O,C = batch_y.shape\n-                mask = torch.arange(C).expand(B, C).to(self.device) < lengths.unsqueeze(1).to(self.device)\n+                # mask = torch.arange(C).expand(B, C).to(self.device) < lengths.unsqueeze(1).to(self.device)\n                 \n-                outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark, mask)\n+                outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)\n                 batch_y = batch_y[:, :, :].to(self.device)\n                 # if is_test or self.args.nonautoregressive:\n                 #         outputs = outputs[:, -self.args.output_token_len:, :]\n                 #         batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n@@ -104,9 +104,9 @@\n                         outputs = outputs[:, :, -1]\n                         batch_y = batch_y[:, :, -1]\n                 batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n                 for length,output,tl in zip([96,192,336,720],outputs,total_loss):\n-                    loss = (criterion(output[:, -length:, :], batch_y[:,:length,:])*mask.unsqueeze(1)).mean().detach().cpu()\n+                    loss = (criterion(output[:, -length:, :], batch_y[:,:length,:])).mean().detach().cpu()\n                     tl.append(loss)\n                 # loss = criterion(outputs, batch_y)\n \n                 # loss = loss.detach().cpu()\n@@ -154,9 +154,9 @@\n         for epoch in range(self.args.train_epochs):\n             iter_count = 0\n             self.model.train()\n             epoch_time = time.time()\n-            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark, lengths) in enumerate(train_loader):\n+            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n                 iter_count += 1\n                 model_optim.zero_grad()\n                 batch_x = batch_x.float().to(self.device)\n                 batch_y = batch_y.float().to(self.device)\n@@ -176,10 +176,10 @@\n                 # gsize = M//gnum\n                 # batch_x = batch_x[...,:gsize*gnum].reshape(B,-1,gnum,gsize).permute(0,2,1,3).reshape(B*gnum,-1,gsize)\n                 # batch_y = batch_y[...,:gsize*gnum].reshape(B,-1,gnum,gsize).permute(0,2,1,3).reshape(B*gnum,-1,gsize)\n                 B,O,C = batch_y.shape\n-                mask = torch.arange(C).expand(B, C).to(self.device) < lengths.unsqueeze(1)\n-                outputs = self.model(batch_x, batch_x_mark, batch_y, batch_y_mark, lengths)\n+                # mask = torch.arange(C).expand(B, C).to(self.device) < lengths.unsqueeze(1)\n+                outputs = self.model(batch_x, batch_x_mark, batch_y, batch_y_mark)\n                 if self.args.dp:\n                     torch.cuda.synchronize()\n                 if self.args.nonautoregressive:\n                     batch_y = batch_y[:, -self.args.output_token_len:, :]\n@@ -193,9 +193,9 @@\n                 loss = 0\n                 \n                 batch_y = batch_y.reshape(B,7,O//7,C)\n                 for output in outputs:\n-                    loss += (train_criterion(output, batch_y[:,:,:output.shape[1]//7,:].reshape(B,-1,C))*mask.unsqueeze(1)).mean()\n+                    loss += (train_criterion(output, batch_y[:,:,:output.shape[1]//7,:].reshape(B,-1,C))).mean()\n                 # loss = (train_criterion(outputs, batch_y) * weights).mean()\n                 if (i + 1) % 100 == 0:\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                         print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n"
                },
                {
                    "date": 1731816745726,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -167,16 +167,16 @@\n                 # lookback = random.randint(1,min(7,epoch+4))*self.args.input_token_len\n                 # batch_x = batch_x[:,-lookback:,:]\n                 # batch_y = batch_y[:,-lookback:,:]\n                 \n-                # B,S,M = batch_x.shape\n-                # indices = torch.randperm(M).to(batch_x.device)\n-                # batch_x = batch_x.index_select(-1, indices)\n-                # batch_y = batch_y.index_select(-1, indices)\n-                # gnum = 2\n-                # gsize = M//gnum\n-                # batch_x = batch_x[...,:gsize*gnum].reshape(B,-1,gnum,gsize).permute(0,2,1,3).reshape(B*gnum,-1,gsize)\n-                # batch_y = batch_y[...,:gsize*gnum].reshape(B,-1,gnum,gsize).permute(0,2,1,3).reshape(B*gnum,-1,gsize)\n+                B,S,M = batch_x.shape\n+                indices = torch.randperm(M).to(batch_x.device)\n+                batch_x = batch_x.index_select(-1, indices)\n+                batch_y = batch_y.index_select(-1, indices)\n+                gnum = 12\n+                gsize = M//gnum\n+                batch_x = batch_x[...,:gsize*gnum].reshape(B,-1,gnum,gsize).permute(0,2,1,3).reshape(B*gnum,-1,gsize)\n+                batch_y = batch_y[...,:gsize*gnum].reshape(B,-1,gnum,gsize).permute(0,2,1,3).reshape(B*gnum,-1,gsize)\n                 B,O,C = batch_y.shape\n                 # mask = torch.arange(C).expand(B, C).to(self.device) < lengths.unsqueeze(1)\n                 outputs = self.model(batch_x, batch_x_mark, batch_y, batch_y_mark)\n                 if self.args.dp:\n"
                },
                {
                    "date": 1731830395120,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -161,9 +161,9 @@\n                 batch_x = batch_x.float().to(self.device)\n                 batch_y = batch_y.float().to(self.device)\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n                 batch_y_mark = batch_y_mark.float().to(self.device)\n-                lengths = lengths.long().to(self.device)\n+                # lengths = lengths.long().to(self.device)\n                 \n                 # lookback = random.randint(1,min(7,epoch+4))*self.args.input_token_len\n                 # batch_x = batch_x[:,-lookback:,:]\n                 # batch_y = batch_y[:,-lookback:,:]\n"
                },
                {
                    "date": 1731831218062,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -154,8 +154,9 @@\n         for epoch in range(self.args.train_epochs):\n             iter_count = 0\n             self.model.train()\n             epoch_time = time.time()\n+            train_loader.dataset.__shuffle_data__()\n             for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n                 iter_count += 1\n                 model_optim.zero_grad()\n                 batch_x = batch_x.float().to(self.device)\n"
                },
                {
                    "date": 1731831242216,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -168,17 +168,17 @@\n                 # lookback = random.randint(1,min(7,epoch+4))*self.args.input_token_len\n                 # batch_x = batch_x[:,-lookback:,:]\n                 # batch_y = batch_y[:,-lookback:,:]\n                 \n-                B,S,M = batch_x.shape\n-                indices = torch.randperm(M).to(batch_x.device)\n-                batch_x = batch_x.index_select(-1, indices)\n-                batch_y = batch_y.index_select(-1, indices)\n-                gnum = 12\n-                gsize = M//gnum\n-                batch_x = batch_x[...,:gsize*gnum].reshape(B,-1,gnum,gsize).permute(0,2,1,3).reshape(B*gnum,-1,gsize)\n-                batch_y = batch_y[...,:gsize*gnum].reshape(B,-1,gnum,gsize).permute(0,2,1,3).reshape(B*gnum,-1,gsize)\n-                B,O,C = batch_y.shape\n+                # B,S,M = batch_x.shape\n+                # indices = torch.randperm(M).to(batch_x.device)\n+                # batch_x = batch_x.index_select(-1, indices)\n+                # batch_y = batch_y.index_select(-1, indices)\n+                # gnum = 12\n+                # gsize = M//gnum\n+                # batch_x = batch_x[...,:gsize*gnum].reshape(B,-1,gnum,gsize).permute(0,2,1,3).reshape(B*gnum,-1,gsize)\n+                # batch_y = batch_y[...,:gsize*gnum].reshape(B,-1,gnum,gsize).permute(0,2,1,3).reshape(B*gnum,-1,gsize)\n+                # B,O,C = batch_y.shape\n                 # mask = torch.arange(C).expand(B, C).to(self.device) < lengths.unsqueeze(1)\n                 outputs = self.model(batch_x, batch_x_mark, batch_y, batch_y_mark)\n                 if self.args.dp:\n                     torch.cuda.synchronize()\n"
                },
                {
                    "date": 1731833707100,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -176,9 +176,9 @@\n                 # gnum = 12\n                 # gsize = M//gnum\n                 # batch_x = batch_x[...,:gsize*gnum].reshape(B,-1,gnum,gsize).permute(0,2,1,3).reshape(B*gnum,-1,gsize)\n                 # batch_y = batch_y[...,:gsize*gnum].reshape(B,-1,gnum,gsize).permute(0,2,1,3).reshape(B*gnum,-1,gsize)\n-                # B,O,C = batch_y.shape\n+                B,O,C = batch_y.shape\n                 # mask = torch.arange(C).expand(B, C).to(self.device) < lengths.unsqueeze(1)\n                 outputs = self.model(batch_x, batch_x_mark, batch_y, batch_y_mark)\n                 if self.args.dp:\n                     torch.cuda.synchronize()\n@@ -192,11 +192,11 @@\n                         outputs = outputs[:, :, -1]\n                         batch_y = batch_y[:, :, -1]\n                 loss = 0\n                 \n-                batch_y = batch_y.reshape(B,7,O//7,C)\n+                batch_y = batch_y.reshape(B,32,O//32,C)\n                 for output in outputs:\n-                    loss += (train_criterion(output, batch_y[:,:,:output.shape[1]//7,:].reshape(B,-1,C))).mean()\n+                    loss += (train_criterion(output, batch_y[:,:,:output.shape[1]//32,:].reshape(B,-1,C))).mean()\n                 # loss = (train_criterion(outputs, batch_y) * weights).mean()\n                 if (i + 1) % 100 == 0:\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                         print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n"
                },
                {
                    "date": 1731894578411,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -207,30 +207,31 @@\n                         time_now = time.time()\n \n                 loss.backward()\n                 model_optim.step()\n+                \n+                if (i + 1) % 10000 == 0:\n+                    if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n+                        print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n \n-            if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n-                print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n-\n-            vali_loss = self.vali(vali_data, vali_loader, train_criterion, is_test=self.args.valid_last)\n-            test_loss = self.vali(test_data, test_loader, train_criterion, is_test=True)\n-            if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n-                print(\"Epoch: {}, Steps: {} | Vali Loss: {} Test Loss: {}\".format(\n-                    epoch + 1, train_steps, vali_loss, test_loss))\n-            early_stopping(sum(vali_loss), self.model, path)\n-            if early_stopping.early_stop:\n-                if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n-                    print(\"Early stopping\")\n-                break\n-            if self.args.cosine:\n-                scheduler.step()\n-                if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n-                    print(\"lr = {:.10f}\".format(model_optim.param_groups[0]['lr']))\n-            else:\n-                adjust_learning_rate(model_optim, epoch + 1, self.args)\n-            if self.args.ddp:\n-                train_loader.sampler.set_epoch(epoch + 1)\n+                    vali_loss = self.vali(vali_data, vali_loader, train_criterion, is_test=self.args.valid_last)\n+                    test_loss = self.vali(test_data, test_loader, train_criterion, is_test=True)\n+                    if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n+                        print(\"Epoch: {}, Steps: {} | Vali Loss: {} Test Loss: {}\".format(\n+                            epoch + 1, train_steps, vali_loss, test_loss))\n+                    early_stopping(sum(vali_loss), self.model, path)\n+                    if early_stopping.early_stop:\n+                        if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n+                            print(\"Early stopping\")\n+                        break\n+                    if self.args.cosine:\n+                        scheduler.step()\n+                        if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n+                            print(\"lr = {:.10f}\".format(model_optim.param_groups[0]['lr']))\n+                    else:\n+                        adjust_learning_rate(model_optim, epoch + 1, self.args)\n+                    if self.args.ddp:\n+                        train_loader.sampler.set_epoch(epoch + 1)\n                 \n         best_model_path = path + '/' + 'checkpoint.pth'\n         if self.args.ddp:\n             dist.barrier()\n"
                },
                {
                    "date": 1731894599436,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -208,9 +208,9 @@\n \n                 loss.backward()\n                 model_optim.step()\n                 \n-                if (i + 1) % 10000 == 0:\n+                if (i + 1) % 5000 == 0:\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                         print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n \n                     vali_loss = self.vali(vali_data, vali_loader, train_criterion, is_test=self.args.valid_last)\n"
                },
                {
                    "date": 1731894625280,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -208,9 +208,9 @@\n \n                 loss.backward()\n                 model_optim.step()\n                 \n-                if (i + 1) % 5000 == 0:\n+                if (i + 1) % 50 == 0:\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                         print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n \n                     vali_loss = self.vali(vali_data, vali_loader, train_criterion, is_test=self.args.valid_last)\n"
                },
                {
                    "date": 1731894755162,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -192,11 +192,11 @@\n                         outputs = outputs[:, :, -1]\n                         batch_y = batch_y[:, :, -1]\n                 loss = 0\n                 \n-                batch_y = batch_y.reshape(B,32,O//32,C)\n+                batch_y = batch_y.reshape(B,7,O//7,C)\n                 for output in outputs:\n-                    loss += (train_criterion(output, batch_y[:,:,:output.shape[1]//32,:].reshape(B,-1,C))).mean()\n+                    loss += (train_criterion(output, batch_y[:,:,:output.shape[1]//7,:].reshape(B,-1,C))).mean()\n                 # loss = (train_criterion(outputs, batch_y) * weights).mean()\n                 if (i + 1) % 100 == 0:\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                         print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n"
                },
                {
                    "date": 1731897927530,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -15,8 +15,24 @@\n from torch.nn import DataParallel\n warnings.filterwarnings('ignore')\n \n \n+def smape(y_true, y_pred):\n+    \"\"\"\n+    Computes the symmetric Mean Absolute Percentage Error (sMAPE).\n+    \n+    Parameters:\n+        y_true (torch.Tensor): Ground truth values.\n+        y_pred (torch.Tensor): Predicted values.\n+        \n+    Returns:\n+        torch.Tensor: sMAPE value.\n+    \"\"\"\n+    numerator = torch.abs(y_true - y_pred)\n+    denominator = (torch.abs(y_true) + torch.abs(y_pred)) / 2\n+    smape = torch.mean(numerator / (denominator + 1e-8)) * 100  # Add epsilon to avoid division by zero\n+    return smape\n+\n class Exp_Forecast(Exp_Basic):\n     def __init__(self, args):\n         super(Exp_Forecast, self).__init__(args)\n         \n"
                },
                {
                    "date": 1731898056861,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -121,8 +121,9 @@\n                         batch_y = batch_y[:, :, -1]\n                 batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n                 for length,output,tl in zip([96,192,336,720],outputs,total_loss):\n                     loss = (criterion(output[:, -length:, :], batch_y[:,:length,:])).mean().detach().cpu()\n+                    loss = smape(batch_y[:,:length,:], output[:, -length:, :])\n                     tl.append(loss)\n                 # loss = criterion(outputs, batch_y)\n \n                 # loss = loss.detach().cpu()\n"
                },
                {
                    "date": 1731898121038,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -120,10 +120,10 @@\n                         outputs = outputs[:, :, -1]\n                         batch_y = batch_y[:, :, -1]\n                 batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n                 for length,output,tl in zip([96,192,336,720],outputs,total_loss):\n-                    loss = (criterion(output[:, -length:, :], batch_y[:,:length,:])).mean().detach().cpu()\n-                    loss = smape(batch_y[:,:length,:], output[:, -length:, :])\n+                    # loss = (criterion(output[:, -length:, :], batch_y[:,:length,:])).mean().detach().cpu()\n+                    loss = smape(batch_y[:,:length,:], output[:, -length:, :]).mean().detach().cpu()\n                     tl.append(loss)\n                 # loss = criterion(outputs, batch_y)\n \n                 # loss = loss.detach().cpu()\n"
                },
                {
                    "date": 1731898441510,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -209,11 +209,12 @@\n                         outputs = outputs[:, :, -1]\n                         batch_y = batch_y[:, :, -1]\n                 loss = 0\n                 \n-                batch_y = batch_y.reshape(B,7,O//7,C)\n+                patch_num = 16\n+                batch_y = batch_y.reshape(B,patch_num,-1,C)\n                 for output in outputs:\n-                    loss += (train_criterion(output, batch_y[:,:,:output.shape[1]//7,:].reshape(B,-1,C))).mean()\n+                    loss += (train_criterion(output, batch_y[:,:,:output.shape[1]//patch_num,:].reshape(B,-1,C))).mean()\n                 # loss = (train_criterion(outputs, batch_y) * weights).mean()\n                 if (i + 1) % 100 == 0:\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                         print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n"
                },
                {
                    "date": 1731898458851,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -226,9 +226,9 @@\n \n                 loss.backward()\n                 model_optim.step()\n                 \n-                if (i + 1) % 50 == 0:\n+                if (i + 1) % 5000 == 0:\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                         print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n \n                     vali_loss = self.vali(vali_data, vali_loader, train_criterion, is_test=self.args.valid_last)\n"
                },
                {
                    "date": 1731910091942,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -209,9 +209,9 @@\n                         outputs = outputs[:, :, -1]\n                         batch_y = batch_y[:, :, -1]\n                 loss = 0\n                 \n-                patch_num = 16\n+                patch_num = 32\n                 batch_y = batch_y.reshape(B,patch_num,-1,C)\n                 for output in outputs:\n                     loss += (train_criterion(output, batch_y[:,:,:output.shape[1]//patch_num,:].reshape(B,-1,C))).mean()\n                 # loss = (train_criterion(outputs, batch_y) * weights).mean()\n"
                },
                {
                    "date": 1731910152386,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -119,9 +119,9 @@\n                     else:\n                         outputs = outputs[:, :, -1]\n                         batch_y = batch_y[:, :, -1]\n                 batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n-                for length,output,tl in zip([96,192,336,720],outputs,total_loss):\n+                for length,output,tl in zip([96],outputs,total_loss):\n                     # loss = (criterion(output[:, -length:, :], batch_y[:,:length,:])).mean().detach().cpu()\n                     loss = smape(batch_y[:,:length,:], output[:, -length:, :]).mean().detach().cpu()\n                     tl.append(loss)\n                 # loss = criterion(outputs, batch_y)\n@@ -300,9 +300,9 @@\n                 # if dis != 0:\n                 #     pred_y = pred_y[:, :-dis, :]\n                 outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)\n                 batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n-                for length,output,pred in zip([96,192,336,720],outputs,preds):\n+                for length,output,pred in zip([96],outputs,preds):\n                     pred.append(output[:, -length:, :].detach().cpu())\n                 # pred_y = pred_y[:, :self.args.test_pred_len, :]\n                 batch_y = batch_y[:, -self.args.test_pred_len:, :].to(self.device)\n                 \n"
                },
                {
                    "date": 1731920541522,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -121,9 +121,9 @@\n                         batch_y = batch_y[:, :, -1]\n                 batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n                 for length,output,tl in zip([96],outputs,total_loss):\n                     # loss = (criterion(output[:, -length:, :], batch_y[:,:length,:])).mean().detach().cpu()\n-                    loss = smape(batch_y[:,:length,:], output[:, -length:, :]).mean().detach().cpu()\n+                    loss = smape(batch_y[:,:length,:], output[:, -length:, :]).detach().cpu()\n                     tl.append(loss)\n                 # loss = criterion(outputs, batch_y)\n \n                 # loss = loss.detach().cpu()\n"
                },
                {
                    "date": 1731920557574,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -94,8 +94,10 @@\n         self.model.eval()\n         with torch.no_grad():\n             for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(vali_loader):\n                 iter_count += 1\n+                if i>200:\n+                    break\n                 batch_x = batch_x.float().to(self.device)\n                 batch_y = batch_y.float()\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n                 batch_y_mark = batch_y_mark.float().to(self.device)\n"
                },
                {
                    "date": 1731920564723,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -228,9 +228,9 @@\n \n                 loss.backward()\n                 model_optim.step()\n                 \n-                if (i + 1) % 5000 == 0:\n+                if (i + 1) % 50 == 0:\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                         print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n \n                     vali_loss = self.vali(vali_data, vali_loader, train_criterion, is_test=self.args.valid_last)\n"
                },
                {
                    "date": 1731920696621,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -125,8 +125,9 @@\n                 for length,output,tl in zip([96],outputs,total_loss):\n                     # loss = (criterion(output[:, -length:, :], batch_y[:,:length,:])).mean().detach().cpu()\n                     loss = smape(batch_y[:,:length,:], output[:, -length:, :]).detach().cpu()\n                     tl.append(loss)\n+                    print(tl)\n                 # loss = criterion(outputs, batch_y)\n \n                 # loss = loss.detach().cpu()\n                 # total_loss.append(loss)\n"
                },
                {
                    "date": 1731920710871,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -94,9 +94,9 @@\n         self.model.eval()\n         with torch.no_grad():\n             for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(vali_loader):\n                 iter_count += 1\n-                if i>200:\n+                if i>100:\n                     break\n                 batch_x = batch_x.float().to(self.device)\n                 batch_y = batch_y.float()\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n@@ -131,8 +131,9 @@\n \n                 # loss = loss.detach().cpu()\n                 # total_loss.append(loss)\n                 total_count.append(batch_x.shape[0])\n+                print(total_count)\n                 if (i + 1) % 100 == 0:\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                         speed = (time.time() - time_now) / iter_count\n                         left_time = speed * (test_steps - i)\n"
                },
                {
                    "date": 1731920858151,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -125,9 +125,9 @@\n                 for length,output,tl in zip([96],outputs,total_loss):\n                     # loss = (criterion(output[:, -length:, :], batch_y[:,:length,:])).mean().detach().cpu()\n                     loss = smape(batch_y[:,:length,:], output[:, -length:, :]).detach().cpu()\n                     tl.append(loss)\n-                    print(tl)\n+                    print(len(tl))\n                 # loss = criterion(outputs, batch_y)\n \n                 # loss = loss.detach().cpu()\n                 # total_loss.append(loss)\n"
                },
                {
                    "date": 1731920864342,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -131,9 +131,9 @@\n \n                 # loss = loss.detach().cpu()\n                 # total_loss.append(loss)\n                 total_count.append(batch_x.shape[0])\n-                print(total_count)\n+                print(len(total_count))\n                 if (i + 1) % 100 == 0:\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                         speed = (time.time() - time_now) / iter_count\n                         left_time = speed * (test_steps - i)\n"
                },
                {
                    "date": 1731921022202,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -123,9 +123,9 @@\n                         batch_y = batch_y[:, :, -1]\n                 batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n                 for length,output,tl in zip([96],outputs,total_loss):\n                     # loss = (criterion(output[:, -length:, :], batch_y[:,:length,:])).mean().detach().cpu()\n-                    loss = smape(batch_y[:,:length,:], output[:, -length:, :]).detach().cpu()\n+                    loss = smape(batch_y[:,:length,:], output[:, -length:, :]).detach().cpu().numpy()\n                     tl.append(loss)\n                     print(len(tl))\n                 # loss = criterion(outputs, batch_y)\n \n"
                },
                {
                    "date": 1731921036059,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -123,9 +123,9 @@\n                         batch_y = batch_y[:, :, -1]\n                 batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n                 for length,output,tl in zip([96],outputs,total_loss):\n                     # loss = (criterion(output[:, -length:, :], batch_y[:,:length,:])).mean().detach().cpu()\n-                    loss = smape(batch_y[:,:length,:], output[:, -length:, :]).detach().cpu().numpy()\n+                    loss = smape(batch_y[:,:length,:], output[:, -length:, :]).item()\n                     tl.append(loss)\n                     print(len(tl))\n                 # loss = criterion(outputs, batch_y)\n \n"
                },
                {
                    "date": 1731921189645,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -122,12 +122,12 @@\n                         outputs = outputs[:, :, -1]\n                         batch_y = batch_y[:, :, -1]\n                 batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n                 for length,output,tl in zip([96],outputs,total_loss):\n-                    # loss = (criterion(output[:, -length:, :], batch_y[:,:length,:])).mean().detach().cpu()\n+                    loss1 = (criterion(output[:, -length:, :], batch_y[:,:length,:])).mean().detach().cpu()\n                     loss = smape(batch_y[:,:length,:], output[:, -length:, :]).item()\n                     tl.append(loss)\n-                    print(len(tl))\n+                    print(loss1,len(tl))\n                 # loss = criterion(outputs, batch_y)\n \n                 # loss = loss.detach().cpu()\n                 # total_loss.append(loss)\n"
                },
                {
                    "date": 1731921288780,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -122,12 +122,12 @@\n                         outputs = outputs[:, :, -1]\n                         batch_y = batch_y[:, :, -1]\n                 batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n                 for length,output,tl in zip([96],outputs,total_loss):\n-                    loss1 = (criterion(output[:, -length:, :], batch_y[:,:length,:])).mean().detach().cpu()\n-                    loss = smape(batch_y[:,:length,:], output[:, -length:, :]).item()\n+                    # loss = (criterion(output[:, -length:, :], batch_y[:,:length,:])).mean().detach().cpu()\n+                    loss = smape(batch_y[:,:length,:], output[:, -length:, :]).detach().cpu()\n                     tl.append(loss)\n-                    print(loss1,len(tl))\n+                    print(len(tl))\n                 # loss = criterion(outputs, batch_y)\n \n                 # loss = loss.detach().cpu()\n                 # total_loss.append(loss)\n"
                },
                {
                    "date": 1731921576282,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -146,9 +146,9 @@\n             dist.reduce(total_loss, dst=0, op=dist.ReduceOp.SUM)\n             total_loss = total_loss.item() / dist.get_world_size()\n         else:\n             # total_loss = np.average(total_loss, weights=total_count)\n-            total_loss = [np.average(tl, weights=total_count) for tl in total_loss]\n+            total_loss = [np.average(tl, weights=total_count, axis=0) for tl in total_loss]\n         self.model.train()\n         return total_loss\n \n     def train(self, setting):\n"
                },
                {
                    "date": 1731924686782,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -146,8 +146,9 @@\n             dist.reduce(total_loss, dst=0, op=dist.ReduceOp.SUM)\n             total_loss = total_loss.item() / dist.get_world_size()\n         else:\n             # total_loss = np.average(total_loss, weights=total_count)\n+            print(len(total_loss[0]),len(total_count))\n             total_loss = [np.average(tl, weights=total_count, axis=0) for tl in total_loss]\n         self.model.train()\n         return total_loss\n \n"
                },
                {
                    "date": 1731925762940,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -123,9 +123,9 @@\n                         batch_y = batch_y[:, :, -1]\n                 batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n                 for length,output,tl in zip([96],outputs,total_loss):\n                     # loss = (criterion(output[:, -length:, :], batch_y[:,:length,:])).mean().detach().cpu()\n-                    loss = smape(batch_y[:,:length,:], output[:, -length:, :]).detach().cpu()\n+                    loss = smape(batch_y[:,:length,:], output[:, -length:, :]).item()\n                     tl.append(loss)\n                     print(len(tl))\n                 # loss = criterion(outputs, batch_y)\n \n"
                },
                {
                    "date": 1731925955928,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -146,10 +146,10 @@\n             dist.reduce(total_loss, dst=0, op=dist.ReduceOp.SUM)\n             total_loss = total_loss.item() / dist.get_world_size()\n         else:\n             # total_loss = np.average(total_loss, weights=total_count)\n-            print(len(total_loss[0]),len(total_count))\n-            total_loss = [np.average(tl, weights=total_count, axis=0) for tl in total_loss]\n+            print(np.array(total_loss[0]).shape,np.array(total_count).shape)\n+            total_loss = [np.average(np.array(tl), weights=np.array(total_count), axis=0) for tl in total_loss]\n         self.model.train()\n         return total_loss\n \n     def train(self, setting):\n"
                },
                {
                    "date": 1731926338247,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -85,9 +85,9 @@\n         criterion = nn.MSELoss()\n         return criterion\n \n     def vali(self, vali_data, vali_loader, criterion, is_test=False):\n-        total_loss = [[] for _ in range(4)]\n+        total_loss = [[] for _ in range(1)]\n         total_count = []\n         time_now = time.time()\n         test_steps = len(vali_loader)\n         iter_count = 0\n@@ -125,9 +125,8 @@\n                 for length,output,tl in zip([96],outputs,total_loss):\n                     # loss = (criterion(output[:, -length:, :], batch_y[:,:length,:])).mean().detach().cpu()\n                     loss = smape(batch_y[:,:length,:], output[:, -length:, :]).item()\n                     tl.append(loss)\n-                    print(len(tl))\n                 # loss = criterion(outputs, batch_y)\n \n                 # loss = loss.detach().cpu()\n                 # total_loss.append(loss)\n@@ -146,9 +145,8 @@\n             dist.reduce(total_loss, dst=0, op=dist.ReduceOp.SUM)\n             total_loss = total_loss.item() / dist.get_world_size()\n         else:\n             # total_loss = np.average(total_loss, weights=total_count)\n-            print(np.array(total_loss[0]).shape,np.array(total_count).shape)\n             total_loss = [np.average(np.array(tl), weights=np.array(total_count), axis=0) for tl in total_loss]\n         self.model.train()\n         return total_loss\n \n"
                },
                {
                    "date": 1731926351341,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -145,9 +145,9 @@\n             dist.reduce(total_loss, dst=0, op=dist.ReduceOp.SUM)\n             total_loss = total_loss.item() / dist.get_world_size()\n         else:\n             # total_loss = np.average(total_loss, weights=total_count)\n-            total_loss = [np.average(np.array(tl), weights=np.array(total_count), axis=0) for tl in total_loss]\n+            total_loss = [np.average(tl, weights=tl, axis=0) for tl in total_loss]\n         self.model.train()\n         return total_loss\n \n     def train(self, setting):\n"
                },
                {
                    "date": 1731926362818,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -145,9 +145,9 @@\n             dist.reduce(total_loss, dst=0, op=dist.ReduceOp.SUM)\n             total_loss = total_loss.item() / dist.get_world_size()\n         else:\n             # total_loss = np.average(total_loss, weights=total_count)\n-            total_loss = [np.average(tl, weights=tl, axis=0) for tl in total_loss]\n+            total_loss = [np.average(tl, weights=total_count, axis=0) for tl in total_loss]\n         self.model.train()\n         return total_loss\n \n     def train(self, setting):\n"
                },
                {
                    "date": 1731926370908,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -130,9 +130,8 @@\n \n                 # loss = loss.detach().cpu()\n                 # total_loss.append(loss)\n                 total_count.append(batch_x.shape[0])\n-                print(len(total_count))\n                 if (i + 1) % 100 == 0:\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                         speed = (time.time() - time_now) / iter_count\n                         left_time = speed * (test_steps - i)\n"
                },
                {
                    "date": 1731926473279,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -94,10 +94,8 @@\n         self.model.eval()\n         with torch.no_grad():\n             for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(vali_loader):\n                 iter_count += 1\n-                if i>100:\n-                    break\n                 batch_x = batch_x.float().to(self.device)\n                 batch_y = batch_y.float()\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n                 batch_y_mark = batch_y_mark.float().to(self.device)\n"
                },
                {
                    "date": 1731926480508,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -226,9 +226,9 @@\n \n                 loss.backward()\n                 model_optim.step()\n                 \n-                if (i + 1) % 50 == 0:\n+                if (i + 1) % 5000 == 0:\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                         print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n \n                     vali_loss = self.vali(vali_data, vali_loader, train_criterion, is_test=self.args.valid_last)\n"
                },
                {
                    "date": 1732008555585,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -258,9 +258,9 @@\n             self.model.load_state_dict(torch.load(best_model_path), strict=False)\n         return self.model\n \n     def test(self, setting, test=0):\n-        test_data, test_loader = self._get_data(flag='test')\n+        test_data, test_loader = self._get_data(flag='T')\n \n         print(\"info:\", self.args.test_seq_len, self.args.input_token_len, self.args.output_token_len, self.args.test_pred_len)\n         if test:\n             print('loading model')\n"
                },
                {
                    "date": 1732015716198,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -267,9 +267,9 @@\n             setting = self.args.test_dir\n             best_model_path = self.args.test_file_name\n             print(\"loading model from {}\".format(os.path.join(self.args.checkpoints, setting, best_model_path)))\n             self.model.load_state_dict(torch.load(os.path.join(self.args.checkpoints, setting, best_model_path)))\n-        preds = [[] for _ in range(4)]\n+        preds = [[] for _ in range(1)]\n         trues = []\n         folder_path = './test_results/' + setting + '/'\n         if not os.path.exists(folder_path):\n             os.makedirs(folder_path)\n"
                },
                {
                    "date": 1732016060686,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -258,9 +258,9 @@\n             self.model.load_state_dict(torch.load(best_model_path), strict=False)\n         return self.model\n \n     def test(self, setting, test=0):\n-        test_data, test_loader = self._get_data(flag='T')\n+        test_data, test_loader = self._get_data(flag='V')\n \n         print(\"info:\", self.args.test_seq_len, self.args.input_token_len, self.args.output_token_len, self.args.test_pred_len)\n         if test:\n             print('loading model')\n"
                },
                {
                    "date": 1732016182587,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -258,9 +258,9 @@\n             self.model.load_state_dict(torch.load(best_model_path), strict=False)\n         return self.model\n \n     def test(self, setting, test=0):\n-        test_data, test_loader = self._get_data(flag='V')\n+        test_data, test_loader = self._get_data(flag='TandV')\n \n         print(\"info:\", self.args.test_seq_len, self.args.input_token_len, self.args.output_token_len, self.args.test_pred_len)\n         if test:\n             print('loading model')\n"
                },
                {
                    "date": 1732017913794,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -31,8 +31,14 @@\n     denominator = (torch.abs(y_true) + torch.abs(y_pred)) / 2\n     smape = torch.mean(numerator / (denominator + 1e-8)) * 100  # Add epsilon to avoid division by zero\n     return smape\n \n+def smape_np(y_true, y_pred):\n+    numerator = np.abs(y_true - y_pred)\n+    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n+    smape = np.mean(numerator / (denominator + 1e-8)) * 100  # Add epsilon to avoid division by zero\n+    return smape\n+\n class Exp_Forecast(Exp_Basic):\n     def __init__(self, args):\n         super(Exp_Forecast, self).__init__(args)\n         \n"
                },
                {
                    "date": 1732017975424,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -343,8 +343,9 @@\n             trues = trues[:, :, -1]\n         maes, mses = [], []\n         for pred in preds:\n             mae, mse, rmse, mape, mspe = metric(pred, trues[:,:pred.shape[1],:])\n+            smape_np(pred,trues[:,:pred.shape[1],:])\n             maes.append(mae)\n             mses.append(mse)\n         print('mse:{}, mae:{}'.format(mses, maes))\n         f = open(\"result_long_term_forecast.txt\", 'a')\n"
                },
                {
                    "date": 1732017982843,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -343,9 +343,9 @@\n             trues = trues[:, :, -1]\n         maes, mses = [], []\n         for pred in preds:\n             mae, mse, rmse, mape, mspe = metric(pred, trues[:,:pred.shape[1],:])\n-            smape_np(pred,trues[:,:pred.shape[1],:])\n+            smape = smape_np(pred,trues[:,:pred.shape[1],:])\n             maes.append(mae)\n             mses.append(mse)\n         print('mse:{}, mae:{}'.format(mses, maes))\n         f = open(\"result_long_term_forecast.txt\", 'a')\n"
                },
                {
                    "date": 1732018031932,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -340,15 +340,16 @@\n         print('trues shape:', trues.shape)\n         if self.args.covariate:\n             preds = preds[:, :, -1]\n             trues = trues[:, :, -1]\n-        maes, mses = [], []\n+        maes, mses,smapes = [], [], []\n         for pred in preds:\n             mae, mse, rmse, mape, mspe = metric(pred, trues[:,:pred.shape[1],:])\n             smape = smape_np(pred,trues[:,:pred.shape[1],:])\n             maes.append(mae)\n             mses.append(mse)\n-        print('mse:{}, mae:{}'.format(mses, maes))\n+            smapes.append(smape)\n+        print('mse:{}, mae:{}, smape:{}'.format(mses, maes,smapes))\n         f = open(\"result_long_term_forecast.txt\", 'a')\n         f.write(setting + \"  \\n\")\n         f.write('mse:{}, mae:{}'.format(mses, maes))\n         f.write('\\n')\n"
                },
                {
                    "date": 1732018062431,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -288,8 +288,10 @@\n         if dis != 0:\n             inference_steps += 1\n         with torch.no_grad():\n             for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n+                if i<100:\n+                    break\n                 iter_count += 1\n                 batch_x = batch_x.float().to(self.device)\n                 batch_y = batch_y.float().to(self.device)\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n"
                },
                {
                    "date": 1732018108486,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -288,9 +288,9 @@\n         if dis != 0:\n             inference_steps += 1\n         with torch.no_grad():\n             for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n-                if i<100:\n+                if i>100:\n                     break\n                 iter_count += 1\n                 batch_x = batch_x.float().to(self.device)\n                 batch_y = batch_y.float().to(self.device)\n"
                },
                {
                    "date": 1732018180676,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -288,10 +288,8 @@\n         if dis != 0:\n             inference_steps += 1\n         with torch.no_grad():\n             for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n-                if i>100:\n-                    break\n                 iter_count += 1\n                 batch_x = batch_x.float().to(self.device)\n                 batch_y = batch_y.float().to(self.device)\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n"
                },
                {
                    "date": 1732018189859,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -264,9 +264,9 @@\n             self.model.load_state_dict(torch.load(best_model_path), strict=False)\n         return self.model\n \n     def test(self, setting, test=0):\n-        test_data, test_loader = self._get_data(flag='TandV')\n+        test_data, test_loader = self._get_data(flag='V')\n \n         print(\"info:\", self.args.test_seq_len, self.args.input_token_len, self.args.output_token_len, self.args.test_pred_len)\n         if test:\n             print('loading model')\n"
                },
                {
                    "date": 1732018227568,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -264,9 +264,9 @@\n             self.model.load_state_dict(torch.load(best_model_path), strict=False)\n         return self.model\n \n     def test(self, setting, test=0):\n-        test_data, test_loader = self._get_data(flag='V')\n+        test_data, test_loader = self._get_data(flag='T')\n \n         print(\"info:\", self.args.test_seq_len, self.args.input_token_len, self.args.output_token_len, self.args.test_pred_len)\n         if test:\n             print('loading model')\n"
                },
                {
                    "date": 1732067732561,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -264,9 +264,9 @@\n             self.model.load_state_dict(torch.load(best_model_path), strict=False)\n         return self.model\n \n     def test(self, setting, test=0):\n-        test_data, test_loader = self._get_data(flag='T')\n+        test_data, test_loader = self._get_data(flag='V')\n \n         print(\"info:\", self.args.test_seq_len, self.args.input_token_len, self.args.output_token_len, self.args.test_pred_len)\n         if test:\n             print('loading model')\n"
                },
                {
                    "date": 1732077258413,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -264,9 +264,9 @@\n             self.model.load_state_dict(torch.load(best_model_path), strict=False)\n         return self.model\n \n     def test(self, setting, test=0):\n-        test_data, test_loader = self._get_data(flag='V')\n+        test_data, test_loader = self._get_data(flag='T')\n \n         print(\"info:\", self.args.test_seq_len, self.args.input_token_len, self.args.output_token_len, self.args.test_pred_len)\n         if test:\n             print('loading model')\n"
                },
                {
                    "date": 1732083080563,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -343,9 +343,9 @@\n             trues = trues[:, :, -1]\n         maes, mses,smapes = [], [], []\n         for pred in preds:\n             mae, mse, rmse, mape, mspe = metric(pred, trues[:,:pred.shape[1],:])\n-            smape = smape_np(pred,trues[:,:pred.shape[1],:])\n+            smape = smape_np(trues[:,:pred.shape[1],:],pred)\n             maes.append(mae)\n             mses.append(mse)\n             smapes.append(smape)\n         print('mse:{}, mae:{}, smape:{}'.format(mses, maes,smapes))\n"
                },
                {
                    "date": 1732083114502,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -288,8 +288,10 @@\n         if dis != 0:\n             inference_steps += 1\n         with torch.no_grad():\n             for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n+                if i>100:\n+                    break\n                 iter_count += 1\n                 batch_x = batch_x.float().to(self.device)\n                 batch_y = batch_y.float().to(self.device)\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n"
                },
                {
                    "date": 1732083485940,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -288,10 +288,8 @@\n         if dis != 0:\n             inference_steps += 1\n         with torch.no_grad():\n             for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n-                if i>100:\n-                    break\n                 iter_count += 1\n                 batch_x = batch_x.float().to(self.device)\n                 batch_y = batch_y.float().to(self.device)\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n"
                },
                {
                    "date": 1732083516265,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -264,9 +264,9 @@\n             self.model.load_state_dict(torch.load(best_model_path), strict=False)\n         return self.model\n \n     def test(self, setting, test=0):\n-        test_data, test_loader = self._get_data(flag='T')\n+        test_data, test_loader = self._get_data(flag='V')\n \n         print(\"info:\", self.args.test_seq_len, self.args.input_token_len, self.args.output_token_len, self.args.test_pred_len)\n         if test:\n             print('loading model')\n"
                },
                {
                    "date": 1732088209490,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -286,8 +286,9 @@\n         inference_steps = self.args.test_pred_len // self.args.output_token_len\n         dis = self.args.test_pred_len - inference_steps * self.args.output_token_len\n         if dis != 0:\n             inference_steps += 1\n+        smape\n         with torch.no_grad():\n             for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n                 iter_count += 1\n                 batch_x = batch_x.float().to(self.device)\n"
                },
                {
                    "date": 1732088238720,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -286,9 +286,9 @@\n         inference_steps = self.args.test_pred_len // self.args.output_token_len\n         dis = self.args.test_pred_len - inference_steps * self.args.output_token_len\n         if dis != 0:\n             inference_steps += 1\n-        smape\n+        smape = 0\n         with torch.no_grad():\n             for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n                 iter_count += 1\n                 batch_x = batch_x.float().to(self.device)\n@@ -309,8 +309,9 @@\n                 outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)\n                 batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n                 for length,output,pred in zip([96],outputs,preds):\n                     pred.append(output[:, -length:, :].detach().cpu())\n+                    smape = \n                 # pred_y = pred_y[:, :self.args.test_pred_len, :]\n                 batch_y = batch_y[:, -self.args.test_pred_len:, :].to(self.device)\n                 \n                 # outputs = pred_y.detach().cpu()\n@@ -344,12 +345,12 @@\n             trues = trues[:, :, -1]\n         maes, mses,smapes = [], [], []\n         for pred in preds:\n             mae, mse, rmse, mape, mspe = metric(pred, trues[:,:pred.shape[1],:])\n-            smape = smape_np(trues[:,:pred.shape[1],:],pred)\n+            # smape = smape_np(trues[:,:pred.shape[1],:],pred)\n             maes.append(mae)\n             mses.append(mse)\n-            smapes.append(smape)\n+            # smapes.append(smape)\n         print('mse:{}, mae:{}, smape:{}'.format(mses, maes,smapes))\n         f = open(\"result_long_term_forecast.txt\", 'a')\n         f.write(setting + \"  \\n\")\n         f.write('mse:{}, mae:{}'.format(mses, maes))\n"
                },
                {
                    "date": 1732088256212,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -339,8 +339,9 @@\n         preds = [torch.cat(pred, dim=0).numpy() for pred in preds]\n         trues = torch.cat(trues, dim=0).numpy()\n         # print('preds shape:', preds.shape)\n         print('trues shape:', trues.shape)\n+        print(smape)\n         if self.args.covariate:\n             preds = preds[:, :, -1]\n             trues = trues[:, :, -1]\n         maes, mses,smapes = [], [], []\n"
                },
                {
                    "date": 1732088268772,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -343,16 +343,16 @@\n         print(smape)\n         if self.args.covariate:\n             preds = preds[:, :, -1]\n             trues = trues[:, :, -1]\n-        maes, mses,smapes = [], [], []\n+        maes, mses = [], []\n         for pred in preds:\n             mae, mse, rmse, mape, mspe = metric(pred, trues[:,:pred.shape[1],:])\n             # smape = smape_np(trues[:,:pred.shape[1],:],pred)\n             maes.append(mae)\n             mses.append(mse)\n             # smapes.append(smape)\n-        print('mse:{}, mae:{}, smape:{}'.format(mses, maes,smapes))\n+        print('mse:{}, mae:{}'.format(mses, maes))\n         f = open(\"result_long_term_forecast.txt\", 'a')\n         f.write(setting + \"  \\n\")\n         f.write('mse:{}, mae:{}'.format(mses, maes))\n         f.write('\\n')\n"
                },
                {
                    "date": 1732088299925,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -309,9 +309,9 @@\n                 outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)\n                 batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n                 for length,output,pred in zip([96],outputs,preds):\n                     pred.append(output[:, -length:, :].detach().cpu())\n-                    smape = \n+                    smape = smape(trues[:,:pred.shape[1],:],pred[-1])\n                 # pred_y = pred_y[:, :self.args.test_pred_len, :]\n                 batch_y = batch_y[:, -self.args.test_pred_len:, :].to(self.device)\n                 \n                 # outputs = pred_y.detach().cpu()\n"
                },
                {
                    "date": 1732088324139,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -286,9 +286,9 @@\n         inference_steps = self.args.test_pred_len // self.args.output_token_len\n         dis = self.args.test_pred_len - inference_steps * self.args.output_token_len\n         if dis != 0:\n             inference_steps += 1\n-        smape = 0\n+        smape_avg = 0\n         with torch.no_grad():\n             for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n                 iter_count += 1\n                 batch_x = batch_x.float().to(self.device)\n"
                },
                {
                    "date": 1732088367328,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -287,8 +287,9 @@\n         dis = self.args.test_pred_len - inference_steps * self.args.output_token_len\n         if dis != 0:\n             inference_steps += 1\n         smape_avg = 0\n+        count = 0\n         with torch.no_grad():\n             for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n                 iter_count += 1\n                 batch_x = batch_x.float().to(self.device)\n@@ -309,9 +310,9 @@\n                 outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)\n                 batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n                 for length,output,pred in zip([96],outputs,preds):\n                     pred.append(output[:, -length:, :].detach().cpu())\n-                    smape = smape(trues[:,:pred.shape[1],:],pred[-1])\n+                    smape_avg = (smape_avg*count + smape(trues[:,:pred.shape[1],:],pred[-1]))/(count+batch_y.shape[0])\n                 # pred_y = pred_y[:, :self.args.test_pred_len, :]\n                 batch_y = batch_y[:, -self.args.test_pred_len:, :].to(self.device)\n                 \n                 # outputs = pred_y.detach().cpu()\n"
                },
                {
                    "date": 1732088381701,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -311,8 +311,9 @@\n                 batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n                 for length,output,pred in zip([96],outputs,preds):\n                     pred.append(output[:, -length:, :].detach().cpu())\n                     smape_avg = (smape_avg*count + smape(trues[:,:pred.shape[1],:],pred[-1]))/(count+batch_y.shape[0])\n+                    count += batch_y.shape[0]\n                 # pred_y = pred_y[:, :self.args.test_pred_len, :]\n                 batch_y = batch_y[:, -self.args.test_pred_len:, :].to(self.device)\n                 \n                 # outputs = pred_y.detach().cpu()\n"
                },
                {
                    "date": 1732088412890,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -341,9 +341,9 @@\n         preds = [torch.cat(pred, dim=0).numpy() for pred in preds]\n         trues = torch.cat(trues, dim=0).numpy()\n         # print('preds shape:', preds.shape)\n         print('trues shape:', trues.shape)\n-        print(smape)\n+        print(smape_avg)\n         if self.args.covariate:\n             preds = preds[:, :, -1]\n             trues = trues[:, :, -1]\n         maes, mses = [], []\n"
                },
                {
                    "date": 1732088497821,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -310,9 +310,9 @@\n                 outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)\n                 batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n                 for length,output,pred in zip([96],outputs,preds):\n                     pred.append(output[:, -length:, :].detach().cpu())\n-                    smape_avg = (smape_avg*count + smape(trues[:,:pred.shape[1],:],pred[-1]))/(count+batch_y.shape[0])\n+                    smape_avg = (smape_avg*count + smape(batch_y[:,:pred[-1].shape[1],:],pred[-1]))/(count+batch_y.shape[0])\n                     count += batch_y.shape[0]\n                 # pred_y = pred_y[:, :self.args.test_pred_len, :]\n                 batch_y = batch_y[:, -self.args.test_pred_len:, :].to(self.device)\n                 \n"
                },
                {
                    "date": 1732088522205,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -313,9 +313,9 @@\n                     pred.append(output[:, -length:, :].detach().cpu())\n                     smape_avg = (smape_avg*count + smape(batch_y[:,:pred[-1].shape[1],:],pred[-1]))/(count+batch_y.shape[0])\n                     count += batch_y.shape[0]\n                 # pred_y = pred_y[:, :self.args.test_pred_len, :]\n-                batch_y = batch_y[:, -self.args.test_pred_len:, :].to(self.device)\n+                # batch_y = batch_y[:, -self.args.test_pred_len:, :].to(self.device)\n                 \n                 # outputs = pred_y.detach().cpu()\n                 batch_y = batch_y.detach().cpu()\n                 # pred = outputs\n"
                },
                {
                    "date": 1732088577285,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -307,18 +307,18 @@\n                 # pred_y = self.model(batch_x, batch_x_mark, None, batch_y_mark, inference_steps)\n                 # if dis != 0:\n                 #     pred_y = pred_y[:, :-dis, :]\n                 outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)\n-                batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n+                batch_y = batch_y[:, -self.args.output_token_len:, :].detach().cpu()\n                 for length,output,pred in zip([96],outputs,preds):\n                     pred.append(output[:, -length:, :].detach().cpu())\n                     smape_avg = (smape_avg*count + smape(batch_y[:,:pred[-1].shape[1],:],pred[-1]))/(count+batch_y.shape[0])\n                     count += batch_y.shape[0]\n                 # pred_y = pred_y[:, :self.args.test_pred_len, :]\n                 # batch_y = batch_y[:, -self.args.test_pred_len:, :].to(self.device)\n                 \n                 # outputs = pred_y.detach().cpu()\n-                batch_y = batch_y.detach().cpu()\n+                \n                 # pred = outputs\n                 true = batch_y\n \n                 # preds.append(pred)\n"
                },
                {
                    "date": 1732088664292,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -326,9 +326,9 @@\n                 if (i + 1) % 100 == 0:\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                         speed = (time.time() - time_now) / iter_count\n                         left_time = speed * (test_steps - i)\n-                        print(\"\\titers: {}, speed: {:.4f}s/iter, left time: {:.4f}s\".format(i + 1, speed, left_time))\n+                        print(\"\\titers: {}, speed: {:.4f}s/iter, left time: {:.4f}s, smape:{}\".format(i + 1, speed, left_time, smape_avg))\n                         iter_count = 0\n                         time_now = time.time()\n                 if self.args.visualize and i % 2 == 0:\n                     dir_path = folder_path + f'{self.args.test_pred_len}/'\n"
                },
                {
                    "date": 1732088972512,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -310,9 +310,10 @@\n                 outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)\n                 batch_y = batch_y[:, -self.args.output_token_len:, :].detach().cpu()\n                 for length,output,pred in zip([96],outputs,preds):\n                     pred.append(output[:, -length:, :].detach().cpu())\n-                    smape_avg = (smape_avg*count + smape(batch_y[:,:pred[-1].shape[1],:],pred[-1]))/(count+batch_y.shape[0])\n+                    smape_cur = smape(batch_y[:,:pred[-1].shape[1],:],pred[-1])\n+                    smape_avg = (smape_avg*count + )/(count+batch_y.shape[0])\n                     count += batch_y.shape[0]\n                 # pred_y = pred_y[:, :self.args.test_pred_len, :]\n                 # batch_y = batch_y[:, -self.args.test_pred_len:, :].to(self.device)\n                 \n"
                },
                {
                    "date": 1732088978333,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -311,9 +311,9 @@\n                 batch_y = batch_y[:, -self.args.output_token_len:, :].detach().cpu()\n                 for length,output,pred in zip([96],outputs,preds):\n                     pred.append(output[:, -length:, :].detach().cpu())\n                     smape_cur = smape(batch_y[:,:pred[-1].shape[1],:],pred[-1])\n-                    smape_avg = (smape_avg*count + )/(count+batch_y.shape[0])\n+                    smape_avg = (smape_avg*count + smape_cur)/(count+batch_y.shape[0])\n                     count += batch_y.shape[0]\n                 # pred_y = pred_y[:, :self.args.test_pred_len, :]\n                 # batch_y = batch_y[:, -self.args.test_pred_len:, :].to(self.device)\n                 \n"
                },
                {
                    "date": 1732089186404,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -311,8 +311,9 @@\n                 batch_y = batch_y[:, -self.args.output_token_len:, :].detach().cpu()\n                 for length,output,pred in zip([96],outputs,preds):\n                     pred.append(output[:, -length:, :].detach().cpu())\n                     smape_cur = smape(batch_y[:,:pred[-1].shape[1],:],pred[-1])\n+                    print(smape_cur)\n                     smape_avg = (smape_avg*count + smape_cur)/(count+batch_y.shape[0])\n                     count += batch_y.shape[0]\n                 # pred_y = pred_y[:, :self.args.test_pred_len, :]\n                 # batch_y = batch_y[:, -self.args.test_pred_len:, :].to(self.device)\n"
                },
                {
                    "date": 1732089252303,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -312,9 +312,9 @@\n                 for length,output,pred in zip([96],outputs,preds):\n                     pred.append(output[:, -length:, :].detach().cpu())\n                     smape_cur = smape(batch_y[:,:pred[-1].shape[1],:],pred[-1])\n                     print(smape_cur)\n-                    smape_avg = (smape_avg*count + smape_cur)/(count+batch_y.shape[0])\n+                    smape_avg = (smape_avg*count + smape_cur*batch_y.shape[0])/(count+batch_y.shape[0])\n                     count += batch_y.shape[0]\n                 # pred_y = pred_y[:, :self.args.test_pred_len, :]\n                 # batch_y = batch_y[:, -self.args.test_pred_len:, :].to(self.device)\n                 \n"
                },
                {
                    "date": 1732089261682,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -311,9 +311,9 @@\n                 batch_y = batch_y[:, -self.args.output_token_len:, :].detach().cpu()\n                 for length,output,pred in zip([96],outputs,preds):\n                     pred.append(output[:, -length:, :].detach().cpu())\n                     smape_cur = smape(batch_y[:,:pred[-1].shape[1],:],pred[-1])\n-                    print(smape_cur)\n+                    print(smape_cur,smape_avg)\n                     smape_avg = (smape_avg*count + smape_cur*batch_y.shape[0])/(count+batch_y.shape[0])\n                     count += batch_y.shape[0]\n                 # pred_y = pred_y[:, :self.args.test_pred_len, :]\n                 # batch_y = batch_y[:, -self.args.test_pred_len:, :].to(self.device)\n"
                },
                {
                    "date": 1732089302279,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -311,9 +311,8 @@\n                 batch_y = batch_y[:, -self.args.output_token_len:, :].detach().cpu()\n                 for length,output,pred in zip([96],outputs,preds):\n                     pred.append(output[:, -length:, :].detach().cpu())\n                     smape_cur = smape(batch_y[:,:pred[-1].shape[1],:],pred[-1])\n-                    print(smape_cur,smape_avg)\n                     smape_avg = (smape_avg*count + smape_cur*batch_y.shape[0])/(count+batch_y.shape[0])\n                     count += batch_y.shape[0]\n                 # pred_y = pred_y[:, :self.args.test_pred_len, :]\n                 # batch_y = batch_y[:, -self.args.test_pred_len:, :].to(self.device)\n"
                },
                {
                    "date": 1732090398411,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -264,9 +264,9 @@\n             self.model.load_state_dict(torch.load(best_model_path), strict=False)\n         return self.model\n \n     def test(self, setting, test=0):\n-        test_data, test_loader = self._get_data(flag='V')\n+        test_data, test_loader = self._get_data(flag='T')\n \n         print(\"info:\", self.args.test_seq_len, self.args.input_token_len, self.args.output_token_len, self.args.test_pred_len)\n         if test:\n             print('loading model')\n"
                },
                {
                    "date": 1732095335208,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -337,14 +337,14 @@\n                         os.makedirs(dir_path)\n                     gt = np.array(true[0, :, -1])\n                     pd = np.array(pred[0, :, -1])\n                     visual(gt, pd, os.path.join(dir_path, f'{i}.pdf'))\n-\n+        print(smape_avg)\n         preds = [torch.cat(pred, dim=0).numpy() for pred in preds]\n         trues = torch.cat(trues, dim=0).numpy()\n         # print('preds shape:', preds.shape)\n         print('trues shape:', trues.shape)\n-        print(smape_avg)\n+        \n         if self.args.covariate:\n             preds = preds[:, :, -1]\n             trues = trues[:, :, -1]\n         maes, mses = [], []\n"
                },
                {
                    "date": 1732095707825,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -264,9 +264,9 @@\n             self.model.load_state_dict(torch.load(best_model_path), strict=False)\n         return self.model\n \n     def test(self, setting, test=0):\n-        test_data, test_loader = self._get_data(flag='T')\n+        test_data, test_loader = self._get_data(flag='V')\n \n         print(\"info:\", self.args.test_seq_len, self.args.input_token_len, self.args.output_token_len, self.args.test_pred_len)\n         if test:\n             print('loading model')\n"
                },
                {
                    "date": 1732102771078,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -264,9 +264,9 @@\n             self.model.load_state_dict(torch.load(best_model_path), strict=False)\n         return self.model\n \n     def test(self, setting, test=0):\n-        test_data, test_loader = self._get_data(flag='V')\n+        test_data, test_loader = self._get_data(flag='TandV')\n \n         print(\"info:\", self.args.test_seq_len, self.args.input_token_len, self.args.output_token_len, self.args.test_pred_len)\n         if test:\n             print('loading model')\n"
                },
                {
                    "date": 1732107773150,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -264,9 +264,9 @@\n             self.model.load_state_dict(torch.load(best_model_path), strict=False)\n         return self.model\n \n     def test(self, setting, test=0):\n-        test_data, test_loader = self._get_data(flag='TandV')\n+        test_data, test_loader = self._get_data(flag='V')\n \n         print(\"info:\", self.args.test_seq_len, self.args.input_token_len, self.args.output_token_len, self.args.test_pred_len)\n         if test:\n             print('loading model')\n"
                },
                {
                    "date": 1732157005926,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -264,9 +264,9 @@\n             self.model.load_state_dict(torch.load(best_model_path), strict=False)\n         return self.model\n \n     def test(self, setting, test=0):\n-        test_data, test_loader = self._get_data(flag='V')\n+        test_data, test_loader = self._get_data(flag='T')\n \n         print(\"info:\", self.args.test_seq_len, self.args.input_token_len, self.args.output_token_len, self.args.test_pred_len)\n         if test:\n             print('loading model')\n"
                },
                {
                    "date": 1732160774926,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -264,9 +264,9 @@\n             self.model.load_state_dict(torch.load(best_model_path), strict=False)\n         return self.model\n \n     def test(self, setting, test=0):\n-        test_data, test_loader = self._get_data(flag='T')\n+        test_data, test_loader = self._get_data(flag='V')\n \n         print(\"info:\", self.args.test_seq_len, self.args.input_token_len, self.args.output_token_len, self.args.test_pred_len)\n         if test:\n             print('loading model')\n"
                },
                {
                    "date": 1732162111309,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -264,9 +264,9 @@\n             self.model.load_state_dict(torch.load(best_model_path), strict=False)\n         return self.model\n \n     def test(self, setting, test=0):\n-        test_data, test_loader = self._get_data(flag='V')\n+        test_data, test_loader = self._get_data(flag='T')\n \n         print(\"info:\", self.args.test_seq_len, self.args.input_token_len, self.args.output_token_len, self.args.test_pred_len)\n         if test:\n             print('loading model')\n"
                },
                {
                    "date": 1732250034002,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -264,9 +264,9 @@\n             self.model.load_state_dict(torch.load(best_model_path), strict=False)\n         return self.model\n \n     def test(self, setting, test=0):\n-        test_data, test_loader = self._get_data(flag='T')\n+        test_data, test_loader = self._get_data(flag='V')\n \n         print(\"info:\", self.args.test_seq_len, self.args.input_token_len, self.args.output_token_len, self.args.test_pred_len)\n         if test:\n             print('loading model')\n"
                },
                {
                    "date": 1732250300805,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -290,8 +290,10 @@\n         smape_avg = 0\n         count = 0\n         with torch.no_grad():\n             for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n+                if i+1<=26400:\n+                    continue\n                 iter_count += 1\n                 batch_x = batch_x.float().to(self.device)\n                 batch_y = batch_y.float().to(self.device)\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n"
                },
                {
                    "date": 1732250313576,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -286,9 +286,9 @@\n         inference_steps = self.args.test_pred_len // self.args.output_token_len\n         dis = self.args.test_pred_len - inference_steps * self.args.output_token_len\n         if dis != 0:\n             inference_steps += 1\n-        smape_avg = 0\n+        smape_avg = 72.11304473876953\n         count = 0\n         with torch.no_grad():\n             for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n                 if i+1<=26400:\n"
                },
                {
                    "date": 1732250749139,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -290,9 +290,9 @@\n         smape_avg = 72.11304473876953\n         count = 0\n         with torch.no_grad():\n             for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n-                if i+1<=26400:\n+                if i+1<=27400:\n                     continue\n                 iter_count += 1\n                 batch_x = batch_x.float().to(self.device)\n                 batch_y = batch_y.float().to(self.device)\n"
                },
                {
                    "date": 1732250761156,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -286,9 +286,9 @@\n         inference_steps = self.args.test_pred_len // self.args.output_token_len\n         dis = self.args.test_pred_len - inference_steps * self.args.output_token_len\n         if dis != 0:\n             inference_steps += 1\n-        smape_avg = 72.11304473876953\n+        smape_avg = 71.76183319091797\n         count = 0\n         with torch.no_grad():\n             for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n                 if i+1<=27400:\n"
                },
                {
                    "date": 1732274091265,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -286,14 +286,12 @@\n         inference_steps = self.args.test_pred_len // self.args.output_token_len\n         dis = self.args.test_pred_len - inference_steps * self.args.output_token_len\n         if dis != 0:\n             inference_steps += 1\n-        smape_avg = 71.76183319091797\n+        smape_avg = 0\n         count = 0\n         with torch.no_grad():\n             for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n-                if i+1<=27400:\n-                    continue\n                 iter_count += 1\n                 batch_x = batch_x.float().to(self.device)\n                 batch_y = batch_y.float().to(self.device)\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n"
                },
                {
                    "date": 1732274145533,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -264,9 +264,9 @@\n             self.model.load_state_dict(torch.load(best_model_path), strict=False)\n         return self.model\n \n     def test(self, setting, test=0):\n-        test_data, test_loader = self._get_data(flag='V')\n+        test_data, test_loader = self._get_data(flag='T')\n \n         print(\"info:\", self.args.test_seq_len, self.args.input_token_len, self.args.output_token_len, self.args.test_pred_len)\n         if test:\n             print('loading model')\n"
                },
                {
                    "date": 1732274159440,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -264,9 +264,9 @@\n             self.model.load_state_dict(torch.load(best_model_path), strict=False)\n         return self.model\n \n     def test(self, setting, test=0):\n-        test_data, test_loader = self._get_data(flag='T')\n+        test_data, test_loader = self._get_data(flag='TandV')\n \n         print(\"info:\", self.args.test_seq_len, self.args.input_token_len, self.args.output_token_len, self.args.test_pred_len)\n         if test:\n             print('loading model')\n"
                },
                {
                    "date": 1732274193554,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -264,9 +264,9 @@\n             self.model.load_state_dict(torch.load(best_model_path), strict=False)\n         return self.model\n \n     def test(self, setting, test=0):\n-        test_data, test_loader = self._get_data(flag='TandV')\n+        test_data, test_loader = self._get_data(flag='T')\n \n         print(\"info:\", self.args.test_seq_len, self.args.input_token_len, self.args.output_token_len, self.args.test_pred_len)\n         if test:\n             print('loading model')\n"
                },
                {
                    "date": 1732274311293,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -264,9 +264,9 @@\n             self.model.load_state_dict(torch.load(best_model_path), strict=False)\n         return self.model\n \n     def test(self, setting, test=0):\n-        test_data, test_loader = self._get_data(flag='T')\n+        test_data, test_loader = self._get_data(flag='V')\n \n         print(\"info:\", self.args.test_seq_len, self.args.input_token_len, self.args.output_token_len, self.args.test_pred_len)\n         if test:\n             print('loading model')\n"
                },
                {
                    "date": 1732274361535,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -264,9 +264,9 @@\n             self.model.load_state_dict(torch.load(best_model_path), strict=False)\n         return self.model\n \n     def test(self, setting, test=0):\n-        test_data, test_loader = self._get_data(flag='V')\n+        test_data, test_loader = self._get_data(flag='TandV')\n \n         print(\"info:\", self.args.test_seq_len, self.args.input_token_len, self.args.output_token_len, self.args.test_pred_len)\n         if test:\n             print('loading model')\n"
                },
                {
                    "date": 1732325404853,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -264,9 +264,9 @@\n             self.model.load_state_dict(torch.load(best_model_path), strict=False)\n         return self.model\n \n     def test(self, setting, test=0):\n-        test_data, test_loader = self._get_data(flag='TandV')\n+        test_data, test_loader = self._get_data(flag='V')\n \n         print(\"info:\", self.args.test_seq_len, self.args.input_token_len, self.args.output_token_len, self.args.test_pred_len)\n         if test:\n             print('loading model')\n"
                },
                {
                    "date": 1732325568119,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -264,9 +264,9 @@\n             self.model.load_state_dict(torch.load(best_model_path), strict=False)\n         return self.model\n \n     def test(self, setting, test=0):\n-        test_data, test_loader = self._get_data(flag='V')\n+        test_data, test_loader = self._get_data(flag='TandV')\n \n         print(\"info:\", self.args.test_seq_len, self.args.input_token_len, self.args.output_token_len, self.args.test_pred_len)\n         if test:\n             print('loading model')\n"
                },
                {
                    "date": 1732336633904,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -264,9 +264,9 @@\n             self.model.load_state_dict(torch.load(best_model_path), strict=False)\n         return self.model\n \n     def test(self, setting, test=0):\n-        test_data, test_loader = self._get_data(flag='TandV')\n+        test_data, test_loader = self._get_data(flag='V')\n \n         print(\"info:\", self.args.test_seq_len, self.args.input_token_len, self.args.output_token_len, self.args.test_pred_len)\n         if test:\n             print('loading model')\n"
                },
                {
                    "date": 1732420388874,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -264,9 +264,9 @@\n             self.model.load_state_dict(torch.load(best_model_path), strict=False)\n         return self.model\n \n     def test(self, setting, test=0):\n-        test_data, test_loader = self._get_data(flag='V')\n+        test_data, test_loader = self._get_data(flag='T')\n \n         print(\"info:\", self.args.test_seq_len, self.args.input_token_len, self.args.output_token_len, self.args.test_pred_len)\n         if test:\n             print('loading model')\n"
                },
                {
                    "date": 1732433420794,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -264,9 +264,9 @@\n             self.model.load_state_dict(torch.load(best_model_path), strict=False)\n         return self.model\n \n     def test(self, setting, test=0):\n-        test_data, test_loader = self._get_data(flag='T')\n+        test_data, test_loader = self._get_data(flag='V')\n \n         print(\"info:\", self.args.test_seq_len, self.args.input_token_len, self.args.output_token_len, self.args.test_pred_len)\n         if test:\n             print('loading model')\n"
                },
                {
                    "date": 1732680670728,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -264,9 +264,9 @@\n             self.model.load_state_dict(torch.load(best_model_path), strict=False)\n         return self.model\n \n     def test(self, setting, test=0):\n-        test_data, test_loader = self._get_data(flag='V')\n+        test_data, test_loader = self._get_data(flag='T')\n \n         print(\"info:\", self.args.test_seq_len, self.args.input_token_len, self.args.output_token_len, self.args.test_pred_len)\n         if test:\n             print('loading model')\n"
                },
                {
                    "date": 1732680688800,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -264,9 +264,9 @@\n             self.model.load_state_dict(torch.load(best_model_path), strict=False)\n         return self.model\n \n     def test(self, setting, test=0):\n-        test_data, test_loader = self._get_data(flag='T')\n+        test_data, test_loader = self._get_data(flag='TandV')\n \n         print(\"info:\", self.args.test_seq_len, self.args.input_token_len, self.args.output_token_len, self.args.test_pred_len)\n         if test:\n             print('loading model')\n"
                },
                {
                    "date": 1732687755220,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -264,9 +264,9 @@\n             self.model.load_state_dict(torch.load(best_model_path), strict=False)\n         return self.model\n \n     def test(self, setting, test=0):\n-        test_data, test_loader = self._get_data(flag='TandV')\n+        test_data, test_loader = self._get_data(flag='V')\n \n         print(\"info:\", self.args.test_seq_len, self.args.input_token_len, self.args.output_token_len, self.args.test_pred_len)\n         if test:\n             print('loading model')\n"
                },
                {
                    "date": 1732774535707,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -264,9 +264,9 @@\n             self.model.load_state_dict(torch.load(best_model_path), strict=False)\n         return self.model\n \n     def test(self, setting, test=0):\n-        test_data, test_loader = self._get_data(flag='V')\n+        test_data, test_loader = self._get_data(flag='T')\n \n         print(\"info:\", self.args.test_seq_len, self.args.input_token_len, self.args.output_token_len, self.args.test_pred_len)\n         if test:\n             print('loading model')\n"
                },
                {
                    "date": 1732774575886,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -264,9 +264,9 @@\n             self.model.load_state_dict(torch.load(best_model_path), strict=False)\n         return self.model\n \n     def test(self, setting, test=0):\n-        test_data, test_loader = self._get_data(flag='T')\n+        test_data, test_loader = self._get_data(flag='V')\n \n         print(\"info:\", self.args.test_seq_len, self.args.input_token_len, self.args.output_token_len, self.args.test_pred_len)\n         if test:\n             print('loading model')\n"
                },
                {
                    "date": 1732774685998,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -264,9 +264,9 @@\n             self.model.load_state_dict(torch.load(best_model_path), strict=False)\n         return self.model\n \n     def test(self, setting, test=0):\n-        test_data, test_loader = self._get_data(flag='V')\n+        test_data, test_loader = self._get_data(flag='T')\n \n         print(\"info:\", self.args.test_seq_len, self.args.input_token_len, self.args.output_token_len, self.args.test_pred_len)\n         if test:\n             print('loading model')\n"
                },
                {
                    "date": 1732874171486,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -264,9 +264,9 @@\n             self.model.load_state_dict(torch.load(best_model_path), strict=False)\n         return self.model\n \n     def test(self, setting, test=0):\n-        test_data, test_loader = self._get_data(flag='T')\n+        test_data, test_loader = self._get_data(flag='V')\n \n         print(\"info:\", self.args.test_seq_len, self.args.input_token_len, self.args.output_token_len, self.args.test_pred_len)\n         if test:\n             print('loading model')\n"
                },
                {
                    "date": 1732880098879,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -264,9 +264,9 @@\n             self.model.load_state_dict(torch.load(best_model_path), strict=False)\n         return self.model\n \n     def test(self, setting, test=0):\n-        test_data, test_loader = self._get_data(flag='V')\n+        test_data, test_loader = self._get_data(flag='TandV')\n \n         print(\"info:\", self.args.test_seq_len, self.args.input_token_len, self.args.output_token_len, self.args.test_pred_len)\n         if test:\n             print('loading model')\n"
                },
                {
                    "date": 1732889927973,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -273,9 +273,9 @@\n             setting = self.args.test_dir\n             best_model_path = self.args.test_file_name\n             print(\"loading model from {}\".format(os.path.join(self.args.checkpoints, setting, best_model_path)))\n             self.model.load_state_dict(torch.load(os.path.join(self.args.checkpoints, setting, best_model_path)))\n-        preds = [[] for _ in range(1)]\n+        preds = [[] for _ in range(4)]\n         trues = []\n         folder_path = './test_results/' + setting + '/'\n         if not os.path.exists(folder_path):\n             os.makedirs(folder_path)\n"
                },
                {
                    "date": 1732889936153,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -308,9 +308,9 @@\n                 # if dis != 0:\n                 #     pred_y = pred_y[:, :-dis, :]\n                 outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)\n                 batch_y = batch_y[:, -self.args.output_token_len:, :].detach().cpu()\n-                for length,output,pred in zip([96],outputs,preds):\n+                for length,output,pred in zip([96,192,336,720],outputs,preds):\n                     pred.append(output[:, -length:, :].detach().cpu())\n                     smape_cur = smape(batch_y[:,:pred[-1].shape[1],:],pred[-1])\n                     smape_avg = (smape_avg*count + smape_cur*batch_y.shape[0])/(count+batch_y.shape[0])\n                     count += batch_y.shape[0]\n"
                },
                {
                    "date": 1732889997406,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -125,11 +125,11 @@\n                     else:\n                         outputs = outputs[:, :, -1]\n                         batch_y = batch_y[:, :, -1]\n                 batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n-                for length,output,tl in zip([96],outputs,total_loss):\n-                    # loss = (criterion(output[:, -length:, :], batch_y[:,:length,:])).mean().detach().cpu()\n-                    loss = smape(batch_y[:,:length,:], output[:, -length:, :]).item()\n+                for length,output,tl in zip([96,192,336,720],outputs,total_loss):\n+                    loss = (criterion(output[:, -length:, :], batch_y[:,:length,:])).mean().detach().cpu()\n+                    # loss = smape(batch_y[:,:length,:], output[:, -length:, :]).item()\n                     tl.append(loss)\n                 # loss = criterion(outputs, batch_y)\n \n                 # loss = loss.detach().cpu()\n"
                },
                {
                    "date": 1732890005249,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -91,9 +91,9 @@\n         criterion = nn.MSELoss()\n         return criterion\n \n     def vali(self, vali_data, vali_loader, criterion, is_test=False):\n-        total_loss = [[] for _ in range(1)]\n+        total_loss = [[] for _ in range(4)]\n         total_count = []\n         time_now = time.time()\n         test_steps = len(vali_loader)\n         iter_count = 0\n"
                },
                {
                    "date": 1732890055291,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -177,9 +177,9 @@\n         for epoch in range(self.args.train_epochs):\n             iter_count = 0\n             self.model.train()\n             epoch_time = time.time()\n-            train_loader.dataset.__shuffle_data__()\n+            # train_loader.dataset.__shuffle_data__()\n             for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n                 iter_count += 1\n                 model_optim.zero_grad()\n                 batch_x = batch_x.float().to(self.device)\n"
                },
                {
                    "date": 1732890090020,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -215,9 +215,9 @@\n                         outputs = outputs[:, :, -1]\n                         batch_y = batch_y[:, :, -1]\n                 loss = 0\n                 \n-                patch_num = 32\n+                patch_num = 7\n                 batch_y = batch_y.reshape(B,patch_num,-1,C)\n                 for output in outputs:\n                     loss += (train_criterion(output, batch_y[:,:,:output.shape[1]//patch_num,:].reshape(B,-1,C))).mean()\n                 # loss = (train_criterion(outputs, batch_y) * weights).mean()\n"
                },
                {
                    "date": 1732931627226,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -232,30 +232,30 @@\n \n                 loss.backward()\n                 model_optim.step()\n                 \n-                if (i + 1) % 5000 == 0:\n-                    if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n-                        print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n+                # if (i + 1) % 5000 == 0:\n+                if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n+                    print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n \n-                    vali_loss = self.vali(vali_data, vali_loader, train_criterion, is_test=self.args.valid_last)\n-                    test_loss = self.vali(test_data, test_loader, train_criterion, is_test=True)\n+                vali_loss = self.vali(vali_data, vali_loader, train_criterion, is_test=self.args.valid_last)\n+                test_loss = self.vali(test_data, test_loader, train_criterion, is_test=True)\n+                if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n+                    print(\"Epoch: {}, Steps: {} | Vali Loss: {} Test Loss: {}\".format(\n+                        epoch + 1, train_steps, vali_loss, test_loss))\n+                early_stopping(sum(vali_loss), self.model, path)\n+                if early_stopping.early_stop:\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n-                        print(\"Epoch: {}, Steps: {} | Vali Loss: {} Test Loss: {}\".format(\n-                            epoch + 1, train_steps, vali_loss, test_loss))\n-                    early_stopping(sum(vali_loss), self.model, path)\n-                    if early_stopping.early_stop:\n-                        if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n-                            print(\"Early stopping\")\n-                        break\n-                    if self.args.cosine:\n-                        scheduler.step()\n-                        if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n-                            print(\"lr = {:.10f}\".format(model_optim.param_groups[0]['lr']))\n-                    else:\n-                        adjust_learning_rate(model_optim, epoch + 1, self.args)\n-                    if self.args.ddp:\n-                        train_loader.sampler.set_epoch(epoch + 1)\n+                        print(\"Early stopping\")\n+                    break\n+                if self.args.cosine:\n+                    scheduler.step()\n+                    if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n+                        print(\"lr = {:.10f}\".format(model_optim.param_groups[0]['lr']))\n+                else:\n+                    adjust_learning_rate(model_optim, epoch + 1, self.args)\n+                if self.args.ddp:\n+                    train_loader.sampler.set_epoch(epoch + 1)\n                 \n         best_model_path = path + '/' + 'checkpoint.pth'\n         if self.args.ddp:\n             dist.barrier()\n"
                },
                {
                    "date": 1732932104479,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -233,29 +233,29 @@\n                 loss.backward()\n                 model_optim.step()\n                 \n                 # if (i + 1) % 5000 == 0:\n-                if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n-                    print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n+            if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n+                print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n \n-                vali_loss = self.vali(vali_data, vali_loader, train_criterion, is_test=self.args.valid_last)\n-                test_loss = self.vali(test_data, test_loader, train_criterion, is_test=True)\n+            vali_loss = self.vali(vali_data, vali_loader, train_criterion, is_test=self.args.valid_last)\n+            test_loss = self.vali(test_data, test_loader, train_criterion, is_test=True)\n+            if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n+                print(\"Epoch: {}, Steps: {} | Vali Loss: {} Test Loss: {}\".format(\n+                    epoch + 1, train_steps, vali_loss, test_loss))\n+            early_stopping(sum(vali_loss), self.model, path)\n+            if early_stopping.early_stop:\n                 if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n-                    print(\"Epoch: {}, Steps: {} | Vali Loss: {} Test Loss: {}\".format(\n-                        epoch + 1, train_steps, vali_loss, test_loss))\n-                early_stopping(sum(vali_loss), self.model, path)\n-                if early_stopping.early_stop:\n-                    if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n-                        print(\"Early stopping\")\n-                    break\n-                if self.args.cosine:\n-                    scheduler.step()\n-                    if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n-                        print(\"lr = {:.10f}\".format(model_optim.param_groups[0]['lr']))\n-                else:\n-                    adjust_learning_rate(model_optim, epoch + 1, self.args)\n-                if self.args.ddp:\n-                    train_loader.sampler.set_epoch(epoch + 1)\n+                    print(\"Early stopping\")\n+                break\n+            if self.args.cosine:\n+                scheduler.step()\n+                if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n+                    print(\"lr = {:.10f}\".format(model_optim.param_groups[0]['lr']))\n+            else:\n+                adjust_learning_rate(model_optim, epoch + 1, self.args)\n+            if self.args.ddp:\n+                train_loader.sampler.set_epoch(epoch + 1)\n                 \n         best_model_path = path + '/' + 'checkpoint.pth'\n         if self.args.ddp:\n             dist.barrier()\n"
                },
                {
                    "date": 1732954849329,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -264,9 +264,9 @@\n             self.model.load_state_dict(torch.load(best_model_path), strict=False)\n         return self.model\n \n     def test(self, setting, test=0):\n-        test_data, test_loader = self._get_data(flag='TandV')\n+        test_data, test_loader = self._get_data(flag='test')\n \n         print(\"info:\", self.args.test_seq_len, self.args.input_token_len, self.args.output_token_len, self.args.test_pred_len)\n         if test:\n             print('loading model')\n"
                },
                {
                    "date": 1733149256258,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -178,16 +178,16 @@\n             iter_count = 0\n             self.model.train()\n             epoch_time = time.time()\n             # train_loader.dataset.__shuffle_data__()\n-            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n+            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark, lengths) in enumerate(train_loader):\n                 iter_count += 1\n                 model_optim.zero_grad()\n                 batch_x = batch_x.float().to(self.device)\n                 batch_y = batch_y.float().to(self.device)\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n                 batch_y_mark = batch_y_mark.float().to(self.device)\n-                # lengths = lengths.long().to(self.device)\n+                lengths = lengths.long().to(self.device)\n                 \n                 # lookback = random.randint(1,min(7,epoch+4))*self.args.input_token_len\n                 # batch_x = batch_x[:,-lookback:,:]\n                 # batch_y = batch_y[:,-lookback:,:]\n@@ -200,9 +200,9 @@\n                 # gsize = M//gnum\n                 # batch_x = batch_x[...,:gsize*gnum].reshape(B,-1,gnum,gsize).permute(0,2,1,3).reshape(B*gnum,-1,gsize)\n                 # batch_y = batch_y[...,:gsize*gnum].reshape(B,-1,gnum,gsize).permute(0,2,1,3).reshape(B*gnum,-1,gsize)\n                 B,O,C = batch_y.shape\n-                # mask = torch.arange(C).expand(B, C).to(self.device) < lengths.unsqueeze(1)\n+                mask = (torch.arange(C).expand(B, C).to(self.device) < lengths.unsqueeze(1)).unsqueeze(1)\n                 outputs = self.model(batch_x, batch_x_mark, batch_y, batch_y_mark)\n                 if self.args.dp:\n                     torch.cuda.synchronize()\n                 if self.args.nonautoregressive:\n"
                },
                {
                    "date": 1733149270688,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -218,9 +218,9 @@\n                 \n                 patch_num = 7\n                 batch_y = batch_y.reshape(B,patch_num,-1,C)\n                 for output in outputs:\n-                    loss += (train_criterion(output, batch_y[:,:,:output.shape[1]//patch_num,:].reshape(B,-1,C))).mean()\n+                    loss += (train_criterion(output, batch_y[:,:,:output.shape[1]//patch_num,:].reshape(B,-1,C))*mask).mean()\n                 # loss = (train_criterion(outputs, batch_y) * weights).mean()\n                 if (i + 1) % 100 == 0:\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                         print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n"
                },
                {
                    "date": 1733149356890,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -218,9 +218,9 @@\n                 \n                 patch_num = 7\n                 batch_y = batch_y.reshape(B,patch_num,-1,C)\n                 for output in outputs:\n-                    loss += (train_criterion(output, batch_y[:,:,:output.shape[1]//patch_num,:].reshape(B,-1,C))*mask).mean()\n+                    loss += (train_criterion(output, batch_y[:,:,:output.shape[1]//patch_num,:].reshape(B,-1,C))*mask).mean()*B*C/mask.sum()\n                 # loss = (train_criterion(outputs, batch_y) * weights).mean()\n                 if (i + 1) % 100 == 0:\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                         print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n"
                },
                {
                    "date": 1733149512154,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -98,17 +98,17 @@\n         test_steps = len(vali_loader)\n         iter_count = 0\n         self.model.eval()\n         with torch.no_grad():\n-            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(vali_loader):\n+            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark,lengths) in enumerate(vali_loader):\n                 iter_count += 1\n                 batch_x = batch_x.float().to(self.device)\n                 batch_y = batch_y.float()\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n                 batch_y_mark = batch_y_mark.float().to(self.device)\n                 \n                 B,O,C = batch_y.shape\n-                # mask = torch.arange(C).expand(B, C).to(self.device) < lengths.unsqueeze(1).to(self.device)\n+                mask = (torch.arange(C).expand(B, C).to(self.device) < lengths.unsqueeze(1).to(self.device)).unsqueeze(1)\n                 \n                 outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)\n                 batch_y = batch_y[:, :, :].to(self.device)\n                 # if is_test or self.args.nonautoregressive:\n"
                },
                {
                    "date": 1733149574254,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -126,9 +126,9 @@\n                         outputs = outputs[:, :, -1]\n                         batch_y = batch_y[:, :, -1]\n                 batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n                 for length,output,tl in zip([96,192,336,720],outputs,total_loss):\n-                    loss = (criterion(output[:, -length:, :], batch_y[:,:length,:])).mean().detach().cpu()\n+                    loss = (criterion(output[:, -length:, :], batch_y[:,:length,:])*mask).mean()*B*C/lengths.sum().detach().cpu()\n                     # loss = smape(batch_y[:,:length,:], output[:, -length:, :]).item()\n                     tl.append(loss)\n                 # loss = criterion(outputs, batch_y)\n \n@@ -218,9 +218,9 @@\n                 \n                 patch_num = 7\n                 batch_y = batch_y.reshape(B,patch_num,-1,C)\n                 for output in outputs:\n-                    loss += (train_criterion(output, batch_y[:,:,:output.shape[1]//patch_num,:].reshape(B,-1,C))*mask).mean()*B*C/mask.sum()\n+                    loss += (train_criterion(output, batch_y[:,:,:output.shape[1]//patch_num,:].reshape(B,-1,C))*mask).mean()*B*C/lengths.sum()\n                 # loss = (train_criterion(outputs, batch_y) * weights).mean()\n                 if (i + 1) % 100 == 0:\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                         print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n"
                },
                {
                    "date": 1733193098695,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -126,9 +126,9 @@\n                         outputs = outputs[:, :, -1]\n                         batch_y = batch_y[:, :, -1]\n                 batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n                 for length,output,tl in zip([96,192,336,720],outputs,total_loss):\n-                    loss = (criterion(output[:, -length:, :], batch_y[:,:length,:])*mask).mean()*B*C/lengths.sum().detach().cpu()\n+                    loss = ((criterion(output[:, -length:, :], batch_y[:,:length,:])*mask).mean()*B*C/lengths.sum()).detach().cpu()\n                     # loss = smape(batch_y[:,:length,:], output[:, -length:, :]).item()\n                     tl.append(loss)\n                 # loss = criterion(outputs, batch_y)\n \n"
                },
                {
                    "date": 1733196762383,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -126,9 +126,9 @@\n                         outputs = outputs[:, :, -1]\n                         batch_y = batch_y[:, :, -1]\n                 batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n                 for length,output,tl in zip([96,192,336,720],outputs,total_loss):\n-                    loss = ((criterion(output[:, -length:, :], batch_y[:,:length,:])*mask).mean()*B*C/lengths.sum()).detach().cpu()\n+                    loss = ((criterion(output[:, -length:, :], batch_y[:,:length,:])*mask).mean()).detach().cpu()\n                     # loss = smape(batch_y[:,:length,:], output[:, -length:, :]).item()\n                     tl.append(loss)\n                 # loss = criterion(outputs, batch_y)\n \n"
                },
                {
                    "date": 1733196773209,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -98,17 +98,17 @@\n         test_steps = len(vali_loader)\n         iter_count = 0\n         self.model.eval()\n         with torch.no_grad():\n-            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark,lengths) in enumerate(vali_loader):\n+            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(vali_loader):\n                 iter_count += 1\n                 batch_x = batch_x.float().to(self.device)\n                 batch_y = batch_y.float()\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n                 batch_y_mark = batch_y_mark.float().to(self.device)\n                 \n                 B,O,C = batch_y.shape\n-                mask = (torch.arange(C).expand(B, C).to(self.device) < lengths.unsqueeze(1).to(self.device)).unsqueeze(1)\n+                # mask = (torch.arange(C).expand(B, C).to(self.device) < lengths.unsqueeze(1).to(self.device)).unsqueeze(1)\n                 \n                 outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)\n                 batch_y = batch_y[:, :, :].to(self.device)\n                 # if is_test or self.args.nonautoregressive:\n"
                },
                {
                    "date": 1733196778676,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -178,9 +178,9 @@\n             iter_count = 0\n             self.model.train()\n             epoch_time = time.time()\n             # train_loader.dataset.__shuffle_data__()\n-            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark, lengths) in enumerate(train_loader):\n+            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n                 iter_count += 1\n                 model_optim.zero_grad()\n                 batch_x = batch_x.float().to(self.device)\n                 batch_y = batch_y.float().to(self.device)\n"
                },
                {
                    "date": 1733196786239,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -185,9 +185,9 @@\n                 batch_x = batch_x.float().to(self.device)\n                 batch_y = batch_y.float().to(self.device)\n                 batch_x_mark = batch_x_mark.float().to(self.device)\n                 batch_y_mark = batch_y_mark.float().to(self.device)\n-                lengths = lengths.long().to(self.device)\n+                # lengths = lengths.long().to(self.device)\n                 \n                 # lookback = random.randint(1,min(7,epoch+4))*self.args.input_token_len\n                 # batch_x = batch_x[:,-lookback:,:]\n                 # batch_y = batch_y[:,-lookback:,:]\n@@ -200,9 +200,9 @@\n                 # gsize = M//gnum\n                 # batch_x = batch_x[...,:gsize*gnum].reshape(B,-1,gnum,gsize).permute(0,2,1,3).reshape(B*gnum,-1,gsize)\n                 # batch_y = batch_y[...,:gsize*gnum].reshape(B,-1,gnum,gsize).permute(0,2,1,3).reshape(B*gnum,-1,gsize)\n                 B,O,C = batch_y.shape\n-                mask = (torch.arange(C).expand(B, C).to(self.device) < lengths.unsqueeze(1)).unsqueeze(1)\n+                # mask = (torch.arange(C).expand(B, C).to(self.device) < lengths.unsqueeze(1)).unsqueeze(1)\n                 outputs = self.model(batch_x, batch_x_mark, batch_y, batch_y_mark)\n                 if self.args.dp:\n                     torch.cuda.synchronize()\n                 if self.args.nonautoregressive:\n"
                },
                {
                    "date": 1733196796109,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -218,9 +218,9 @@\n                 \n                 patch_num = 7\n                 batch_y = batch_y.reshape(B,patch_num,-1,C)\n                 for output in outputs:\n-                    loss += (train_criterion(output, batch_y[:,:,:output.shape[1]//patch_num,:].reshape(B,-1,C))*mask).mean()*B*C/lengths.sum()\n+                    loss += (train_criterion(output, batch_y[:,:,:output.shape[1]//patch_num,:].reshape(B,-1,C))).mean()\n                 # loss = (train_criterion(outputs, batch_y) * weights).mean()\n                 if (i + 1) % 100 == 0:\n                     if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                         print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n"
                },
                {
                    "date": 1733198148011,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -126,9 +126,9 @@\n                         outputs = outputs[:, :, -1]\n                         batch_y = batch_y[:, :, -1]\n                 batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n                 for length,output,tl in zip([96,192,336,720],outputs,total_loss):\n-                    loss = ((criterion(output[:, -length:, :], batch_y[:,:length,:])*mask).mean()).detach().cpu()\n+                    loss = ((criterion(output[:, -length:, :], batch_y[:,:length,:])).mean()).detach().cpu()\n                     # loss = smape(batch_y[:,:length,:], output[:, -length:, :]).item()\n                     tl.append(loss)\n                 # loss = criterion(outputs, batch_y)\n \n"
                },
                {
                    "date": 1733285468049,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -42,8 +42,9 @@\n     def __init__(self, args):\n         super(Exp_Forecast, self).__init__(args)\n         \n     def _build_model(self):\n+        self.pred_lens = [self.args.output_token_len]\n         model = self.model_dict[self.args.model].Model(self.args)\n         if self.args.ddp:\n             self.device = torch.device('cuda:{}'.format(self.args.local_rank))\n             model = DDP(model.cuda(), device_ids=[self.args.local_rank])\n@@ -91,9 +92,9 @@\n         criterion = nn.MSELoss()\n         return criterion\n \n     def vali(self, vali_data, vali_loader, criterion, is_test=False):\n-        total_loss = [[] for _ in range(4)]\n+        total_loss = [[] for _ in range(len(self.pred_lens))]\n         total_count = []\n         time_now = time.time()\n         test_steps = len(vali_loader)\n         iter_count = 0\n@@ -125,9 +126,9 @@\n                     else:\n                         outputs = outputs[:, :, -1]\n                         batch_y = batch_y[:, :, -1]\n                 batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n-                for length,output,tl in zip([96,192,336,720],outputs,total_loss):\n+                for length,output,tl in zip(self.pred_lens,outputs,total_loss):\n                     loss = ((criterion(output[:, -length:, :], batch_y[:,:length,:])).mean()).detach().cpu()\n                     # loss = smape(batch_y[:,:length,:], output[:, -length:, :]).item()\n                     tl.append(loss)\n                 # loss = criterion(outputs, batch_y)\n@@ -308,9 +309,9 @@\n                 # if dis != 0:\n                 #     pred_y = pred_y[:, :-dis, :]\n                 outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)\n                 batch_y = batch_y[:, -self.args.output_token_len:, :].detach().cpu()\n-                for length,output,pred in zip([96,192,336,720],outputs,preds):\n+                for length,output,pred in zip(self.pred_lens,outputs,preds):\n                     pred.append(output[:, -length:, :].detach().cpu())\n                     smape_cur = smape(batch_y[:,:pred[-1].shape[1],:],pred[-1])\n                     smape_avg = (smape_avg*count + smape_cur*batch_y.shape[0])/(count+batch_y.shape[0])\n                     count += batch_y.shape[0]\n"
                },
                {
                    "date": 1733294565995,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -274,9 +274,9 @@\n             setting = self.args.test_dir\n             best_model_path = self.args.test_file_name\n             print(\"loading model from {}\".format(os.path.join(self.args.checkpoints, setting, best_model_path)))\n             self.model.load_state_dict(torch.load(os.path.join(self.args.checkpoints, setting, best_model_path)))\n-        preds = [[] for _ in range(4)]\n+        preds = [[] for _ in range(len(self.pred_lens))]\n         trues = []\n         folder_path = './test_results/' + setting + '/'\n         if not os.path.exists(folder_path):\n             os.makedirs(folder_path)\n"
                },
                {
                    "date": 1733372076348,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -42,9 +42,9 @@\n     def __init__(self, args):\n         super(Exp_Forecast, self).__init__(args)\n         \n     def _build_model(self):\n-        self.pred_lens = [self.args.output_token_len]\n+        self.pred_lens = [96,192,336,720]#[self.args.output_token_len]\n         model = self.model_dict[self.args.model].Model(self.args)\n         if self.args.ddp:\n             self.device = torch.device('cuda:{}'.format(self.args.local_rank))\n             model = DDP(model.cuda(), device_ids=[self.args.local_rank])\n"
                },
                {
                    "date": 1733388993241,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -42,9 +42,9 @@\n     def __init__(self, args):\n         super(Exp_Forecast, self).__init__(args)\n         \n     def _build_model(self):\n-        self.pred_lens = [96,192,336,720]#[self.args.output_token_len]\n+        self.pred_lens = [96]#[self.args.output_token_len]\n         model = self.model_dict[self.args.model].Model(self.args)\n         if self.args.ddp:\n             self.device = torch.device('cuda:{}'.format(self.args.local_rank))\n             model = DDP(model.cuda(), device_ids=[self.args.local_rank])\n"
                },
                {
                    "date": 1733389106049,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -216,9 +216,9 @@\n                         outputs = outputs[:, :, -1]\n                         batch_y = batch_y[:, :, -1]\n                 loss = 0\n                 \n-                patch_num = 7\n+                patch_num = 8\n                 batch_y = batch_y.reshape(B,patch_num,-1,C)\n                 for output in outputs:\n                     loss += (train_criterion(output, batch_y[:,:,:output.shape[1]//patch_num,:].reshape(B,-1,C))).mean()\n                 # loss = (train_criterion(outputs, batch_y) * weights).mean()\n"
                },
                {
                    "date": 1733389193847,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -233,30 +233,30 @@\n \n                 loss.backward()\n                 model_optim.step()\n                 \n-                # if (i + 1) % 5000 == 0:\n-            if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n-                print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n+                if (i + 1) % 5000 == 0:\n+                    if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n+                        print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n \n-            vali_loss = self.vali(vali_data, vali_loader, train_criterion, is_test=self.args.valid_last)\n-            test_loss = self.vali(test_data, test_loader, train_criterion, is_test=True)\n-            if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n-                print(\"Epoch: {}, Steps: {} | Vali Loss: {} Test Loss: {}\".format(\n-                    epoch + 1, train_steps, vali_loss, test_loss))\n-            early_stopping(sum(vali_loss), self.model, path)\n-            if early_stopping.early_stop:\n-                if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n-                    print(\"Early stopping\")\n-                break\n-            if self.args.cosine:\n-                scheduler.step()\n-                if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n-                    print(\"lr = {:.10f}\".format(model_optim.param_groups[0]['lr']))\n-            else:\n-                adjust_learning_rate(model_optim, epoch + 1, self.args)\n-            if self.args.ddp:\n-                train_loader.sampler.set_epoch(epoch + 1)\n+                    vali_loss = self.vali(vali_data, vali_loader, train_criterion, is_test=self.args.valid_last)\n+                    test_loss = self.vali(test_data, test_loader, train_criterion, is_test=True)\n+                    if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n+                        print(\"Epoch: {}, Steps: {} | Vali Loss: {} Test Loss: {}\".format(\n+                            epoch + 1, train_steps, vali_loss, test_loss))\n+                    early_stopping(sum(vali_loss), self.model, path)\n+                    if early_stopping.early_stop:\n+                        if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n+                            print(\"Early stopping\")\n+                        break\n+                    if self.args.cosine:\n+                        scheduler.step()\n+                        if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n+                            print(\"lr = {:.10f}\".format(model_optim.param_groups[0]['lr']))\n+                    else:\n+                        adjust_learning_rate(model_optim, epoch + 1, self.args)\n+                    if self.args.ddp:\n+                        train_loader.sampler.set_epoch(epoch + 1)\n                 \n         best_model_path = path + '/' + 'checkpoint.pth'\n         if self.args.ddp:\n             dist.barrier()\n"
                },
                {
                    "date": 1733389220926,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -127,10 +127,10 @@\n                         outputs = outputs[:, :, -1]\n                         batch_y = batch_y[:, :, -1]\n                 batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n                 for length,output,tl in zip(self.pred_lens,outputs,total_loss):\n-                    loss = ((criterion(output[:, -length:, :], batch_y[:,:length,:])).mean()).detach().cpu()\n-                    # loss = smape(batch_y[:,:length,:], output[:, -length:, :]).item()\n+                    # loss = ((criterion(output[:, -length:, :], batch_y[:,:length,:])).mean()).detach().cpu()\n+                    loss = smape(batch_y[:,:length,:], output[:, -length:, :]).item()\n                     tl.append(loss)\n                 # loss = criterion(outputs, batch_y)\n \n                 # loss = loss.detach().cpu()\n"
                },
                {
                    "date": 1733451749470,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -265,9 +265,9 @@\n             self.model.load_state_dict(torch.load(best_model_path), strict=False)\n         return self.model\n \n     def test(self, setting, test=0):\n-        test_data, test_loader = self._get_data(flag='test')\n+        test_data, test_loader = self._get_data(flag='T')\n \n         print(\"info:\", self.args.test_seq_len, self.args.input_token_len, self.args.output_token_len, self.args.test_pred_len)\n         if test:\n             print('loading model')\n"
                },
                {
                    "date": 1733452492153,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -265,9 +265,9 @@\n             self.model.load_state_dict(torch.load(best_model_path), strict=False)\n         return self.model\n \n     def test(self, setting, test=0):\n-        test_data, test_loader = self._get_data(flag='T')\n+        test_data, test_loader = self._get_data(flag='V')\n \n         print(\"info:\", self.args.test_seq_len, self.args.input_token_len, self.args.output_token_len, self.args.test_pred_len)\n         if test:\n             print('loading model')\n"
                },
                {
                    "date": 1733454231813,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -265,9 +265,9 @@\n             self.model.load_state_dict(torch.load(best_model_path), strict=False)\n         return self.model\n \n     def test(self, setting, test=0):\n-        test_data, test_loader = self._get_data(flag='V')\n+        test_data, test_loader = self._get_data(flag='TandV')\n \n         print(\"info:\", self.args.test_seq_len, self.args.input_token_len, self.args.output_token_len, self.args.test_pred_len)\n         if test:\n             print('loading model')\n"
                },
                {
                    "date": 1733454380869,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -42,9 +42,9 @@\n     def __init__(self, args):\n         super(Exp_Forecast, self).__init__(args)\n         \n     def _build_model(self):\n-        self.pred_lens = [96]#[self.args.output_token_len]\n+        self.pred_lens = [96,192,336,720]#[self.args.output_token_len]\n         model = self.model_dict[self.args.model].Model(self.args)\n         if self.args.ddp:\n             self.device = torch.device('cuda:{}'.format(self.args.local_rank))\n             model = DDP(model.cuda(), device_ids=[self.args.local_rank])\n"
                },
                {
                    "date": 1733454403735,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -127,10 +127,10 @@\n                         outputs = outputs[:, :, -1]\n                         batch_y = batch_y[:, :, -1]\n                 batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n                 for length,output,tl in zip(self.pred_lens,outputs,total_loss):\n-                    # loss = ((criterion(output[:, -length:, :], batch_y[:,:length,:])).mean()).detach().cpu()\n-                    loss = smape(batch_y[:,:length,:], output[:, -length:, :]).item()\n+                    loss = ((criterion(output[:, -length:, :], batch_y[:,:length,:])).mean()).detach().cpu()\n+                    # loss = smape(batch_y[:,:length,:], output[:, -length:, :]).item()\n                     tl.append(loss)\n                 # loss = criterion(outputs, batch_y)\n \n                 # loss = loss.detach().cpu()\n"
                },
                {
                    "date": 1733454463501,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -216,9 +216,9 @@\n                         outputs = outputs[:, :, -1]\n                         batch_y = batch_y[:, :, -1]\n                 loss = 0\n                 \n-                patch_num = 8\n+                patch_num = 7\n                 batch_y = batch_y.reshape(B,patch_num,-1,C)\n                 for output in outputs:\n                     loss += (train_criterion(output, batch_y[:,:,:output.shape[1]//patch_num,:].reshape(B,-1,C))).mean()\n                 # loss = (train_criterion(outputs, batch_y) * weights).mean()\n"
                },
                {
                    "date": 1733456111132,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -233,30 +233,30 @@\n \n                 loss.backward()\n                 model_optim.step()\n                 \n-                if (i + 1) % 5000 == 0:\n-                    if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n-                        print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n+                # if (i + 1) % 5000 == 0:\n+            if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n+                print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n \n-                    vali_loss = self.vali(vali_data, vali_loader, train_criterion, is_test=self.args.valid_last)\n-                    test_loss = self.vali(test_data, test_loader, train_criterion, is_test=True)\n-                    if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n-                        print(\"Epoch: {}, Steps: {} | Vali Loss: {} Test Loss: {}\".format(\n-                            epoch + 1, train_steps, vali_loss, test_loss))\n-                    early_stopping(sum(vali_loss), self.model, path)\n-                    if early_stopping.early_stop:\n-                        if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n-                            print(\"Early stopping\")\n-                        break\n-                    if self.args.cosine:\n-                        scheduler.step()\n-                        if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n-                            print(\"lr = {:.10f}\".format(model_optim.param_groups[0]['lr']))\n-                    else:\n-                        adjust_learning_rate(model_optim, epoch + 1, self.args)\n-                    if self.args.ddp:\n-                        train_loader.sampler.set_epoch(epoch + 1)\n+            vali_loss = self.vali(vali_data, vali_loader, train_criterion, is_test=self.args.valid_last)\n+            test_loss = self.vali(test_data, test_loader, train_criterion, is_test=True)\n+            if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n+                print(\"Epoch: {}, Steps: {} | Vali Loss: {} Test Loss: {}\".format(\n+                    epoch + 1, train_steps, vali_loss, test_loss))\n+            early_stopping(sum(vali_loss), self.model, path)\n+            if early_stopping.early_stop:\n+                if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n+                    print(\"Early stopping\")\n+                break\n+            if self.args.cosine:\n+                scheduler.step()\n+                if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n+                    print(\"lr = {:.10f}\".format(model_optim.param_groups[0]['lr']))\n+            else:\n+                adjust_learning_rate(model_optim, epoch + 1, self.args)\n+            if self.args.ddp:\n+                train_loader.sampler.set_epoch(epoch + 1)\n                 \n         best_model_path = path + '/' + 'checkpoint.pth'\n         if self.args.ddp:\n             dist.barrier()\n"
                },
                {
                    "date": 1733456749430,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -265,9 +265,9 @@\n             self.model.load_state_dict(torch.load(best_model_path), strict=False)\n         return self.model\n \n     def test(self, setting, test=0):\n-        test_data, test_loader = self._get_data(flag='TandV')\n+        test_data, test_loader = self._get_data(flag='test')\n \n         print(\"info:\", self.args.test_seq_len, self.args.input_token_len, self.args.output_token_len, self.args.test_pred_len)\n         if test:\n             print('loading model')\n"
                }
            ],
            "date": 1730422452540,
            "name": "Commit-0",
            "content": "from data_provider.data_factory import data_provider\nfrom exp.exp_basic import Exp_Basic\nfrom utils.tools import EarlyStopping, adjust_learning_rate, visual\nfrom utils.metrics import metric\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nimport os\nimport time\nimport warnings\nimport numpy as np\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nimport torch.distributed as dist\nfrom torch.nn import DataParallel\nwarnings.filterwarnings('ignore')\n\n\nclass Exp_Forecast(Exp_Basic):\n    def __init__(self, args):\n        super(Exp_Forecast, self).__init__(args)\n        \n    def _build_model(self):\n        model = self.model_dict[self.args.model].Model(self.args)\n        if self.args.ddp:\n            self.device = torch.device('cuda:{}'.format(self.args.local_rank))\n            model = DDP(model.cuda(), device_ids=[self.args.local_rank])\n        elif self.args.dp:\n            self.device = self.args.gpu\n            model = DataParallel(model, device_ids=self.args.device_ids).to(self.device)\n        else:\n            self.device = self.args.gpu\n            model = model.to(self.device)\n            \n        if self.args.adaptation:\n            model.load_state_dict(torch.load(self.args.pretrain_model_path))\n        return model\n\n    def _get_data(self, flag):\n        data_set, data_loader = data_provider(self.args, flag)\n        return data_set, data_loader\n\n    def _select_optimizer(self):\n        p_list = []\n        for n, p in self.model.named_parameters():\n            if not p.requires_grad:\n                continue\n            else:\n                p_list.append(p)\n        model_optim = optim.Adam([{'params': p_list}], lr=self.args.learning_rate, weight_decay=self.args.weight_decay)\n        if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n            print('next learning rate is {}'.format(self.args.learning_rate))\n        return model_optim\n\n    def _select_criterion(self):\n        criterion = nn.MSELoss()\n        return criterion\n\n    def vali(self, vali_data, vali_loader, criterion, is_test=False):\n        total_loss = []\n        total_count = []\n        time_now = time.time()\n        test_steps = len(vali_loader)\n        iter_count = 0\n        self.model.eval()\n        with torch.no_grad():\n            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(vali_loader):\n                iter_count += 1\n                batch_x = batch_x.float().to(self.device)\n                batch_y = batch_y.float()\n                batch_x_mark = batch_x_mark.float().to(self.device)\n                batch_y_mark = batch_y_mark.float().to(self.device)\n                \n                outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)\n                if is_test or self.args.nonautoregressive:\n                        outputs = outputs[:, -self.args.output_token_len:, :]\n                        batch_y = batch_y[:, -self.args.output_token_len:, :].to(self.device)\n                else:\n                    outputs = outputs[:, :, :]\n                    batch_y = batch_y[:, :, :].to(self.device)\n\n                if self.args.covariate:\n                    if self.args.last_token:\n                        outputs = outputs[:, -self.args.output_token_len:, -1]\n                        batch_y = batch_y[:, -self.args.output_token_len:, -1]\n                    else:\n                        outputs = outputs[:, :, -1]\n                        batch_y = batch_y[:, :, -1]\n                loss = criterion(outputs, batch_y)\n\n                loss = loss.detach().cpu()\n                total_loss.append(loss)\n                total_count.append(batch_x.shape[0])\n                if (i + 1) % 100 == 0:\n                    if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                        speed = (time.time() - time_now) / iter_count\n                        left_time = speed * (test_steps - i)\n                        print(\"\\titers: {}, speed: {:.4f}s/iter, left time: {:.4f}s\".format(i + 1, speed, left_time))\n                        iter_count = 0\n                        time_now = time.time()\n        if self.args.ddp:\n            total_loss = torch.tensor(np.average(total_loss, weights=total_count)).to(self.device)\n            dist.barrier()\n            dist.reduce(total_loss, dst=0, op=dist.ReduceOp.SUM)\n            total_loss = total_loss.item() / dist.get_world_size()\n        else:\n            total_loss = np.average(total_loss, weights=total_count)\n        self.model.train()\n        return total_loss\n\n    def train(self, setting):\n        train_data, train_loader = self._get_data(flag='train')\n        vali_data, vali_loader = self._get_data(flag='val')\n        test_data, test_loader = self._get_data(flag='test')\n        \n        path = os.path.join(self.args.checkpoints, setting)\n        if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n            if not os.path.exists(path):\n                os.makedirs(path)\n\n        time_now = time.time()\n\n        train_steps = len(train_loader)\n        early_stopping = EarlyStopping(self.args, verbose=True)\n        \n        model_optim = self._select_optimizer()\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(model_optim, T_max=self.args.tmax, eta_min=1e-8)\n        criterion = self._select_criterion()\n        \n        for epoch in range(self.args.train_epochs):\n            iter_count = 0\n            self.model.train()\n            epoch_time = time.time()\n            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n                iter_count += 1\n                model_optim.zero_grad()\n                batch_x = batch_x.float().to(self.device)\n                batch_y = batch_y.float().to(self.device)\n                batch_x_mark = batch_x_mark.float().to(self.device)\n                batch_y_mark = batch_y_mark.float().to(self.device)\n                \n                indices = torch.randperm(batch_x.size(-1)).to(batch_x.device)\n                batch_x = batch_x.index_select(-1, indices)\n                batch_y = batch_y.index_select(-1, indices)\n\n                outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)\n                if self.args.dp:\n                    torch.cuda.synchronize()\n                if self.args.nonautoregressive:\n                    batch_y = batch_y[:, -self.args.output_token_len:, :]\n                if self.args.covariate:\n                    if self.args.last_token:\n                        outputs = outputs[:, -self.args.output_token_len:, -1]\n                        batch_y = batch_y[:, -self.args.output_token_len:, -1]\n                    else:\n                        outputs = outputs[:, :, -1]\n                        batch_y = batch_y[:, :, -1]\n                loss = criterion(outputs, batch_y)\n                if (i + 1) % 100 == 0:\n                    if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                        print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n                        speed = (time.time() - time_now) / iter_count\n                        left_time = speed * ((self.args.train_epochs - epoch) * train_steps - i)\n                        print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n                        iter_count = 0\n                        time_now = time.time()\n\n                loss.backward()\n                model_optim.step()\n\n            if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n\n            vali_loss = self.vali(vali_data, vali_loader, criterion, is_test=self.args.valid_last)\n            test_loss = self.vali(test_data, test_loader, criterion, is_test=True)\n            if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                print(\"Epoch: {}, Steps: {} | Vali Loss: {:.7f} Test Loss: {:.7f}\".format(\n                    epoch + 1, train_steps, vali_loss, test_loss))\n            early_stopping(vali_loss, self.model, path)\n            if early_stopping.early_stop:\n                if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                    print(\"Early stopping\")\n                break\n            if self.args.cosine:\n                scheduler.step()\n                if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                    print(\"lr = {:.10f}\".format(model_optim.param_groups[0]['lr']))\n            else:\n                adjust_learning_rate(model_optim, epoch + 1, self.args)\n            if self.args.ddp:\n                train_loader.sampler.set_epoch(epoch + 1)\n                \n        best_model_path = path + '/' + 'checkpoint.pth'\n        if self.args.ddp:\n            dist.barrier()\n            self.model.load_state_dict(torch.load(best_model_path), strict=False)\n        else:\n            self.model.load_state_dict(torch.load(best_model_path), strict=False)\n        return self.model\n\n    def test(self, setting, test=0):\n        test_data, test_loader = self._get_data(flag='test')\n\n        print(\"info:\", self.args.test_seq_len, self.args.input_token_len, self.args.output_token_len, self.args.test_pred_len)\n        if test:\n            print('loading model')\n            setting = self.args.test_dir\n            best_model_path = self.args.test_file_name\n            print(\"loading model from {}\".format(os.path.join(self.args.checkpoints, setting, best_model_path)))\n            self.model.load_state_dict(torch.load(os.path.join(self.args.checkpoints, setting, best_model_path)))\n        preds = []\n        trues = []\n        folder_path = './test_results/' + setting + '/'\n        if not os.path.exists(folder_path):\n            os.makedirs(folder_path)\n        time_now = time.time()\n        test_steps = len(test_loader)\n        iter_count = 0\n        self.model.eval()\n        with torch.no_grad():\n            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n                iter_count += 1\n                batch_x = batch_x.float().to(self.device)\n                batch_y = batch_y.float().to(self.device)\n                batch_x_mark = batch_x_mark.float().to(self.device)\n                batch_y_mark = batch_y_mark.float().to(self.device)\n                \n                inference_steps = self.args.test_pred_len // self.args.input_token_len\n                dis = self.args.test_pred_len - inference_steps * self.args.input_token_len\n                if dis != 0:\n                    inference_steps += 1\n                pred_y = []\n                for j in range(inference_steps):  \n                    if len(pred_y) != 0:\n                        batch_x = torch.cat([batch_x[:, self.args.input_token_len:, :], pred_y[-1]], dim=1)\n                    outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)\n                    pred_y.append(outputs[:, -self.args.output_token_len:, :])\n                pred_y = torch.cat(pred_y, dim=1)\n                if dis != 0:\n                    pred_y = pred_y[:, :-dis, :]\n                batch_y = batch_y[:, -self.args.test_pred_len:, :].to(self.device)\n                \n                outputs = pred_y.detach().cpu()\n                batch_y = batch_y.detach().cpu()\n                pred = outputs\n                true = batch_y\n\n                preds.append(pred)\n                trues.append(true)\n                if (i + 1) % 100 == 0:\n                    if (self.args.ddp and self.args.local_rank == 0) or not self.args.ddp:\n                        speed = (time.time() - time_now) / iter_count\n                        left_time = speed * (test_steps - i)\n                        print(\"\\titers: {}, speed: {:.4f}s/iter, left time: {:.4f}s\".format(i + 1, speed, left_time))\n                        iter_count = 0\n                        time_now = time.time()\n                if self.args.visualize and i % 2 == 0:\n                    dir_path = folder_path + f'{self.args.test_pred_len}/'\n                    if not os.path.exists(dir_path):\n                        os.makedirs(dir_path)\n                    gt = np.array(true[0, :, -1])\n                    pd = np.array(pred[0, :, -1])\n                    visual(gt, pd, os.path.join(dir_path, f'{i}.pdf'))\n\n        preds = torch.cat(preds, dim=0).numpy()\n        trues = torch.cat(trues, dim=0).numpy()\n        print('preds shape:', preds.shape)\n        print('trues shape:', trues.shape)\n        if self.args.covariate:\n            preds = preds[:, :, -1]\n            trues = trues[:, :, -1]\n        mae, mse, rmse, mape, mspe = metric(preds, trues)\n        print('mse:{}, mae:{}'.format(mse, mae))\n        f = open(\"result_long_term_forecast.txt\", 'a')\n        f.write(setting + \"  \\n\")\n        f.write('mse:{}, mae:{}'.format(mse, mae))\n        f.write('\\n')\n        f.write('\\n')\n        f.close()\n        return\n"
        }
    ]
}